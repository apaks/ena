{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import scipy.stats as sstat\n",
    "import scipy.signal as ssig\n",
    "import h5py\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import re\n",
    "import ephys_unit_analysis as ena\n",
    "import fnmatch\n",
    "# from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "mpl.rcParams['pdf.fonttype'] = 42 \n",
    "mpl.rcParams['font.sans-serif']=['Arial', 'Helvetica','Bitstream Vera Sans', 'DejaVu Sans', 'Lucida Grande', \n",
    "                                 'Verdana', 'Geneva', 'Lucid', 'Avant Garde', 'sans-serif']  \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# pal=sns.blend_palette([\"black\", \"crimson\"], 2)\n",
    "sns.despine()\n",
    "# current_palette = sns.color_palette(\"colorblind\", 10)\n",
    "# sns.set_palette(current_palette)\n",
    "\n",
    "# for publication quality plots, not bar graphs, use this: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pub_plots(pal=sns.blend_palette([\"gray\",\"crimson\", 'cyan', 'magenta', 'purple'  ],5)):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_palette(pal)\n",
    "    sns.set_context(\"poster\", font_scale=1.5, rc={\"lines.linewidth\": 2.5, \"axes.linewidth\":2.5, 'figure.facecolor': 'white'}) \n",
    "    sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n",
    "    # optional, makes markers bigger, too, axes.linewidth doesn't seem to work\n",
    "    plt.rcParams['axes.linewidth'] = 2.5\n",
    "\n",
    "rc_pub={'font.size': 25, 'axes.labelsize': 25, 'legend.fontsize': 25.0, \n",
    "    'axes.titlesize': 25, 'xtick.labelsize': 25, 'ytick.labelsize': 25, \n",
    "    #'axes.color_cycle':pal, # image.cmap - rewritesd the default colormap\n",
    "    'axes.linewidth':2.5, 'lines.linewidth': 2.5,\n",
    "    'xtick.color': 'black', 'ytick.color': 'black', 'axes.edgecolor': 'black','axes.labelcolor':'black','text.color':'black'}\n",
    "# to restore the defaults, call plt.rcdefaults() \n",
    "\n",
    "#set_pub_bargraphs()\n",
    "set_pub_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.blend_palette([\"gray\",\"crimson\", 'magenta','cyan',  'purple'  ],5)\n",
    "sns.palplot(pal)\n",
    "sns.set_palette(pal)\n",
    "groups = { '002': 'ko', '003': 'ko', '005': 'wt', '006': 'het', '007':'het', '008': 'het', '009':'ko', '010': 'ko', \n",
    "            '367': 'wt', '369': 'ko', '377': 'het', '378': 'ko', '380': 'ko', '488': 'wt', '489': 'ko', \n",
    "           '930': 'ko', '931': 'het', '936': 'het', '937': 'ko', '942': 'wt', '944': 'het', '958': 'ko', '959': 'het'\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir12_seq = np.array([10, 7, 3, 2, 4, 8, 9, 5, 7, 3, 4, 8, 3, 2, 1, 8, 0, 4, 9, 11, \n",
    "10, 9, 1, 11, 4, 0, 7, 1, 2, 8, 2, 9, 11, 9, 6, 5, 10, 4, 9, 0, 7, 11, 9, \n",
    "5, 9, 10, 11, 6, 8, 9, 5, 4, 2, 8, 11, 2, 10, 3, 5, 1, 7, 0, 4, 9, 1, 5, \n",
    "11, 3, 5, 10, 1, 2, 9, 6, 2, 2, 11, 5, 10, 7, 3, 7, 4, 6, 8, 4, 1, 8, 0, \n",
    "11, 0, 6, 2, 11, 1, 10, 3, 8, 3, 1, 2, 10, 5, 3, 11, 1, 7, 3, 4, 7, 8, 4, 6, \n",
    "7, 11, 7, 0, 8, 6, 10, 4, 5, 7, 2, 10, 3, 5, 9, 8, 6, 3, 2, 0, 11, 0, 6, 10, \n",
    "0, 7, 4, 5, 0, 10, 6, 8, 10, 3, 11, 9, 0, 5, 1, 3, 7, 0, 6, 9, 1, 6, 10, 5, \n",
    "6, 11, 7, 0, 5, 1, 4, 1, 6, 8, 2, 9, 2, 8, 3, 0, 4, 6, 1])\n",
    "\n",
    "dir8_seq = np.array([2, 0, 7, 5, 6, 3, 3, 1, 4, 5, 4, 7, 4, 7, 5, 2, 2, 2, 2, 1, 1, 7,\n",
    "       5, 3, 1, 1, 3, 4, 2, 3, 6, 6, 0, 5, 1, 2, 7, 6, 5, 3, 5, 4, 5, 6,\n",
    "       5, 4, 4, 6, 6, 4, 0, 2, 7, 7, 0, 4, 5, 1, 2, 4, 3, 3, 4, 5, 7, 5,\n",
    "       4, 7, 2, 4, 5, 7, 0, 1, 2, 2, 3, 6, 1, 3, 5, 1, 5, 1, 2, 6, 0, 1,\n",
    "       7, 1, 0, 6, 7, 6, 5, 3, 3, 2, 5, 7, 0, 4, 3, 0, 1, 4, 1, 3, 5, 2,\n",
    "       0, 1, 6, 0, 5, 7, 1, 7, 3, 0, 6, 7, 0, 2, 5, 2, 3, 7, 4, 3, 3, 6,\n",
    "       3, 1, 2, 1, 5, 7, 6, 0, 3, 3, 0, 4, 0, 7, 1, 2, 0, 3, 0, 0, 6, 5,\n",
    "       2, 5, 4, 2, 7, 0, 1, 3, 2, 7, 6, 6, 1, 0, 1, 5, 6, 0, 3, 2, 6, 4,\n",
    "       7, 0, 7, 4, 6, 5, 4, 1, 6, 2, 4, 4, 2, 4, 7, 6, 0, 4, 3, 1, 6, 6,\n",
    "       0, 7])\n",
    "\n",
    "contrast_seq = np.array([1, 0.25, 0.125, 1, 0.0625, 1, 1, 0.5, 0.0625, 0.125, 0.5, \n",
    "0.125, 0.5, 0.5, 0.125, 0.125, 0.5, 0.25, 0.5, 0.0625, 1, 0.0625, 0.5, 1, \n",
    "1, 0.125, 1, 0.125, 0.125, 0.25, 0.0625, 0.0625, 0.125, 0.5, 0.125, 0.5, \n",
    "0.0625, 1, 1, 0.25, 0.25, 1, 1, 0.25, 0.125, 1, 0.25, 0.5, 0.0625, 0.5, \n",
    "0.125, 0.0625, 0.25, 0.0625, 0.25, 0.5, 0.25, 0.5, 0.125, 0.125, 1, 1, \n",
    "0.5, 0.5, 0.125, 1, 0.25, 0.0625, 0.25, 0.25, 1, 1, 0.25, 0.5, 0.5, 0.125, \n",
    "1, 0.125, 0.0625, 0.125, 0.25, 0.0625, 0.25, 0.0625, 0.0625, 0.0625, 0.5, \n",
    "0.125, 0.25, 0.5, 0.0625, 0.25, 0.5, 0.0625, 0.25, 1, 0.0625, 0.0625, 0.25, 0.125])\n",
    "\n",
    "sf_seq = np.array([2, 0, 0, 4, 3, 3, 0, 4, 4, 3, 2, 5, 3, 2, 0, 4, 1, 0, 5, 2, 0, \n",
    "1, 0, 1, 3, 5, 2, 5, 1, 2, 0, 5, 2, 3, 5, 1, 0, 4, 3, 2, 5, 5, 3, 5, \n",
    "2, 0, 3, 3, 0, 3, 4, 5, 4, 1, 4, 0, 1, 5, 4, 1, 5, 3, 3, 5, 3, 3, 2, \n",
    "3, 2, 1, 1, 5, 1, 4, 1, 2, 3, 2, 4, 2, 1, 0, 5, 5, 2, 2, 4, 1, 4, 1, \n",
    "3, 1, 0, 4, 4, 4, 4, 0, 5, 4, 4, 0, 3, 5, 5, 2, 1, 3, 4, 1, 5, 0, 2, \n",
    "2, 0, 0, 0, 1, 2, 1])\n",
    "\n",
    "d_dir12 = {}\n",
    "d_dir8 = {}\n",
    "d_contrast = {}\n",
    "d_sf = {}\n",
    "\n",
    "for i, v in enumerate(dir12_seq):\n",
    "    d_dir12[i] = v \n",
    "for i, v in enumerate(dir8_seq):\n",
    "    d_dir8[i] = v \n",
    "for i, v in enumerate(contrast_seq):\n",
    "    d_contrast[i] = v \n",
    "for i, v in enumerate(sf_seq):\n",
    "    d_sf[i] = v \n",
    "\n",
    "probe = '64DA'\n",
    "channel_groups = ena.get_channel_depth(probe)\n",
    "ch_map = ena.get_channel_depth(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_group = ['k', 'violet', 'blue']\n",
    "colors_training = ['gray', 'crimson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [] # list of experiment folders\n",
    "source_folder = r\"U:\\Data\\pak6\\OpenEphys\\probe_64DB\\SERT KO\\stim-1-only-training\"\n",
    "for root, dirnames, filenames in os.walk(source_folder):\n",
    "    for dirname in fnmatch.filter(dirnames, '*conc'):\n",
    "        if 'atch' in dirname or 'bad' in dirname:\n",
    "            continue\n",
    "        matches.append(os.path.join(root, dirname)) \n",
    "print matches[:5]\n",
    "print len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in matches[:]:\n",
    "\n",
    "    training = 'no'\n",
    "    if 'bad' in path or '489 post' in path:\n",
    "        continue\n",
    "    print path\n",
    "    if 'Male' in path:\n",
    "        sex = 'M'\n",
    "    else:\n",
    "        sex = 'F'\n",
    "    et = path.split('\\\\')[-2].split()[0][:3]\n",
    "    training = path.split('\\\\')[-2].split()[-1]\n",
    "    if 'pre' in training or 'right' in training:\n",
    "        training = 'pre'\n",
    "    else:\n",
    "        training = 'post'\n",
    "    print et, training, sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Receptive field mapping\n",
    "ls_psth = []\n",
    "ls_spikes = []\n",
    "ls_tmt = []\n",
    "\n",
    "for path in matches[:]:\n",
    "    df = 0\n",
    "    training = 'no'\n",
    "    if 'bad' in path or '489 post' in path:\n",
    "        continue\n",
    "    print path\n",
    "    if 'Male' in path:\n",
    "        sex = 'M'\n",
    "    else:\n",
    "        sex = 'F'\n",
    "    et = path.split('\\\\')[-2].split()[0][:3]\n",
    "    training = path.split('\\\\')[-2].split()[-1]\n",
    "    if 'pre' in training or 'right' in training:\n",
    "        training = 'pre'\n",
    "    else:\n",
    "        training = 'post'\n",
    "    print et, training, sex\n",
    "    \n",
    "    path_cluster_groups = os.path.join(path, 'cluster_groups.csv')\n",
    "    cluster_groups = pd.read_csv(path_cluster_groups, sep = '\\t')\n",
    "    # good_units = cluster_groups[cluster_groups.group != 'noise'].cluster_id.values\n",
    "    noise_units = cluster_groups[(cluster_groups['group'] == 'noise') | \n",
    "                                 (cluster_groups['group'] == 'mua') ].cluster_id.values\n",
    "    spike_times = np.load(os.path.join(path, 'spike_times.npy'))\n",
    "    spike_clusters = np.load(os.path.join(path, 'spike_clusters.npy'))\n",
    "    templates = np.load(os.path.join(path, 'templates.npy'))\n",
    "    spike_templates = np.load(os.path.join(path, 'spike_templates.npy'))\n",
    "    df = pd.DataFrame({'times':spike_times.flatten()/30000.0, \n",
    "                       'unit_id':spike_clusters.flatten(), \n",
    "                       'templates':spike_templates.flatten() })\n",
    "\n",
    "    df = df[~df.unit_id.isin(noise_units)]\n",
    "    df.loc[:,'et'] = et\n",
    "    df.loc[:,'training'] = training\n",
    "    df.loc[:,'cluster_id'] = df.loc[:,'unit_id'].astype('str') + 'et' + df.loc[:,'et'] + training\n",
    "    df.loc[:,'sex'] = sex\n",
    "    df.loc[:,'path'] = path\n",
    "#     df.loc[:,'trial_n'] = df.loc[:,'times']//1.0\n",
    "\n",
    "    ls_spikes.append(df)\n",
    "\n",
    "# df_tmt = pd.concat(ls_tmt)\n",
    "df_spikes = pd.concat(ls_spikes)\n",
    "\n",
    "# master_kic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spikes.templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spikes.to_pickle('sert_wt_spikes_probe64da.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\sert_ko_master_spikes.pkl\"\n",
    "df_spikes = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recs with 8 drifitng instead of 12 drifting, 25 trials 8 directions (45 deg)\n",
    "# rec_dir8 = df_spikes[df_spikes.times > 560].path.unique()\n",
    "\n",
    "# tmp = df_spikes[~df_spikes.path.isin(rec_dir8)]\n",
    "# tmp2 = df_spikes[df_spikes.path.isin(rec_dir8)]\n",
    "tmp = df_spikes\n",
    "\n",
    "# 20 trials, 4s trial length\n",
    "df1 = df_spikes[(df_spikes.loc[:, 'times'] < 80)] #sf-tuning\n",
    "df1.loc[:, 'trial_n'] = df1.loc[:, 'times'] //1.0\n",
    "df1.loc[:,'paradigm'] = 'G-1'\n",
    "df1.loc[:,'stim1'] = 'G-1'\n",
    "# 20 trials, 4s trial length\n",
    "df2 = df_spikes[(df_spikes.loc[:, 'times'] > 80) & (df_spikes.loc[:, 'times'] < 160)] \n",
    "df2.times = df2.times - 80\n",
    "df2.loc[:, 'trial_n'] = df2.loc[:, 'times'] //1.0\n",
    "df2.loc[:,'paradigm'] = 'G-N'\n",
    "df2.loc[:,'stim1'] = 'G-N'\n",
    "\n",
    "# 15 trials, 12 directions, 1s trial length\n",
    "df3 = tmp[(tmp.loc[:, 'times'] > 160) & (tmp.loc[:, 'times'] < 340)] \n",
    "df3.times = df3.times - 160\n",
    "df3.loc[:, 'trial_n'] = df3.loc[:, 'times'] //1.0\n",
    "df3.loc[:,'paradigm'] = '12-drifting'\n",
    "df3.loc[:,'stim1'] = df3.trial_n.map(d_dir12)\n",
    "\n",
    "# 25 trials, 8 directions, 1s trial length\n",
    "# df4 = tmp2[(tmp2.loc[:, 'times'] > 160) & (tmp2.loc[:, 'times'] < 360)] \n",
    "# df4.times = df4.times - 160\n",
    "# df4.loc[:, 'trial_n'] = df4.loc[:, 'times'] //1.0\n",
    "# df4.loc[:,'paradigm'] = '8-drifting'\n",
    "# df4.loc[:,'stim1'] = df4.trial_n.map(d_dir8)\n",
    "\n",
    "# 20 trials, 6 sf, 1s trial length\n",
    "df5 = tmp[(tmp.loc[:, 'times'] > 340) & (tmp.loc[:, 'times'] < 460)] \n",
    "df5.times = df5.times - 340\n",
    "df5.loc[:, 'trial_n'] = df5.loc[:, 'times'] //1.0\n",
    "df5.loc[:,'paradigm'] = 'sf-tuning'\n",
    "df5.loc[:,'stim1'] = df5.trial_n.map(d_sf)\n",
    "\n",
    "# df6 = tmp2[(tmp2.loc[:, 'times'] > 360) & (tmp2.loc[:, 'times'] < 480)] \n",
    "# df6.times = df6.times - 360\n",
    "# df6.loc[:, 'trial_n'] = df6.loc[:, 'times'] //1.0\n",
    "# df6.loc[:,'paradigm'] = 'sf-tuning'\n",
    "# df6.loc[:,'stim1'] = df6.trial_n.map(d_sf)\n",
    "# df6 = pd.concat([df5, df6])\n",
    "\n",
    "# 20 trials, 5 contasrts, 1s trial length\n",
    "df7 = tmp[(tmp.loc[:, 'times'] > 460) & (tmp.loc[:, 'times'] < 560)] \n",
    "df7.times = df7.times - 460\n",
    "df7.loc[:, 'trial_n'] = df7.loc[:, 'times'] //1.0\n",
    "df7.loc[:,'paradigm'] = 'contrast-tuning'\n",
    "df7.loc[:,'stim1'] = df7.trial_n.map(d_contrast)\n",
    "\n",
    "# df8 = tmp2[(tmp2.loc[:, 'times'] > 480) & (tmp2.loc[:, 'times'] < 580)] \n",
    "# df8.times = df8.times - 480\n",
    "# df8.loc[:, 'trial_n'] = df8.loc[:, 'times'] //1.0\n",
    "# df8.loc[:,'paradigm'] = 'contrast-tuning'\n",
    "# df8.loc[:,'stim1'] = df8.trial_n.map(d_contrast)\n",
    "# df8 = pd.concat([df7, df8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df1.shape, df2.shape, df3.shape, df5.shape, df7.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conc.to_pickle('sert_wt_spikes_mapped.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_psth = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#generate psth for vmmn data\n",
    "trial_length = 1.0\n",
    "th_bin = 0.01\n",
    "trials_number = 20.0\n",
    "ls_psth = []\n",
    "ls_tmt = []\n",
    "\n",
    "data = df1\n",
    "# data['opto'] = 1\n",
    "paradigm = data.paradigm.unique()[0]\n",
    "if 'G' in paradigm:\n",
    "    trial_length = 4.0\n",
    "elif '12' in paradigm:\n",
    "    trials_number = 15\n",
    "elif '8' in paradigm:\n",
    "    trials_number = 25\n",
    "print paradigm, trials_number, trial_length\n",
    "for idx_train, train in enumerate(sorted(data.training.unique())):\n",
    "    print train\n",
    "    tmp = data[data.training == train]\n",
    "    for idx_stim1, stim1 in enumerate(sorted(tmp['stim1'].unique())):\n",
    "        print stim1\n",
    "\n",
    "        tmp2 = tmp[ (tmp.stim1 == stim1)]\n",
    "\n",
    "        for unit in tmp2['cluster_id'].unique():\n",
    "            tmp3 = tmp2[(tmp2.cluster_id == unit)]\n",
    "  \n",
    "            et = tmp3.et.unique()[0]\n",
    "            sex = tmp3.sex.unique()[0]\n",
    "            path = tmp3.path.unique()[0]\n",
    "            try:\n",
    "                tmt, depth, ch_idx = ena.ksort_get_tmt(tmp3, unit, templates, channel_groups)\n",
    "            except:\n",
    "                depth = np.nan\n",
    "            cuid =  str(unit) + 'et' + str(et) + str(stim1) + str(paradigm) + train\n",
    "            \n",
    "            df = ena.getRaster_kilosort(tmp3, unit, trial_length) \n",
    "            # check the reliability of the neuron, spike in 0.8 of trials\n",
    "#             if df.trial.unique().size < 0.5*trials_number:\n",
    "#                 continue\n",
    "            h, ttr = ena.PSTH(df.times, th_bin, trial_length, trials_number) # all times rescaled to 0-4 this is why trias number 1.0\n",
    "            zscore = sstat.mstats.zscore(h)\n",
    "#             mean = np.mean(h[:30])\n",
    "#             std = np.std(h[:30])\n",
    "#             if mean==0:\n",
    "#                 std=1\n",
    "#             ztc = (h - mean)/std\n",
    "\n",
    "            df_psth_tmp = pd.DataFrame({     'times':ttr,    'Hz':h,      'cuid': cuid, 'depth':depth,  \n",
    "                                        'path': path, 'stim1': str(stim1), 'training': train, 'sex':sex,\n",
    "                               'abs_times': idx_stim1*trial_length+ttr, 'paradigm':paradigm, \n",
    "                                    'zscore':zscore,  'et': et, 'cluster_id': unit  })\n",
    "\n",
    "            ls_psth.append(df_psth_tmp)\n",
    "#         df_tmt_tmp = pd.DataFrame({'tmt':tmt,  'cluster_id' : unit,  'depth': depth, 'path': path })\n",
    "#         ls_tmt.append(df_tmt_tmp)\n",
    "#     break\n",
    "d_psth[paradigm] = pd.concat(ls_psth)\n",
    "# df_tmt = pd.concat(ls_tmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate psth for vmmn data\n",
    "ls_tmt = []\n",
    "\n",
    "data = df1\n",
    "# data['opto'] = 1\n",
    "paradigm = data.paradigm.unique()[0]\n",
    "\n",
    "for idx_train, train in enumerate(sorted(data.training.unique())):\n",
    "    print train\n",
    "    tmp = data[data.training == train]\n",
    "    for idx_stim1, stim1 in enumerate(sorted(tmp['stim1'].unique()[:5])):\n",
    "        \n",
    "\n",
    "        tmp2 = tmp[ (tmp.stim1 == stim1)]\n",
    "\n",
    "        for unit in tmp2['cluster_id'].unique():\n",
    "            \n",
    "            tmp3 = tmp2[(tmp2.cluster_id == unit)]\n",
    "  \n",
    "            et = tmp3.et.unique()[0]\n",
    "            sex = tmp3.sex.unique()[0]\n",
    "            path = tmp3.path.unique()[0]\n",
    "            try:\n",
    "                tmt, depth, ch_idx = ena.ksort_get_tmt(tmp3, unit, templates, channel_groups)\n",
    "            except:\n",
    "                depth = np.nan\n",
    "#             cuid =  str(unit) + 'et' + str(et) + str(stim1) + str(paradigm) + train\n",
    "\n",
    "            df_tmt_tmp = pd.DataFrame({'tmt':tmt,  'cluster_id' : unit,  'depth': depth, 'path': path, 'et':et })\n",
    "            ls_tmt.append(df_tmt_tmp)\n",
    "\n",
    "df_tmt_db = pd.concat(ls_tmt)\n",
    "df_tmt_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmt_ms.to_pickle('sert_paper_tmt_master.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print d_psth[paradigm].shape\n",
    "print d_psth[paradigm].stim1.unique()\n",
    "print d_psth[paradigm].cluster_id.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert = 0\n",
    "df_sert = pd.concat(d_psth.values()).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert['group'] =  df_sert.et.map(groups)\n",
    "print df_sert['group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert[df_sert.group == 'het'].groupby(['group', 'et', 'training']).path.unique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df_sert['depth'] <= 600) & (df_sert['depth'] >= 400),\n",
    "    (df_sert['depth'] < 400) ,\n",
    "    (df_sert['depth'] > 600)]\n",
    "choices = ['l4', 'l5/6', 'l2/3']\n",
    "df_sert['layer'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\sert_psth_master.pkl\"\n",
    "df_sert = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ztc(data):\n",
    "    if 'G' in data.paradigm.unique()[0]:\n",
    "        base_fr = data[(data.times > 0) & (data.times < 0.5)].Hz.values\n",
    "    else:\n",
    "        base_fr = data[(data.times > 0) & (data.times < 0.3)].Hz.values\n",
    "    mean = np.mean(base_fr)\n",
    "    std = np.std(base_fr)\n",
    "    if mean == 0:\n",
    "        std=1\n",
    "    ztc = (data.Hz.values - mean)/std\n",
    "    data['ztc'] = ztc\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vis_resp(data):    \n",
    "    tmp = data[data.stim1 == 'G-1']\n",
    "    base_fr = tmp[(tmp.times > 0.05) & (tmp.times < 0.35)].Hz.values\n",
    "    dev_fr = tmp[(tmp.times > 0.55) & (tmp.times < 0.85) ].Hz.values\n",
    "    exc_inh = np.mean(dev_fr) - np.mean(base_fr)\n",
    "    p_resp = sstat.wilcoxon(dev_fr, base_fr)\n",
    "    if p_resp[1] < 0.05:\n",
    "        if exc_inh > 0:\n",
    "            data['resp_sig'] = 'exc'\n",
    "        else:\n",
    "            data['resp_sig'] = 'inh'\n",
    "    else:\n",
    "        data['resp_sig'] = 'ns'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert = df_sert.groupby(('cluster_id', 'paradigm' ,'stim1')).apply(compute_ztc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert = df_sert.groupby(('cluster_id', 'paradigm' ,'stim1')).apply(_duration_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert['n_type'] = df_sert.cluster_id.map(neuron_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "f, ax = plt.subplots(figsize = (8, 8), facecolor = 'w')\n",
    "idx = 13\n",
    "\n",
    "_data = df_sert\n",
    "\n",
    "data = _data[(_data.paradigm == '12-drifting' )\n",
    "              & (_data.training == 'post') \n",
    "#                 & (_data.group == 'ko') \n",
    "#              & (_data.n_type == 'fs') \n",
    "#                 & (_data.resp_sig == 'exc')\n",
    "               ]  \n",
    "high_ztc_units = data[data.ztc > 400].cluster_id.unique()\n",
    "data = data[~data.cluster_id.isin(high_ztc_units)]\n",
    "\n",
    "\n",
    "# data = data[(data.et != '378') & (data.et != '377')]\n",
    "\n",
    "# data = data.dropna()\n",
    "# data = data[ (data.times >= 0.05) & (data.times < 2.5)]\n",
    "\n",
    "hm = data.pivot('cluster_id', 'abs_times', 'zscore')\n",
    "hm = hm.dropna()\n",
    "good_units = hm.index.values\n",
    "hm2 = hm.values[ np.argsort(np.mean(hm.values[:,50:100], axis = 1) )]\n",
    "\n",
    "sns.heatmap(hm2, cmap = 'jet',  annot=False, xticklabels = 200, vmax = 6,\n",
    "            vmin = -1, robust = True, yticklabels=False, ax = ax, cbar = True )\n",
    "ax.set(xlabel='', ylabel= hm.shape[0])\n",
    "# plt.savefig('hm-sert_het_units-g1-pre.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zsc = 'ztc'\n",
    "\n",
    "fig, ax = plt.subplots(figsize= (14,6))\n",
    "# data = _data[ (_data.paradigm == 'sf_tuning') & (_data.cluster_id.isin(good_units)) ]\n",
    "# data = _data[ (_data.paradigm.str.contains('odd')) | (_data.stim1.str.contains('ctr2'))]\n",
    "data = data[ data.cluster_id.isin(good_units)]\n",
    "\n",
    "# data = data[data.side.str.contains('left')]\n",
    "fig_inp = data[(data.times > 0.05) & (data.times < 2.5)]\n",
    "# target_units = df_odd[df_odd.resp_sig == 'inh'].cluster_id.unique()\n",
    "\n",
    "fig_inp = fig_inp.sort_values(by = ['group',], ascending=False)\n",
    "\n",
    "# f, ax = plt.subplots(figsize = (12, 8), facecolor = 'w\n",
    "\n",
    "sns.lineplot(data=fig_inp, x = 'times', y = zsc, ci = None,\n",
    "              legend = False, hue = 'group', \n",
    "             palette = ['k', 'magenta', 'blue'],\n",
    "      estimator=np.nanmean)\n",
    "sns.despine()\n",
    "\n",
    "# for i in range(5):\n",
    "#     plt.axvspan(0.35 +i, 0.8+i, alpha=0.2, color='gray')\n",
    "    \n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# plt.axvspan(0.5, 0.9, color='gray', alpha = 0.2)\n",
    "# plt.savefig('line-sert-contrast-post.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert.loc[((df_sert.paradigm.str.contains('G')) ), 'times'] = df_sert.loc[((df_sert.paradigm.str.contains('G'))), 'times'] - 0.50\n",
    "df_sert = df_sert.round({'times': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,v in enumerate((sorted(df_sert[ (df_sert.et.str.startswith('S'))].et.unique()))[:]):\n",
    "    print v\n",
    "    df_sert.loc[((df_sert.et == v) & (df_sert.training == 'post')), 'times'] = df_sert.loc[((df_sert.et == v) & (df_sert.training == 'post')), 'times'] - 0.50\n",
    "df_sert = df_sert.round({'times': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_training = ['gray', 'crimson']\n",
    "colors_group = [ 'k', 'blue', 'magenta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _data = mmn_exp[mmn_exp.paradigm =='paired-oddball']\n",
    "_data = data\n",
    "\n",
    "vep1 = _data[(_data.times>0.53) & (_data.times<0.63)]\n",
    "vep1 = vep1.groupby([ 'group' ,'cluster_id']).ztc.mean().reset_index()\n",
    "\n",
    "vep1.loc[:, 'vep'] = 1\n",
    "\n",
    "vep2 = _data[(_data.times > 0.73) & (_data.times < 0.83)]\n",
    "vep2 = vep2.groupby([ 'group' ,'cluster_id']).ztc.mean().reset_index()\n",
    "\n",
    "vep2.loc[:, 'vep'] = 2\n",
    "\n",
    "vep3 = _data[(_data.times > 0.93) & (_data.times < 1.03)]\n",
    "vep3 = vep3.groupby([ 'group' ,'cluster_id']).ztc.mean().reset_index()\n",
    "\n",
    "vep3.loc[:, 'vep'] = 3\n",
    "\n",
    "out = pd.concat([vep1, vep2, vep3])\n",
    "# __data = band_df\n",
    "g = sns.factorplot(x = \"vep\", y= \"ztc\", hue ='group',  data=out,  kind = 'bar', ci = 68, \n",
    "                   palette = colors_group, hue_order = ['wt', 'het', 'ko'],\n",
    "                   capsize = 0.02, saturation = 1,\n",
    "                   size = 5, aspect=1.5 )\n",
    "\n",
    "# plt.savefig('bar-sert-g1-pre.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vep2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = 'vep'\n",
    "val = 'ztc'\n",
    "_inp = out\n",
    "for l in sorted(_inp[cond].unique()):\n",
    "    stat_inp = _inp[_inp[cond] == l]\n",
    "    stat_inp = stat_inp.groupby(['group', 'cluster_id']).mean().reset_index()\n",
    "    \n",
    "    x1 = stat_inp[(stat_inp.group == 'wt')   ][val].dropna().values\n",
    "    x2 = stat_inp[(stat_inp.group == 'het')   ][val].dropna().values\n",
    "    x3 = stat_inp[(stat_inp.group == 'ko')   ][val].dropna().values\n",
    "\n",
    "    print '\\t', l\n",
    "    print 'wt mean sem', np.mean(x1), sstat.sem(x1)\n",
    "    print 'het mean sem', np.mean(x2), sstat.sem(x2)\n",
    "    print 'ko mean sem', np.mean(x3), sstat.sem(x3)\n",
    "    print len(x1), len(x2), len(x3)\n",
    "    \n",
    "    print sstat.kruskal(x1, x2, x3)\n",
    "    print sstat.mannwhitneyu(x1, x2)\n",
    "    print sstat.mannwhitneyu(x1, x3)\n",
    "    print sstat.mannwhitneyu(x2, x3)\n",
    "    print '------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert.loc[((df_sert.et == '380') & (df_sert.training == 'pre')), 'times'] = df_sert.loc[((df_sert.et == '380') \n",
    "                                               & (df_sert.training == 'pre')), 'times'] - 0.50\n",
    "df_sert = df_sert.round({'times': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detect_peaks import detect_peaks\n",
    "def _duration_peaks(df):\n",
    "    ls = []\n",
    "    thres = np.std(df[(df.times > 0) & (df.times < 0.3)])\n",
    "    data = df[(df.times > 0) & (df.times < 1)].zscore\n",
    "    peakind = detect_peaks(data, mph=1.5, mpd=10)\n",
    "    if peakind.size>0:\n",
    "        if peakind.size>1:#what if I change this to >2? to make it so in pre, if only one peak is detected then unit not considered oscillatory\n",
    "            if peakind[0] > 30 and peakind[0] < 60:\n",
    "            #if peakind[0]>50 and peakind[0]<70: #to be considered, first peak of unit must be found within the cycle 1 period\n",
    "                mask = np.array(np.diff(peakind)<50)\n",
    "                for ind, val in enumerate(mask):\n",
    "                    if val == False:\n",
    "                        mask[ind:] = False\n",
    "                mask = np.insert(mask, 0, True)\n",
    "                peakind = peakind[mask]\n",
    "                dur = (peakind[-1]/100.0)- 0.3\n",
    "            else:   \n",
    "                dur = np.nan\n",
    "        elif peakind[0] > 30 and peakind[0] < 60:\n",
    "        #elif peakind[0]>50 and peakind[0]<70: #to be considered, first peak of unit must be found within the cycle 1 period  \n",
    "            dur = (peakind[0]/100.0)- 0.3\n",
    "        else:\n",
    "            dur = np.nan\n",
    "        if dur<0 or dur>1:\n",
    "            dur = np.nan\n",
    "    else:\n",
    "            dur = np.nan\n",
    "    df['dur'] = dur\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = df_sert\n",
    "\n",
    "data = _data[(_data.paradigm == '12-drifting' )\n",
    "              & (_data.training == 'post') \n",
    "#                 & (_data.group == 'ko') \n",
    "#                 & (_data.resp_sig == 'exc')\n",
    "               ]  \n",
    "data = data.groupby(['group', 'cluster_id', 'stim1']).apply(_duration_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data.groupby(['group','cluster_id']).mean().reset_index()\n",
    "\n",
    "x = tmp[tmp.group == 'wt'].dur.dropna().values\n",
    "x2 = tmp[tmp.group == 'het'].dur.dropna().values\n",
    "x3 = tmp[tmp.group == 'ko'].dur.dropna().values\n",
    "\n",
    "sns.kdeplot(x, cumulative=True,  color = colors_group[0])\n",
    "sns.kdeplot(x2, cumulative=True,  color = colors_group[1])\n",
    "sns.kdeplot(x3, cumulative=True,  color = colors_group[2])\n",
    "sns.despine()\n",
    "# plt.savefig('cdf-dur_sert-contrast-bystims-post.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data.groupby(['group','cluster_id', 'stim1' ]).mean().reset_index()\n",
    "tmp['ori'] = tmp.stim1.astype('float')*30%180\n",
    "sns.factorplot(x = 'ori', y = 'dur', hue = 'group', kind = 'bar' , data = tmp,\n",
    "              size = 5, aspect = 1.5, hue_order = ['wt', 'het', 'ko'], legend = False,\n",
    "               palette = colors_group)\n",
    "# plt.savefig('bar-dur_sert-ori-post-bystim.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.mean(x), sstat.sem(x)\n",
    "print np.mean(x2), sstat.sem(x2)\n",
    "print np.mean(x3), sstat.sem(x3)\n",
    "\n",
    "print len(x), len(x2), len(x3)\n",
    "\n",
    "print sstat.kruskal(x, x2, x3)\n",
    "print sstat.ks_2samp(x, x2)\n",
    "print sstat.ks_2samp(x, x3)\n",
    "print sstat.ks_2samp(x2, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d\n",
    "def double_gaus(x, base, theta_pref, rp, rn, sigma):\n",
    "    y = []\n",
    "    for idx in range(x.size):\n",
    "        pref = min(abs(x[idx] - theta_pref), abs(x[idx] - theta_pref+360), abs(x[idx] - theta_pref-360)) \n",
    "        null = min(abs(x[idx] - theta_pref+180), abs(x[idx] - theta_pref+180+360), abs(x[idx] - theta_pref+180-360))\n",
    "\n",
    "        y.append( base + rp * np.exp( - (pref**2)/(2*(sigma**2))) + rn * np.exp( -(null**2)/(2*(sigma**2))  ))\n",
    "        \n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_gaus(x, base, theta_pref, rp, sigma):\n",
    "    y = []\n",
    "    for idx in range(x.size):\n",
    "        pref = min(abs(x[idx] - theta_pref), abs(x[idx] - theta_pref+180), abs(x[idx] - theta_pref-180)) \n",
    "        \n",
    "\n",
    "        y.append( base + rp * np.exp( - (pref**2)/(2*(sigma**2))) )\n",
    "        \n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ang_diff(a1, a2):\n",
    "    return 180 - abs(abs(a1 - a2) - 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir2ori(angle):   \n",
    "    if angle>180:\n",
    "        angle = angle - 180\n",
    "    return angle\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_odsi(data):\n",
    "    val = 'Hz'\n",
    "    pref_dir = data.pref.iloc[0]\n",
    "    null_ang_fr = data[data.dir == (pref_dir+180)%360][val].values[0]\n",
    "    ort_ang = (pref_dir + 90)%360, ((pref_dir + 90)%360 + 180)%360\n",
    "    pref_ang_fr = data[data.dir == pref_dir][val].values[0]\n",
    "    pref_ori = (null_ang_fr + pref_ang_fr)/2.0\n",
    "    ort_ang_fr = (data[data.dir == ort_ang[0]][val].values[0] + data[data.dir == ort_ang[1]][val].values[0])/2.0\n",
    "    \n",
    "    osi = (pref_ori - ort_ang_fr)/pref_ori \n",
    "    dsi = (pref_ang_fr - null_ang_fr)/pref_ang_fr \n",
    "    data['osi'] = osi\n",
    "    data['dsi'] = dsi\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cv_odsi(data):\n",
    "    fr = data.Hz.values\n",
    "    angles = np.deg2rad(data.dir.values)\n",
    "    \n",
    "    cv_dir = abs(np.dot(fr, np.exp(1j*angles))/np.sum(abs(fr)))\n",
    "    cv_ori = abs(np.dot(fr, np.exp(2j*angles))/np.sum(abs(fr)))\n",
    "    data['cv_ori'] = cv_ori\n",
    "    data['cv_dir'] = cv_dir\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp.group.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df_sert[(df_sert.paradigm == '12-drifting') ]\n",
    "# df_vmmn['n_type'] = df_vmmn.cluster_id.map(neuron_type)\n",
    "data = data[data.cluster_id.isin(good_units)]\n",
    "\n",
    "fig_inp = data[(data.times > 0.35) & (data.times < 0.8) ]\n",
    "fig_inp = fig_inp.groupby([ 'group', 'training' , 'layer', 'n_type', 'cluster_id', 'stim1']).mean().reset_index()\n",
    "tuning_good_units = fig_inp.pivot('cluster_id', 'stim1', 'Hz').dropna().index.values.tolist()\n",
    "\n",
    "base = data[(data.times > 0) & (data.times < 0.3)  ].groupby(['group' , 'training', 'layer', 'n_type',\n",
    "                            'cluster_id', 'stim1']).mean().reset_index().Hz.values\n",
    "fig_inp.loc[:,'bc_fr'] = fig_inp['Hz'] - base\n",
    "\n",
    "# fig_inp = fig_inp[fig_inp.cluster_id.isin(tuning_good_units)]\n",
    "\n",
    "fig_inp['norm'] = fig_inp.groupby('cluster_id')['bc_fr'].apply(lambda x: (x-x.min())/(x.max()-x.min()))\n",
    "fig_inp['stim1'] = fig_inp.stim1.astype('float')\n",
    "\n",
    "fig_inp = fig_inp.sort_values(by = ['stim1', 'training', 'group'], ascending= [True, False, False])\n",
    "print fig_inp.cluster_id.unique().size\n",
    "ax = sns.catplot(x=\"stim1\", y = \"norm\", data = fig_inp,  kind = 'point', ci = 68, hue= 'group',\n",
    "                    palette = colors_group, hue_order = ['wt', 'het', 'ko'],\n",
    "#                  col = 'group', col_wrap = 1,\n",
    "#                    capsize = 0.02, saturation = 1,\n",
    "                   height = 5, aspect=1.5 )\n",
    "# plt.savefig('bar-contrast-tuning-pre-35-08.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp.times.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_df = fig_inp.sort_values('Hz', ascending=False).groupby('cluster_id', as_index=False).first()\n",
    "pref_d = dict(zip(dir_df.cluster_id, dir_df.stim1*30))\n",
    "\n",
    "fig_inp['pref'] = fig_inp.cluster_id.map(pref_d)\n",
    "fig_inp['dir'] = fig_inp.stim1*30\n",
    "fig_inp['ori'] = fig_inp['dir']%180\n",
    "fig_inp['pref_ori'] = fig_inp['pref']%180\n",
    "fig_inp = fig_inp.groupby('cluster_id').apply(compute_odsi)\n",
    "fig_inp = fig_inp.groupby('cluster_id').apply(compute_cv_odsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = fig_inp.sort_values('norm', ascending=False).groupby('cluster_id', as_index=False).first()\n",
    "# sf_list = sf_df.stim1.str.split('r', expand = True)[1].values.astype('int')\n",
    "pref_d = dict(zip(sf_df.cluster_id, sf_df.stim1))\n",
    "\n",
    "# sf_df = fig_inp[fig_inp.opto == 1].sort_values('ztc', ascending=False).groupby('cluster_id', as_index=False).first()\n",
    "# pref_opto_d = dict(zip(sf_df.cluster_id, sf_df.stim1))\n",
    "\n",
    "fig_inp['pref'] = fig_inp.cluster_id.map(pref_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unit in fig_inp.cluster_id.unique()[15:50]:\n",
    "    \n",
    "    data = fig_inp[fig_inp.cluster_id == unit]\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    angles = data.dir.values\n",
    "    fr = data.Hz.values\n",
    "#     a = np.deg2rad(angles) \n",
    "    a = np.deg2rad(angles)\n",
    "    cv_dir = abs(np.dot(fr, np.exp(1j*a))/np.sum(abs(fr)))\n",
    "    \n",
    "    cv_ori = abs(np.dot(fr, np.exp(2j*a))/np.sum(abs(fr)))\n",
    "\n",
    "\n",
    "#     cir_mean = cmath.polar(r)\n",
    "\n",
    "    print cv_dir, cv_ori\n",
    "    \n",
    "#     ax.set_theta_zero_location(\"N\")\n",
    "#     ax.set_theta_direction(-1)\n",
    "    fr = np.append(fr, fr[0])\n",
    "    a = np.append(a, a[0])\n",
    "    ax.plot(a, fr)\n",
    "    ax.plot(mean[1], mean[0], 'ro')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = angles/360*2*pi;\n",
    "r = (rates * exp(i*angles)') / sum(abs(rates));\n",
    "cv = 1-abs(r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(fr, angles_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a double Gaus to 12 directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit double gaus to the firing rate of individual units\n",
    "sigma = [15, 30, 40, 60, 90]\n",
    "tot_error = []\n",
    "fit_err = {}\n",
    "pref_ori = {}\n",
    "ori_tuning_w = {}\n",
    "dir_tuning_w = {}\n",
    "pref_ang = {}\n",
    "xnew = np.arange(0, 340, 15)\n",
    "\n",
    "tmp = fig_inp[(fig_inp.group != 'o') \n",
    "#             & (fig_inp.training == 'post')\n",
    "#             & (fig_inp.fit_err < 0.5) \n",
    "             ]\n",
    "\n",
    "for unit in tmp.cluster_id.unique()[:]:\n",
    "    \n",
    "    data = fig_inp[fig_inp.cluster_id == unit]\n",
    "    m = data.Hz.max()\n",
    "    fr = data.Hz.values\n",
    "    theta_pref = data.pref.iloc[0]\n",
    "    angles = np.deg2rad(data.dir.values)\n",
    "    si_ori = np.sqrt(np.dot(fr, np.sin(angles*2))**2 + \n",
    "                          np.dot(fr, np.cos(angles*2))**2)/np.sum(fr)\n",
    "    si_dir = np.sqrt(np.dot(fr, np.sin(angles))**2 + \n",
    "                          np.dot(fr, np.cos(angles))**2)/np.sum(fr)\n",
    "    \n",
    "    cv_dir = abs(np.dot(fr, np.exp(1j*angles))/np.sum(abs(fr)))\n",
    "    cv_ori = abs(np.dot(fr, np.exp(2j*angles))/np.sum(abs(fr)))\n",
    "    \n",
    "    rp = data[data['dir']== theta_pref].Hz.values[0]\n",
    "    rn =  data[data['dir']==ang_diff(theta_pref, 180)].Hz.values[0]\n",
    "   \n",
    "    x = data.dir.unique()\n",
    "    \n",
    "    y = data.Hz.values\n",
    "#     y_std = data['fr_std'].values +0.001\n",
    "    \n",
    "   \n",
    "    init_params = [0, theta_pref, m, sigma[2] ]\n",
    "    try:\n",
    "        # fir single gaus for orientatio, double for direction tuning\n",
    "#         bound_params = ([0, 0, 0,15], [m, 361, m*3, 180])\n",
    "#         popt, pcov = curve_fit(single_gaus, x, y, bounds = bound_params , p0 = init_params, sigma = y_std, absolute_sigma=True)\n",
    "\n",
    "        init_params_double = [0, theta_pref, m, m, sigma[2] ]\n",
    "        bound_params_double = ([0, 0, 0, 0,15], [m, 361, m*3, m*3, 180])\n",
    "        popt_double, pcov_double = curve_fit(double_gaus, x, y, bounds = bound_params_double , p0 = init_params_double)\n",
    "        perr_double = np.sqrt(np.diag(pcov_double))\n",
    "\n",
    "        fit_double = double_gaus(x, *popt_double)\n",
    "        fit_error_double = sum((y-fit_double)**2)  /  sum((y-y.mean())**2)\n",
    "        perr = np.sqrt(np.diag(pcov_double))\n",
    "    except:\n",
    "        print('error')\n",
    "        continue\n",
    "\n",
    "\n",
    "    fit_err[unit] = fit_error_double\n",
    "    pref_ang[unit] = popt_double[1]\n",
    "#     pref_ori[unit] = ang_2ori(popt_double[1])\n",
    "    \n",
    "    dir_tuning_w[unit] = round(popt_double[-1], 2)\n",
    "    #dir_tuning_w[n] = popt_double[-1]\n",
    "    #print ang_2ori(popt[1]), popt[-1]\n",
    "#     if fit_error>0.7:\n",
    "\n",
    "\n",
    "#     print 'error ', fit_error_double, 'pref ' , popt_double[1]\n",
    "# #     print 'dsi ', data.dsi.values[0], 'osi ', data.osi.values[0]\n",
    "#     print 'osi', round(cv_ori, 3)\n",
    "# #     print 'si', round(si_dir, 3), round(si_ori, 3)\n",
    "#     print 'tuning_w', round(popt_double[-1], 2)\n",
    "#     f, ax = plt.subplots()\n",
    "#     sns.despine(ax = ax)\n",
    "#     f2 = interp1d(x, fit_double, kind = 'quadratic')\n",
    "#     ax.plot(x, y, 'mo-')\n",
    "#     ax.plot(xnew, f2(xnew), 'r')\n",
    "# #     ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "#     plt.show()\n",
    "\n",
    "#         continue\n",
    "    \n",
    "\n",
    "\n",
    "# print np.sum(tot_error)\n",
    "fig_inp['fit_err'] = fig_inp.cluster_id.map(fit_err)\n",
    "fig_inp['tuning_w'] = fig_inp.cluster_id.map(dir_tuning_w)\n",
    "\n",
    "# fig_inp['pref_ang'] = fig_inp.cluster_id.map(pref_ang)\n",
    "# df_ori0['pref_ori'] = df_ori0.cuid.map(pref_ori)\n",
    "# plt.savefig( str(unit) + 'ko.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "tmp = fig_inp[(fig_inp.group == 'ko') \n",
    "            & (fig_inp.training == 'post')\n",
    "#             & (fig_inp.fit_err < 0.5) \n",
    "             ]\n",
    "for unit in tmp.cluster_id.unique()[:]:\n",
    "    data = tmp[tmp.cluster_id == unit]\n",
    "#     ax = plt.subplot(111, projection='polar')\n",
    "    fr = data.Hz.values\n",
    "    pref_dir = data.pref.iloc[0]\n",
    "    angles = data.dir.values\n",
    "    angles = (angles - pref_dir + 360)%360\n",
    "    idx = np.argsort(angles)\n",
    "    ls.append(fr[idx]) \n",
    "    a = np.deg2rad(angles) \n",
    "#     print pref_dir\n",
    "#     ax.set_theta_zero_location(\"N\")\n",
    "#     ax.set_theta_direction(-1)\n",
    "#     fr = np.append(fr, fr[0])\n",
    "#     a = np.append(a, a[0])\n",
    "#     ax.plot(a, fr, 'gray')\n",
    "#     plt.show()\n",
    "print len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = np.mean(ls, axis = 0)\n",
    "std = sstat.sem(ls, axis = 0)\n",
    "\n",
    "angles = np.arange(0, 350, 30)\n",
    "a = np.deg2rad(angles)\n",
    "a = np.append(a, a[0])\n",
    "fr = np.append(fr, fr[0])\n",
    "std = np.append(std, std[0])\n",
    "std1 = fr - std\n",
    "std2 = fr + std\n",
    "ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "ax.plot(a, fr, 'magenta')\n",
    "ax.fill_between(a, std1, std2, alpha=0.2, color = 'magenta')\n",
    "ax.set_ylim(0, 30)\n",
    "# plt.savefig( 'polar_sert_ko_12-dir-post.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(fig_inp.groupby('cluster_id').mean().fit_err.values)\n",
    "plt.axvline(x = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp[fig_inp.stim1 == 0].groupby(['group', 'n_type']).cluster_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = fig_inp[(fig_inp.fit_err < 1) & (fig_inp.depth > -1 )].groupby(['group', 'training' ,\n",
    "                                                    'layer' , 'n_type' ,'cluster_id']).mean().reset_index()\n",
    "tmp = tmp.sort_values(by = ['training', 'group'], ascending= [False, False])\n",
    "\n",
    "ax = sns.catplot(x = \"n_type\", y = \"cv_ori\", hue = 'group' , data = tmp,  kind = 'point', ci = 68, \n",
    "                  palette = colors_group, hue_order = ['wt', 'het', 'ko'],\n",
    "#                  col = 'layer', col_wrap = 1,\n",
    "                    height = 5, aspect = 1.5\n",
    "                    )\n",
    "# plt.savefig( 'bar_12dir_sigma_pre.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'cv_ori'\n",
    "x = tmp[tmp.group == 'wt'][val].dropna().values\n",
    "x2 = tmp[tmp.group == 'het'][val].dropna().values\n",
    "x3 = tmp[tmp.group == 'ko'][val].dropna().values\n",
    "\n",
    "print np.mean(x), sstat.sem(x)\n",
    "print np.mean(x2), sstat.sem(x2)\n",
    "print np.mean(x3), sstat.sem(x3)\n",
    "\n",
    "print len(x), len(x2), len(x3)\n",
    "print sstat.kruskal(x, x2, x3)\n",
    "print sstat.mannwhitneyu(x, x2)\n",
    "print sstat.mannwhitneyu(x, x3)\n",
    "print sstat.mannwhitneyu(x2, x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = fig_inp.groupby(['group', 'training' ,'cluster_id']).mean().reset_index().groupby(['group',\n",
    "                                                       'training', 'pref']).count().reset_index()\n",
    "tmp = tmp.sort_values(by = ['group', 'training'], ascending= [False, False])\n",
    "\n",
    "ax = sns.catplot(x = \"pref\", y = \"stim1\",   data = tmp,  kind = 'point', ci = 68, \n",
    "                  palette = colors_group, hue = 'group',  \n",
    "                 hue_order = ['wt', 'het', 'ko'],\n",
    "#                  col = 'group', col_wrap = 1,\n",
    "                    size = 5, aspect = 1.5\n",
    "                    )\n",
    "# plt.savefig('bar-prefsf_count_post.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit DOG to SF tuning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_of_gaus(x, base, k_exc, mu_exc, sigma_exc, k_inh, mu_inh, sigma_inh):\n",
    "    y_hat = []\n",
    "    for idx in range(x.size):\n",
    "\n",
    "        y_hat.append( base + k_exc * np.exp( - ((x[idx] - mu_exc)**2)/(2*(sigma_exc**2))) - \n",
    "                     k_inh * np.exp( -((x[idx] - mu_inh)**2)/(2*(sigma_inh**2))  ))\n",
    "        \n",
    "    return np.array(y_hat)\n",
    "\n",
    "def exc_comp(x, base, k_exc, mu_exc, sigma_exc):\n",
    "    y_hat = []\n",
    "    for idx in range(x.size):\n",
    "        y_hat.append( base + k_exc * np.exp( - ((x[idx] - mu_exc)**2)/(2*(sigma_exc**2))))\n",
    "    return np.array(y_hat)\n",
    "    \n",
    "def inh_comp(x, base, k_inh, mu_inh, sigma_inh):\n",
    "    y_hat = []\n",
    "    for idx in range(x.size):\n",
    "        y_hat.append( k_inh * np.exp( -((x[idx] - mu_inh)**2)/(2*(sigma_inh**2))  ))\n",
    "    return np.array(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsfs(data):\n",
    "    lsf_fr = data[data.stim1 == 0].Hz\n",
    "    pref_fr = data.Hz.max()\n",
    "    data['lsfs'] = lsf_fr/pref_fr\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.625, 0.125, 0.25, 0.5, 1])\n",
    "d_k_exc = {}\n",
    "d_mu_exc = {}\n",
    "d_sigma_exc = {}\n",
    "d_k_inh = {}\n",
    "d_mu_inh = {}\n",
    "d_sigma_inh = {}\n",
    "d_fit_error = {}\n",
    "x = np.array([7.5e-3, 0.015, 0.03, 0.06, 0.12, 0.24])\n",
    "x_new = np.arange(7.5e-3, 0.24, 0.015)\n",
    "for unit in fig_inp[fig_inp.group != 't'].cluster_id.unique()[:]:\n",
    "    tmp = fig_inp[fig_inp.cluster_id == unit]\n",
    "#     x  = tmp.stim1.values.astype('float')\n",
    "    y = tmp.Hz\n",
    "    m = tmp.Hz.max()\n",
    "    try:\n",
    "        init_params = [1, 1, 0.01, 0.01, 0.01, 0.01, 0.01 ]\n",
    "        bound_params = ([0, 0, 0, 0, 0, 0, 0], [m*2, m*2, 1, 1, m*2, 1, 1])\n",
    "        popt, pcov = curve_fit(diff_of_gaus, x, y, bounds = bound_params , p0 = init_params)\n",
    "        perr = np.sqrt(np.diag(pcov_double))\n",
    "\n",
    "        fit = diff_of_gaus(x, *popt)\n",
    "        fit_exc = exc_comp(x, *popt[:4] )\n",
    "        fit_inh = inh_comp(x, popt[0], *popt[4:])\n",
    "        fit_error = sum((y-fit)**2)  /  sum((y-y.mean())**2)\n",
    "        perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "        d_k_exc[unit] = popt[1]\n",
    "        d_mu_exc[unit] = popt[2]\n",
    "        d_sigma_exc[unit] = popt[3]\n",
    "        d_k_inh[unit] = popt[4]\n",
    "        d_mu_inh[unit] = popt[5]\n",
    "        d_sigma_inh[unit] = popt[6]\n",
    "        d_fit_error[unit] =  round(fit_error, 3)\n",
    "        \n",
    "    except:\n",
    "        print unit\n",
    "        continue\n",
    "    \n",
    "\n",
    "#     print 's_exc', round(popt[3], 3)\n",
    "#     print 's_inh', round(popt[6], 3)\n",
    "#     f, ax = plt.subplots()\n",
    "#     sns.despine(ax = ax)\n",
    "\n",
    "#     f2 = interp1d(x, fit, kind = 'quadratic')\n",
    "#     f3 = interp1d(x, fit_exc, kind = 'quadratic')\n",
    "#     f4 = interp1d(x, fit_inh, kind = 'quadratic')\n",
    "#     ax.plot(x, y, 'ko-')\n",
    "#     ax.plot(x_new, f2(x_new), 'r-')\n",
    "    \n",
    "#     ax.plot(x_new, f3(x_new), 'r--')\n",
    "#     ax.plot(x_new, f4(x_new), 'b--')\n",
    "#     plt.show()\n",
    "    \n",
    "fig_inp['k_exc'] = fig_inp.cluster_id.map(d_k_exc)\n",
    "fig_inp['mu_exc'] = fig_inp.cluster_id.map(d_mu_exc)\n",
    "fig_inp['s_exc'] = fig_inp.cluster_id.map(d_sigma_exc)\n",
    "fig_inp['k_inh'] = fig_inp.cluster_id.map(d_k_inh)\n",
    "fig_inp['mu_inh'] = fig_inp.cluster_id.map(d_mu_inh)\n",
    "fig_inp['s_inh'] = fig_inp.cluster_id.map(d_sigma_inh)\n",
    "fig_inp['fit_err'] = fig_inp.cluster_id.map(d_fit_error)\n",
    "\n",
    "# plt.savefig(str(unit) + 'sf_wt.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp = fig_inp.groupby('cluster_id').apply(lsfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp['fwhm'] = 2.355*fig_inp['s_exc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'fwhm'\n",
    "tmp = fig_inp[(fig_inp.fit_err < 0.7) & (fig_inp.depth > -1 )].groupby(['group',\n",
    "                                            'cluster_id']).mean().reset_index()\n",
    "\n",
    "x = tmp[tmp.group == 'wt'][val].dropna().values\n",
    "x2 = tmp[tmp.group == 'het'][val].dropna().values\n",
    "x3 = tmp[tmp.group == 'ko'][val].dropna().values\n",
    "\n",
    "\n",
    "sns.kdeplot(x, cumulative=True,  color = colors_group[0])\n",
    "sns.kdeplot(x2, cumulative=True,  color = colors_group[1])\n",
    "sns.kdeplot(x3, cumulative=True,  color = colors_group[2])\n",
    "sns.despine()\n",
    "plt.savefig('cdf-fwhm_sert-sf-post.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(x = 'group', y = val, data  = tmp,  palette=colors_group, \n",
    "              order = ['wt', 'het', 'ko'], height = 5)\n",
    "sns.despine()\n",
    "plt.savefig('point-fwhm_sert-sf-post.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.mean(x), sstat.sem(x)\n",
    "print np.mean(x2), sstat.sem(x2)\n",
    "print np.mean(x3), sstat.sem(x3)\n",
    "\n",
    "print len(x), len(x2), len(x3)\n",
    "\n",
    "print sstat.kruskal(x, x2, x3)\n",
    "print sstat.mannwhitneyu(x, x2)\n",
    "print sstat.mannwhitneyu(x, x3)\n",
    "print sstat.mannwhitneyu(x2, x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp[fig_inp.cluster_id == '244et009post']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit CRF to contrast tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_ratio(x, max_fr, c50, n):\n",
    "    y_hat = []\n",
    "    for idx in range(x.size):\n",
    "        y_hat.append(max_fr*(x[idx]**n/(x[idx]**n + c50**n)))  \n",
    "    return np.array(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.625, 0.125, 0.25, 0.5, 1])\n",
    "d_c50 = {}\n",
    "d_exp = {}\n",
    "d_fit_error = {}\n",
    "\n",
    "for unit in fig_inp[fig_inp.group != 't'].cluster_id.unique()[:]:\n",
    "    tmp = fig_inp[fig_inp.cluster_id == unit]\n",
    "    x  = tmp.stim1.values.astype('float')\n",
    "    y = tmp.norm\n",
    "    try:\n",
    "        init_params = [1, 0.2, 2 ]\n",
    "        bound_params = ([0, 0, 0], [1.5, 1, 10])\n",
    "        popt, pcov = curve_fit(h_ratio, x, y, bounds = bound_params , p0 = init_params)\n",
    "        perr = np.sqrt(np.diag(pcov_double))\n",
    "\n",
    "        fit = h_ratio(x, *popt)\n",
    "        fit_error = sum((y-fit)**2)  /  sum((y-y.mean())**2)\n",
    "        perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "        d_c50[unit] =  round(popt[1], 3)\n",
    "        d_exp[unit] =  round(popt[-1], 3)\n",
    "        d_fit_error[unit] =  round(fit_error, 3)\n",
    "        \n",
    "#         print 'c50', round(popt[1], 3)\n",
    "#         print 'n', round(popt[-1], 3)\n",
    "#         print 'fit err', round(fit_error, 3)\n",
    "    except:\n",
    "        continue\n",
    "#     if fit_error < 0.7:\n",
    "#         f, ax = plt.subplots()\n",
    "#         sns.despine(ax = ax)\n",
    "\n",
    "#         f2 = interp1d(x, fit, kind = 'quadratic')\n",
    "#         ax.plot(x, y, 'mo-')\n",
    "#         ax.plot(x, f2(x), 'r')\n",
    "#         plt.show()\n",
    "    \n",
    "fig_inp['c50'] = fig_inp.cluster_id.map(d_c50)\n",
    "fig_inp['n_exp'] = fig_inp.cluster_id.map(d_exp)\n",
    "fig_inp['fit_err'] = fig_inp.cluster_id.map(d_fit_error)\n",
    "\n",
    "# plt.savefig(str(unit) + 'ko.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'n_exp'\n",
    "tmp = fig_inp[(fig_inp.fit_err < 0.7) & (fig_inp.depth > -1 )].groupby(['group',\n",
    "                                            'cluster_id']).mean().reset_index()\n",
    "\n",
    "x = tmp[tmp.group == 'wt'][val].dropna().values\n",
    "x2 = tmp[tmp.group == 'het'][val].dropna().values\n",
    "x3 = tmp[tmp.group == 'ko'][val].dropna().values\n",
    "\n",
    "sns.kdeplot(x, cumulative=True,  color = colors_group[0])\n",
    "sns.kdeplot(x2, cumulative=True,  color = colors_group[1])\n",
    "sns.kdeplot(x3, cumulative=True,  color = colors_group[2])\n",
    "sns.despine()\n",
    "# plt.savefig('cdf-c50_sert-contrast-pre.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.mean(x), sstat.sem(x)\n",
    "print np.mean(x2), sstat.sem(x2)\n",
    "print np.mean(x3), sstat.sem(x3)\n",
    "\n",
    "print len(x), len(x2), len(x3)\n",
    "print sstat.kruskal(x, x2, x3)\n",
    "print sstat.mannwhitneyu(x, x2)\n",
    "print sstat.mannwhitneyu(x, x3)\n",
    "print sstat.mannwhitneyu(x2, x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\SERT KO paper\\sert_ko_spikes_probe64da.pkl\"\n",
    "df_spikes_da = pd.read_pickle(path)\n",
    "\n",
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\SERT KO paper\\sert_ko_spikes_probe64db.pkl\"\n",
    "df_spikes_db = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recs with 8 drifitng instead of 12 drifting, 25 trials 8 directions (45 deg)\n",
    "rec_dir8 = df_spikes_db[df_spikes_db.times > 560].path.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recs with 8 drifitng instead of 12 drifting, 25 trials 8 directions (45 deg)\n",
    "\n",
    "tmp = df_spikes_db\n",
    "rec_dir8 = tmp[tmp.times > 560].path.unique()\n",
    "\n",
    "tmp2 = tmp[tmp.path.isin(rec_dir8)]\n",
    "# tmp2 = df_spikes[df_spikes.path.isin(rec_dir8)]\n",
    "\n",
    "# 15 trials, 12 directions, 1s trial length\n",
    "# df3 = tmp[(tmp.loc[:, 'times'] > 160) & (tmp.loc[:, 'times'] < 340)] \n",
    "# df3.times = df3.times - 160\n",
    "# df3.loc[:, 'trial_n'] = df3.loc[:, 'times'] //1.0\n",
    "# df3.loc[:,'paradigm'] = '12-drifting'\n",
    "# df3.loc[:,'stim1'] = df3.trial_n.map(d_dir12)\n",
    "\n",
    "# 25 trials, 8 directions, 1s trial length\n",
    "df4 = tmp2[(tmp2.loc[:, 'times'] > 160) & (tmp2.loc[:, 'times'] < 360)] \n",
    "df4.times = df4.times - 160\n",
    "df4.loc[:, 'trial_n'] = df4.loc[:, 'times'] //1.0\n",
    "df4.loc[:,'paradigm'] = '8-drifting'\n",
    "df4.loc[:,'stim1'] = df4.trial_n.map(d_dir8)\n",
    "\n",
    "# 20 trials, 6 sf, 1s trial length\n",
    "# df5 = tmp[(tmp.loc[:, 'times'] > 340) & (tmp.loc[:, 'times'] < 460)] \n",
    "# df5.times = df5.times - 340\n",
    "# df5.loc[:, 'trial_n'] = df5.loc[:, 'times'] //1.0\n",
    "# df5.loc[:,'paradigm'] = 'sf-tuning'\n",
    "# df5.loc[:,'stim1'] = df5.trial_n.map(d_sf)\n",
    "\n",
    "df6 = tmp2[(tmp2.loc[:, 'times'] > 360) & (tmp2.loc[:, 'times'] < 480)] \n",
    "df6.times = df6.times - 360\n",
    "df6.loc[:, 'trial_n'] = df6.loc[:, 'times'] //1.0\n",
    "df6.loc[:,'paradigm'] = 'sf-tuning'\n",
    "df6.loc[:,'stim1'] = df6.trial_n.map(d_sf)\n",
    "df6 = pd.concat([df5, df6])\n",
    "\n",
    "# 20 trials, 5 contasrts, 1s trial length\n",
    "# df7 = tmp[(tmp.loc[:, 'times'] > 460) & (tmp.loc[:, 'times'] < 560)] \n",
    "# df7.times = df7.times - 460\n",
    "# df7.loc[:, 'trial_n'] = df7.loc[:, 'times'] //1.0\n",
    "# df7.loc[:,'paradigm'] = 'contrast-tuning'\n",
    "# df7.loc[:,'stim1'] = df7.trial_n.map(d_contrast)\n",
    "\n",
    "df8 = tmp2[(tmp2.loc[:, 'times'] > 480) & (tmp2.loc[:, 'times'] < 580)] \n",
    "df8.times = df8.times - 480\n",
    "df8.loc[:, 'trial_n'] = df8.loc[:, 'times'] //1.0\n",
    "df8.loc[:,'paradigm'] = 'contrast-tuning'\n",
    "df8.loc[:,'stim1'] = df8.trial_n.map(d_contrast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_spikes['group'] = master_spikes.et.map(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_spikes2.to_pickle('sert_spikes_master_mapped.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_spikes2 = pd.concat([df_wt, master_spikes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, train_test_split, RepeatedStratifiedKFold, cross_val_score, StratifiedKFold\n",
    "from collections import defaultdict\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = master_spikes2[master_spikes2.paradigm == '12-drifting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sp.loc[:, 'dir'] = df_sp.stim1*30\n",
    "df_sp.loc[:,'ori'] = df_sp['dir']%180\n",
    "df_sp.loc[:,'trial_spikes'] = df_sp.times - df_sp.trial_n\n",
    "df_sp = df_sp[(df_sp.trial_spikes > 0.35) & (df_sp.trial_spikes < 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(d_dir12.values())*30%180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver = 'lsqr', shrinkage = 'auto')\n",
    "# kf = KFold(n_splits = 4)\n",
    "ls_cond = []\n",
    "rskf = RepeatedStratifiedKFold(n_splits = 4, n_repeats = 5,\n",
    "     random_state = 3)\n",
    "df_sp.loc[:,'trial_spikes'] = df_sp.times - df_sp.trial_n\n",
    "_data = df_sp\n",
    "y_ = np.array(d_dir12.values())*30%180\n",
    "\n",
    "for gr in sorted(_data.group.unique()):\n",
    "    for tr in (_data.training.unique()):\n",
    "        \n",
    "        df2_sub = _data[(_data.training == tr) & (_data.group == gr)]\n",
    "        resp_units = df2_sub.groupby('cluster_id').times.count().reset_index()\n",
    "        resp_units = resp_units[resp_units.times > 10].cluster_id.unique()\n",
    "        tmp = df2_sub[df2_sub.cluster_id.isin(resp_units)].groupby(['cluster_id', \n",
    "                                                'trial_n']).trial_spikes.count().reset_index()\n",
    "        tmp2 = tmp.pivot('trial_n', 'cluster_id', 'trial_spikes')\n",
    "        tmp2 = tmp2.fillna(0)\n",
    "\n",
    "        ls = []\n",
    "    # coef_arr = np.zeros((100,5, _data.cuid.unique().size ))\n",
    "        print tr, gr\n",
    "\n",
    "        x_ = tmp2.values\n",
    "        print x_.shape\n",
    "\n",
    "        scores = cross_val_score(lda, x_, y_, cv = rskf)\n",
    "    #         print scores\n",
    "        tmp_df = pd.DataFrame({'cv': i, 'group': gr, 'training': tr, 'acc': scores})\n",
    "        ls_cond.append(tmp_df)\n",
    "out = pd.concat(ls_cond)\n",
    "out.loc[:,'error'] = 1 - out.acc\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = 'training', y = 'acc', data = out, hue = 'group', \n",
    "            kind = 'bar', height = 5, aspect = 1.5, \n",
    "            hue_order = ['wt', 'het', 'ko'], order = ['pre', 'post'],\n",
    "            palette = colors_group , ci = 95)\n",
    "plt.ylim(0, 1)\n",
    "# plt.savefig('bar-sert-ctr-decod-lda.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.groupby(['group', 'training']).acc.agg(['mean', 'sem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveform split RS/FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resampy\n",
    "import scipy.optimize as opt\n",
    "def gaus(x,a,x0,sigma):\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_tmt_ms\n",
    "samples = np.concatenate([np.arange(82)] * result.cluster_id.unique().size)\n",
    "result['samples'] = samples\n",
    "spk_width = {}\n",
    "d_fwhm = {}\n",
    "tr2peak = {}\n",
    "neuron_type = {}\n",
    "ls = []\n",
    "ls2 = []\n",
    "ls3 = []\n",
    "# f, ax = plt.subplots(1,2)\n",
    "for ii in result.cluster_id.unique()[:]:\n",
    "#trough-to-peak    \n",
    "    #tp =  result[(result['cuid'] == ii)  & (result.index > 18)  ].tmt.idxmax() - result[(result['cuid'] == ii)  & (result.index == 18)  ].index\n",
    "    tmt_data = np.array(result[(result['cluster_id'] == ii) ].tmt)\n",
    "\n",
    "    y = resampy.resample( tmt_data[::-1] , 1 ,10,  filter='sinc_window',\n",
    "                                    num_zeros=10, precision=5,\n",
    "                                    window=ssig.hann)\n",
    "    trough_idx = y.argmin()\n",
    "    peak_idx = y[:y.argmin()].argmax()\n",
    "#     plt.plot(y)\n",
    "#     plt.axvline(x= trough_idx, color = 'k', linestyle = '--')\n",
    "#     plt.axvline(x= peak_idx, color = 'r',linestyle = '--')\n",
    "#     plt.axvline(x= peak_idx-100, color = 'b',linestyle = '--')\n",
    "#     plt.show()\n",
    "\n",
    "    tp = abs((trough_idx - peak_idx)/300.0)\n",
    "    \n",
    "    x = np.arange(y.size)\n",
    "    y_gaus = y*(-1)\n",
    "    popt,pcov = opt.curve_fit(gaus,x,y_gaus,p0=[0.2, y.argmin(), 10])\n",
    "    fwhm = popt[-1]/300*2.355\n",
    "    \n",
    "#     plt.plot(x,y*(-1),'b+:')\n",
    "#     plt.plot(x,gaus(x,*popt),'r--')\n",
    "#     plt.show()\n",
    "\n",
    "    f,pxx = ssig.welch(tmt_data, fs=3e4,  nfft=5096,  nperseg=48,\n",
    "                          return_onesided=True, scaling='spectrum')\n",
    "\n",
    "    df = np.vstack((f, pxx))\n",
    "    df = pd.DataFrame(df)\n",
    "    idx = df.T[1].idxmax()\n",
    "    w = df.T[0][idx]\n",
    "    w = 1/w*1000.0\n",
    "\n",
    "    ls2.append(w)\n",
    "    ls.append(tp)\n",
    "    \n",
    "    spk_width[ii] = w\n",
    "    tr2peak[ii] = tp\n",
    "    \n",
    "    if tp < 0.45 and w < 1.2:\n",
    "        neuron_type[ii] = 'fs'\n",
    "    elif tp > 0.45 and w > 1.2:\n",
    "        neuron_type[ii] = 'rs'\n",
    "    else:\n",
    "        neuron_type[ii] = 'un'\n",
    "\n",
    "#p/t ratio\n",
    "#     edge = 100\n",
    "#     if peak_idx < 100:\n",
    "#         edge = peak_idx\n",
    "#     y_slope = y[peak_idx-edge:peak_idx]\n",
    "#     x_slope = np.arange(y_slope.size)\n",
    "#     slope, intercept, r_value, p_value, std_err = sstat.linregress(x_slope, y_slope)\n",
    "#     ls3.append(1e3*(slope))\n",
    "    \n",
    "#     plt.plot(x_slope, y_slope, 'o')\n",
    "#     plt.plot(x_slope, intercept + slope*x_slope, 'r')\n",
    "#     plt.show()\n",
    "result['n_type'] = result.cluster_id.map(neuron_type)\n",
    "result['sp_w'] = result.cluster_id.map(spk_width)\n",
    "result['fwhm'] = result.cluster_id.map(d_fwhm)\n",
    "result['tp'] = result.cluster_id.map(tr2peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = result.cluster_id.unique().size\n",
    "print total\n",
    "print result[result['samples'] == 0].groupby('n_type').cluster_id.count()/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result.n_type == 'rs'].groupby('samples').tmt.mean().plot()\n",
    "result[result.n_type == 'fs'].groupby('samples').tmt.mean().plot()\n",
    "result[result.n_type == 'un'].groupby('samples').tmt.mean().plot()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (sns.jointplot(np.array(ls2), np.array(ls), stat_func= None,\n",
    "             color=\"k\", s = 20)\n",
    "       .plot_joint(sns.kdeplot, zorder=0, n_levels=6))\n",
    "g.set_axis_labels('spike width ', 't-p width [ms]')\n",
    "# plt.axhline(y= 0.45, linestyle = '--')\n",
    "plt.axvline(x= 1.2, linestyle = '--')\n",
    "plt.axhline(y= 0.45, linestyle = '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tkr\n",
    "from PyQt5.QtWidgets import QApplication, QFileDialog\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype']=42\n",
    "import fnmatch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%precision 4\n",
    "mpl.rcParams['pdf.fonttype'] = 42 \n",
    "import seaborn as sns\n",
    "import scipy.signal as sg\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_style(\"ticks\")\n",
    "from collections import Counter\n",
    "#import OpenEphys2 as OE\n",
    "#import OpenEphys_V12 as OE\n",
    "# 06/30/15 - version 1_01\n",
    "import OpenEphys_V14 as OE\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import scipy.stats as sstat\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import mne\n",
    "from scipy.signal import iirfilter, lfilter\n",
    "import sys\n",
    "import fnmatch\n",
    "%matplotlib inline\n",
    "mpl.rcParams['pdf.fonttype'] = 42 \n",
    "mpl.rcParams['font.sans-serif']=['Arial', 'Helvetica','Bitstream Vera Sans', 'DejaVu Sans', 'Lucida Grande', \n",
    "                                 'Verdana', 'Geneva', 'Lucid', 'Avant Garde', 'sans-serif']  \n",
    "\n",
    "# pal=sns.blend_palette([\"black\", \"crimson\"], 2)\n",
    "sns.despine()\n",
    "# current_palette = sns.color_palette(\"colorblind\", 10)\n",
    "# sns.set_palette(current_palette)\n",
    "\n",
    "# for publication quality plots, not bar graphs, use this: \n",
    "def set_pub_plots(pal=sns.blend_palette([    \"gray\", \"crimson\", 'magenta', 'cyan', 'purple' ],5)):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_palette(pal)\n",
    "    sns.set_context(\"poster\", font_scale=1.5, rc={\"lines.linewidth\": 2.5, \"axes.linewidth\":2.5}) \n",
    "    sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8, 'figure.facecolor': 'white',})\n",
    "    # optional, makes markers bigger, too, axes.linewidth doesn't seem to work\n",
    "    plt.rcParams['axes.linewidth'] = 2.5\n",
    "\n",
    "rc_pub={'font.size': 25, 'axes.labelsize': 25, 'legend.fontsize': 25.0, \n",
    "    'axes.titlesize': 25, 'xtick.labelsize': 25, 'ytick.labelsize': 25, \n",
    "    #'axes.color_cycle':pal, # image.cmap - rewritesd the default colormap\n",
    "    'axes.linewidth':2.5, 'lines.linewidth': 2.5,\n",
    "    'xtick.color': 'black', 'ytick.color': 'black', 'axes.edgecolor': 'black','axes.labelcolor':'black','text.color':'black'}\n",
    "# to restore the defaults, call plt.rcdefaults() \n",
    "\n",
    "#set_pub_bargraphs()\n",
    "set_pub_plots()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore the details of these functions for now\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=3):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = sg.butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=3):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = sg.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genotype = {\n",
    "#     137:3, 138:1, 473:1, 474:3, 470:3, 471:1, 472:1, 231:2, 232:2, 352:1, 52:1, 53:1, 54:1, 55:2, 56:2, 57:2,\n",
    "#     173:3, 174:3, 175:3, 283:3, 285:3, 286:2, 287:2\n",
    "# }\n",
    "genotype = {\n",
    "    '137':'wt', '138':'ko', '473':'ko', '474':'wt', '470':'wt', '471':'ko', '472':'ko', '231':'het', '232':'het', \n",
    "    '173':'wt', '174':'wt', '175':'wt', '283':'wt', '285':'wt', '286':'het', '287':'het', '383':'wt', '273':'wt', \n",
    "    '420':'het', '421':'ko', '379':'het', '380':'het', '381':'ko', '382':'wt'\n",
    "}\n",
    "sert_groups = { '002': 'ko', '003': 'ko', '005': 'wt', '006': 'het', '007':'het', '008': 'het', '009':'ko', '010': 'ko', \n",
    "    '367': 'wt', '369': 'ko', '377': 'het', '378': 'ko', '380': 'ko', '488': 'wt', '489': 'ko', \n",
    "           '930': 'ko', '931': 'het', '936': 'het', '937': 'ko', '942': 'wt', '944': 'het', '958': 'ko', '959': 'het'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oe_map_functions_v1 as oem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pkl files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_keyphrases=['Bad Reports','bad', 'MSF', 'phy']\n",
    "directory = r\"U:\\Data\\pak6\\figures\\LFP\\probe_64DA\\SERT KO\\WT\"\n",
    "matches = [os.path.join(dirpath, f)\n",
    "            for dirpath, dirnames, files in os.walk(directory)\n",
    "            for f in files \n",
    "            if (f.endswith('.h5') and not any(filter(lambda bad: bad in f, exclude_keyphrases)))]\n",
    "matches = [f for f in matches if 'bad' not in f and '201' in f]\n",
    "matches[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fname in matches[:]: \n",
    "    et = fname.split('\\\\')[-2].split()[0]\n",
    "    training =  fname.split('\\\\')[-1].split()[1]\n",
    "    rec = fname.split('\\\\')[-1].split()[-1].split('_')[1]\n",
    "    stim1 =  fname.split('\\\\')[-1].split('_')[3][:3]\n",
    "#     training = fname.split('\\\\')[-2].split()[-1]\n",
    "#     if 'pre' in training or '1' in training:\n",
    "#         training = 'pre'\n",
    "#     else:\n",
    "#         training = 'post'\n",
    "\n",
    "    print et, training, rec, stim1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_2 = [f for f in matches if 'bad' not in f and 'opto' not in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stim1-2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial_length = 4000\n",
    "\n",
    "ls = []\n",
    "\n",
    "for fname in matches[:]:\n",
    "    print fname\n",
    "#     tmp = tmp.groupby(np.arange((tmp.shape[0]))//30, axis = 0).mean() #downsample to 1kHz\n",
    "\n",
    "    et = fname.split('\\\\')[-2].split()[0]\n",
    "    training =  fname.split('\\\\')[-1].split()[1]\n",
    "    rec = fname.split('\\\\')[-1].split()[-1].split('_')[1]\n",
    "    stim1 =  fname.split('\\\\')[-1].split('_')[3][:3]\n",
    "    \n",
    "    if 'pre' in training or '1' in training:\n",
    "        training = 'pre'\n",
    "    else:\n",
    "        training = 'post'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "#         tmp =  pd.read_hdf(fname, key = 'layer4') \n",
    "    except:\n",
    "        print fname\n",
    "        continue\n",
    "   \n",
    "    trials =  int(tmp.shape[1]/trial_length)\n",
    "    \n",
    "    times = np.linspace(0, trial_length/1e3, trial_length)\n",
    "    df3_array=np.reshape(tmp.values,(np.shape(tmp)[0],trials, -1))\n",
    "    df3_avg=np.mean(df3_array,1)\n",
    "    df3_avg = pd.DataFrame(df3_avg).T        \n",
    "    df3_avg.loc[:,'times'] = times\n",
    "    negativity_ch_idx=df3_avg[(df3_avg['times'] >= 1) & (df3_avg['times'] < 1.15)].ix[:,11:16].min().idxmin()\n",
    "#     negativity_ch_idx = negativity_ch_idx - 4\n",
    "    print negativity_ch_idx\n",
    "\n",
    "    # plotting the spectra and FFR for the maximum negativity channel, the first trial\n",
    "    \n",
    "    ddf = df3_avg.iloc[:, negativity_ch_idx]\n",
    "\n",
    "    tmp_df = pd.DataFrame({'et': et, 'lfp':ddf, 'stim1':stim1,  'training': training,  'ch': negativity_ch_idx,\n",
    "                           'fname':fname, 'times':times, 'layer': 'l4', 'rec':rec\n",
    "                          })\n",
    "    ls.append(tmp_df)\n",
    "\n",
    "\n",
    "df_sert_l4 = pd.concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert_l4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert_l5['group'] = 'wt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in df_serta.fname.unique():\n",
    "    plt.plot(df_serta[df_serta.fname == fn].lfp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert_l4.to_pickle('sert_ko_stim1_training_l4-lfp,pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_oddball = np.array([3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, \n",
    "3, 9, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, \n",
    "3, 3, 3, 3, 3, 9, 3, 3, 3, 9, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, \n",
    "3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 9, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, \n",
    "3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3, 3, \n",
    "3, 3, 9, 3, 3, 3, 3, 9, 3, 3, 9, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, \n",
    "3, 9, 3, 3, 3, 3, 3, 9, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, \n",
    "3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3])\n",
    "\n",
    "standard_control = np.array([4, 5, 3, 6, 5, 7, 0, 3, 7, 6, 7, 1, 2, 6, 7, 0, 5, 0, 7, 0, 7, 3, 2, 1, 7, 0, 6, 1, 3, 1, 4, 6, 6, 5, 1, 4, 4, 5, 2, 3, 0, 5, 2, 3, 4, 2, 6, 5, 6, 6, 3, 7, 0, 1, 4, 6, 4, 0, 6, 6, 7, 4, 3, 6, 1, 1, 5, 4, 0, 4, 0, 2, 6, 2, 6, 7, 7, 7, 3, 0, 1, 7, 6, 1, 3, 2, 3, 5, 7, 0, 1, 5, 2, 3, 4, 2, 4, 6, 0, 4, 3, 2, 4, 1, 2, 0, 5, 1, 5, 3, 4, 6, 3, 5, 2, 2, 7, 1, 6, 0, 0, 5, 4, 0, 1, 5, 0, 3, 7, 5, 7, 4, 6, 2, 0, 0, 1, 1, 0, 5, 3, 2, 4, 7, 0, 7, 5, 2, 7, 1, 2, 7, 3, 0, 2, 3, 6, 2, 7, 3, 5, 3, 4, 5, 3, 7, 4, 2, 6, 6, 2, 1, 3, 1, 4, 5, 0, 2, 2, 6, 1, 4, 5, 1, 6, 3, 0, 5, 3, 2, 5, 4, 1, 7, 4, 4, 1, 5, 7, 1])\n",
    "\n",
    "sf_tuning = np.array([2, 0, 0, 4, 3, 3, 0, 4, 4, 3, 2, 5, 3, 2, 0, 4, 1, 0, 5, 2, 0, \n",
    "1, 0, 1, 3, 5, 2, 5, 1, 2, 0, 5, 2, 3, 5, 1, 0, 4, 3, 2, 5, 5, 3, 5, \n",
    "2, 0, 3, 3, 0, 3, 4, 5, 4, 1, 4, 0, 1, 5, 4, 1, 5, 3, 3, 5, 3, 3, 2, \n",
    "3, 2, 1, 1, 5, 1, 4, 1, 2, 3, 2, 4, 2, 1, 0, 5, 5, 2, 2, 4, 1, 4, 1, \n",
    "3, 1, 0, 4, 4, 4, 4, 0, 5, 4, 4, 0, 3, 5, 5, 2, 1, 3, 4, 1, 5, 0, 2, \n",
    "2, 0, 0, 0, 1, 2, 1])\n",
    "\n",
    "loc_omission = np.array([3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, \n",
    "0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, \n",
    "3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, \n",
    "3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, \n",
    "3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, \n",
    "3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, \n",
    "3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, \n",
    "3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0])\n",
    "\n",
    "dir_seq = np.array([10, 7, 3, 2, 4, 8, 9, 5, 7, 3, 4, 8, 3, 2, 1, 8, 0, 4, 9, 11, \n",
    "10, 9, 1, 11, 4, 0, 7, 1, 2, 8, 2, 9, 11, 9, 6, 5, 10, 4, 9, 0, 7, 11, 9, \n",
    "5, 9, 10, 11, 6, 8, 9, 5, 4, 2, 8, 11, 2, 10, 3, 5, 1, 7, 0, 4, 9, 1, 5, \n",
    "11, 3, 5, 10, 1, 2, 9, 6, 2, 2, 11, 5, 10, 7, 3, 7, 4, 6, 8, 4, 1, 8, 0, \n",
    "11, 0, 6, 2, 11, 1, 10, 3, 8, 3, 1, 2, 10, 5, 3, 11, 1, 7, 3, 4, 7, 8, 4, 6, \n",
    "7, 11, 7, 0, 8, 6, 10, 4, 5, 7, 2, 10, 3, 5, 9, 8, 6, 3, 2, 0, 11, 0, 6, 10, \n",
    "0, 7, 4, 5, 0, 10, 6, 8, 10, 3, 11, 9, 0, 5, 1, 3, 7, 0, 6, 9, 1, 6, 10, 5, \n",
    "6, 11, 7, 0, 5, 1, 4, 1, 6, 8, 2, 9, 2, 8, 3, 0, 4, 6, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups = {'ET#304': 1, 'ET#271': 1, 'ET#309': 2, 'ET#270': 2, 'ET#313': 1, 'ET#307': 2}\n",
    "# groups = {'ET#970': 'b', 'ET#971': 'b', 'ET#972': 'b', 'ET#973': 'a', 'ET#974': 'a', 'ET#975': 'a', 'ET#758': 'a',\n",
    "#          'ET#759': 'a'}\n",
    "groups = {'479': 'wt', '480': 'fx', '481':'fx', '482': 'fx', '483': 'wt', '484': 'wt', '485':'wt', '486':'fx', \n",
    "          '013': 'wt', '014': 'fx', '016': 'fx', '017': 'wt', '018': 'fx', '019': 'wt', '020': 'wt', '022': 'fx',\n",
    "         '023': 'wt', '024': 'fx', '025': 'fx', '026': 'fx', '027': 'fx', '028': 'fx', '029': 'wt', '030': 'fx', '031': 'fx', '032': 'fx', \n",
    "          '033': 'fx' , '034': 'fx', '035': 'wt', 'W00': 'wt', 'W01': 'wt',\n",
    " 'W02': 'wt', 'W03': 'wt', 'W04': 'wt', 'W05':'wt'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate pkl for SF-vmmn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls = []\n",
    "\n",
    "for fname in matches[:]:\n",
    "    stim_type = ''\n",
    "    et = fname.split('\\\\')[-1].split()[2][3:]\n",
    "    side = 'left-1'\n",
    "    paradigm = fname.split('\\\\')[-1].split()[-1].split('_')[2]\n",
    "    \n",
    "    if 'bad' in fname or 'sf' not in fname:\n",
    "        continue\n",
    "    print fname\n",
    "\n",
    "    if 'nov' in paradigm:\n",
    "        trial_type = novel_oddball\n",
    "        idx_odd = np.where(trial_type==9)[0]\n",
    "        pre_odd = idx_odd-1\n",
    "        print 'novel oddball'\n",
    "    elif 'omis' in paradigm:\n",
    "        trial_type = loc_omission\n",
    "        idx_odd = np.where(trial_type==0)[0]\n",
    "        pre_odd = idx_odd-1\n",
    "        print 'loc_omission'\n",
    "    elif 'control' in paradigm:\n",
    "        trial_type = standard_control\n",
    "        print 'std control'\n",
    "    elif 'tuning' in paradigm:\n",
    "        trial_type = sf_tuning\n",
    "        print 'tuning'\n",
    "    else:\n",
    "        print 'no paradigm'\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "#         trial_length = pd.read_hdf(fname, key = 'trial_duration') \n",
    "    except:\n",
    "        continue\n",
    "    trial_length = 1000\n",
    "    trials =  int(tmp.shape[1]/trial_length)\n",
    "    \n",
    "    times = np.linspace(0, trial_length/1e3, trial_length)\n",
    "    df3_array=np.reshape(tmp.values,(np.shape(tmp)[0],trials, -1))\n",
    "    df3_avg=np.mean(df3_array,1)\n",
    "    df3_avg = pd.DataFrame(df3_avg).T        \n",
    "    df3_avg['times'] = times\n",
    "    negativity_ch_idx=df3_avg[(df3_avg['times']>=0.35) & (df3_avg['times']< 0.5)].ix[:,8:16].min().idxmin()\n",
    "#     negativity_ch_idx = negativity_ch_idx - 4\n",
    "    print negativity_ch_idx\n",
    "\n",
    "    # plotting the spectra and FFR for the maximum negativity channel, the first trial\n",
    "    \n",
    "    ddf = tmp.values[negativity_ch_idx,:]\n",
    "    \n",
    "    ddf2=np.reshape(ddf,(trials, -1))\n",
    "    print ddf2.shape\n",
    "    ddf2 = pd.DataFrame(ddf2)\n",
    "    \n",
    "    if 'control' in paradigm or 'tuning' in paradigm:\n",
    "        for j in np.unique(trial_type):\n",
    "            stim_type = 'ctr'\n",
    "            lfp = ddf2.iloc[np.where( trial_type == j)[0]].mean()\n",
    "            tmp_df = pd.DataFrame({'et': et, 'lfp':lfp, 'paradigm': paradigm,  'stim1': stim_type + str(j), \n",
    "                        'layer': 'l2/3',\n",
    "                           'side':side,    'fname':fname, 'times':times, 'ch_idx': negativity_ch_idx })\n",
    "            ls.append(tmp_df)\n",
    "    else:\n",
    "        for j in np.unique(trial_type):\n",
    "            if j == 3:\n",
    "                stim_type = 'std'\n",
    "                lfp = ddf2.iloc[pre_odd].mean()\n",
    "            elif j == 9 or j == 0:\n",
    "                stim_type = 'dev'\n",
    "                lfp = ddf2.iloc[idx_odd].mean()\n",
    "            else:\n",
    "                continue\n",
    "    #         if j==3 or j ==9:\n",
    "    #             plt.plot(lfp)\n",
    "    #             plt.show()\n",
    "            tmp_df = pd.DataFrame({'et': et, 'lfp':lfp, 'paradigm': paradigm,  'stim1': stim_type + str(j), \n",
    "                                    'layer': 'l2/3',\n",
    "                   'side':side,  'fname':fname, 'times':times, 'ch_idx': negativity_ch_idx })\n",
    "            ls.append(tmp_df)\n",
    "    \n",
    "mmn_df4 = pd.concat(ls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_df4['group'] = mmn_df4.et.map(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(mmn_df4.group.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pkl file for standard ori vmmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls = []\n",
    "\n",
    "oddball = np.array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, \n",
    "3, 3, 3, 3, 3, 3,\n",
    " 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
    " 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3,\n",
    " 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
    " 3, 3, 9, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
    " 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3, 3,\n",
    " 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3,\n",
    " 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3,\n",
    " 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3,\n",
    " 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3,\n",
    " 9, 3, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3,\n",
    " 3, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3,\n",
    " 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 9, 3,\n",
    " 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 9])\n",
    "\n",
    "control = np.array([8, 12, 4, 9, 4, 10, 12, 8, 2, 10, 4, 9, 9, 4, 3, 2, 10, 2, 4, 10, 8, 4, 10, 10, 12, 3, 10, 2, 3, 10, 9, 2, 8, 6, 2, 10, 12, 9, 9, 2, 6, 4, 10, 12, 12, 12, 3, 6, 4, 4, 12, 6, 3, 6, 4, 10, 3, 3, 6, 3, 8, 3, 12, 6, 9, 12, 10, 3, 8, 8, 12, 6, 9, 9, 3, 10, 2, 6, 10, 4, 12, 9, 2, 10, 9, 4, 3, 2, 9, 12, 2, 8, 8, 8, 12, 8, 9, 2, 8, 6, 10, 8, 10, 2, 8, 9, 8, 9, 9, 3, 4, 2, 8, 4, 2, 3, 4, 8, 12, 4, 6, 10, 12, 9, 8, 8, 3, 2, 6, 6, 3, 10, 4, 10, 2, 9, 10, 6, 3, 10, 8, 12, 9, 3, 8, 4, 2, 8, 4, 8, 12, 4, 6, 12, 10, 10, 9, 4, 12, 10, 3, 6, 2, 9, 4, 12, 2, 12, 3, 2, 6, 9, 6, 8, 2, 4, 3, 2, 6, 9, 2, 8, 12, 6, 10, 6, 9, 6, 6, 8, 2, 4, 9, 3, 9, 6, 12, 10, 2, 12, 3, 8, 6, 3, 3, 12, 3, 12, 6, 9, 6, 12, 3, 8, 4, 9, 12, 2, 2, 2, 9, 10, 12, 8, 4, 4, 6, 10, 10, 6, 9, 8, 3, 3, 4, 4, 3, 4, 2, 6])\n",
    "\n",
    "control_old = np.array([10,  2, 10,  8,  9,  9,  2,  2, 10,  4,  3,  2, 10,  4,  9,  3,  3,\n",
    "        4,  9,  2,  8,  9,  3, 10,  2,  4,  8, 10,  9,  9,  3, 10,  2,  2,\n",
    "        2,  4,  2,  3,  8,  8,  3,  3,  4,  2, 10, 10, 10,  3,  3,  8,  8,\n",
    "        4,  4,  9,  2,  9, 10,  8, 10,  8, 10,  8,  3,  2,  3,  9,  4,  3,\n",
    "        4,  3,  2,  2, 10,  4,  8,  8,  2,  8,  9,  9,  4,  2, 10,  3,  9,\n",
    "        3,  2, 10,  3,  2,  3,  8,  4,  9,  3, 10,  9,  4, 10,  8,  2, 10,\n",
    "        8, 10,  4,  4,  4,  2, 10,  9,  4, 10,  9,  9,  9,  4,  3, 10,  3,\n",
    "       10, 10,  9,  8,  4,  4,  2,  4,  3,  2,  9, 10,  2,  9,  3, 10,  8,\n",
    "        8,  4,  4,  3,  2,  3,  4, 10,  9, 10,  2,  8, 10,  9,  8, 10,  2,\n",
    "        4, 10,  2,  8,  9,  2,  8,  3, 10,  8, 10,  4,  4,  8,  4,  8,  9,\n",
    "        3,  9,  2,  3,  4,  3,  4,  3,  8,  8,  3,  9,  3,  2,  9,  4, 10,\n",
    "       10,  9,  4,  2,  4,  3,  2,  8,  3,  8,  8, 10,  9,  2,  3,  9,  4,\n",
    "        4, 10,  4,  4,  2,  4,  8,  9,  8, 10,  8,  2,  8,  3,  2,  2,  2,\n",
    "        8,  3, 10,  3,  8,  3,  2, 10,  9,  3,  9, 10,  2,  3,  8,  8,  9,\n",
    "        9,  9,  8,  9,  9,  9,  9,  4,  4,  2,  4,  2,  4,  9, 10,  4,  3,\n",
    "        3,  8,  8, 10,  8,  8,  8,  3,  9,  9, 10,  8,  3, 10, 10,  2,  9,\n",
    "        4,  8,  8,  2,  3,  9,  2,  9, 10,  3, 10,  4,  3,  3,  4, 10,  2,\n",
    "        4,  9,  4,  8,  9,  2,  8,  2,  4,  3,  2])\n",
    "\n",
    "for fname in matches:\n",
    "    paradigm = fname.split('\\\\')[-1].split()[-1].split('_')[2]\n",
    "    exp = fname.split('\\\\')[-1].split()[-1].split('_')[1]\n",
    "    et = fname.split('\\\\')[-1].split()[0]\n",
    "    rec = fname.split('\\\\')[-1].split()[-1].split('_')[0]\n",
    "    \n",
    "#     if 'oddball' not in paradigm:\n",
    "#         continue\n",
    "    print fname\n",
    "\n",
    "    if 'odd' in paradigm:\n",
    "        trial_type = oddball\n",
    "        idx_odd = np.where(trial_type==9)[0]\n",
    "        pre_odd = idx_odd-1\n",
    "        print 'ori oddball'\n",
    "    elif 'control' in paradigm:\n",
    "        trial_type = control\n",
    "        print 'control'\n",
    "    else:\n",
    "        print 'no paradigm'\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "#         trial_length = pd.read_hdf(fname, key = 'trial_duration') \n",
    "    except:\n",
    "        continue\n",
    "    trial_length = 1000\n",
    "    trials =  int(tmp.shape[1]/trial_length)\n",
    "    if trials != 300:\n",
    "        continue\n",
    "    else:\n",
    "        print 'good'\n",
    "    times = np.linspace(0, trial_length/1e3, trial_length)\n",
    "    df3_array=np.reshape(tmp.values,(np.shape(tmp)[0],trials, -1))\n",
    "    df3_avg=np.mean(df3_array,1)\n",
    "    df3_avg = pd.DataFrame(df3_avg).T        \n",
    "    df3_avg['times'] = times\n",
    "    negativity_ch_idx=df3_avg[(df3_avg['times']>=0.35) & (df3_avg['times']< 0.45)].ix[:,8:16].min().idxmin()\n",
    "#     negativity_ch_idx = negativity_ch_idx - 4\n",
    "    print negativity_ch_idx\n",
    "\n",
    "    # plotting the spectra and FFR for the maximum negativity channel, the first trial\n",
    "    \n",
    "    ddf = tmp.values[negativity_ch_idx,:]\n",
    "    \n",
    "    ddf2=np.reshape(ddf,(trials, -1))\n",
    "    print ddf2.shape\n",
    "    ddf2 = pd.DataFrame(ddf2)\n",
    "    \n",
    "    if 'control' in paradigm:\n",
    "        for j in np.unique(trial_type):\n",
    "            lfp = ddf2.iloc[np.where( trial_type == j)[0]].mean()\n",
    "            tmp_df = pd.DataFrame({'et': et, 'lfp':lfp, 'paradigm': paradigm,  'stim1': 'std' + str(j), \n",
    "                           'rec':rec,     'fname':fname, 'times':times })\n",
    "            ls.append(tmp_df)\n",
    "    else:\n",
    "        for j in np.unique(trial_type):\n",
    "            if j == 3 or j == 1:\n",
    "                lfp = ddf2.iloc[pre_odd].mean()\n",
    "            elif j == 9 or j == 2:\n",
    "                lfp = ddf2.iloc[idx_odd].mean()\n",
    "            else:\n",
    "                continue\n",
    "    #         if j==3 or j ==9:\n",
    "    #             plt.plot(lfp)\n",
    "    #             plt.show()\n",
    "            tmp_df = pd.DataFrame({'et': et, 'lfp':lfp, 'paradigm': paradigm,  'stim1':'odd'+ str(j), 'rec':rec, \n",
    "                   'fname':fname, 'times':times })\n",
    "            ls.append(tmp_df)\n",
    "    \n",
    "mmn_ori = pd.concat(ls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdac3.et.unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  .pkl for trial-based LFP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "trial_length = 1000\n",
    "trials = sf_tuning.size\n",
    "times = np.linspace(0, trial_length/1e3, trial_length)\n",
    "\n",
    "for fname in matches[:]: \n",
    "    df2 = 0\n",
    "    et = fname.split('\\\\')[-2].split()[0]\n",
    "    side =  fname.split('\\\\')[-1].split()[-1].split('_')[0]\n",
    "    rec = et+side\n",
    "    \n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "#         trial_length = pd.read_hdf(fname, key = 'trial_duration') \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    df3_array = np.reshape(tmp.values,(np.shape(tmp)[0],trials, -1))\n",
    "    df3_avg = np.mean(df3_array,1)\n",
    "    df3_avg = pd.DataFrame(df3_avg).T        \n",
    "    df3_avg.loc[:,'times'] = times\n",
    "    negativity_ch_idx = df3_avg[(df3_avg['times']>=0.35) & (df3_avg['times']< 0.5)].ix[:,8:16].min().idxmin()\n",
    "    # convert to long form df\n",
    "    df = pd.DataFrame(df3_array[negativity_ch_idx]).T\n",
    "    df.columns = sf_tuning\n",
    "    df.loc[:, 'times'] = times\n",
    "    df2 = pd.melt(df, id_vars = 'times', var_name = 'stim1', value_name = 'lfp')\n",
    "    df2 = df2.round(4)\n",
    "    df2.loc[:, 'et'], df2.loc[:, 'side'], df2.loc[:, 'rec'], df2.loc[:, 'fname'] = et, side, rec, fname\n",
    "    ls.append(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_sf = pd.concat(ls)\n",
    "master_sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_pickle('master_fx_mmn_sf-tuning_bytrial.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.concat([tmp, tmp2, tmp3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CSD pkl file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stim1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls2 = []\n",
    "times = np.linspace(0, 1.0, 4000)\n",
    "trial_length = 4000\n",
    "f, ax = plt.subplots(2, figsize = (1,1))\n",
    "for fname in matches[:]:\n",
    "    if 'bad' in fname:\n",
    "        continue\n",
    "\n",
    "    tmp_df = 0\n",
    "    \n",
    "    et = fname.split('\\\\')[-2].split()[0]\n",
    "    training = fname.split('\\\\')[-2].split()[-1]\n",
    "    if 'pre' in training or '1' in training:\n",
    "        training = 'pre'\n",
    "    else:\n",
    "        training = 'post'\n",
    "    stim1 =  fname.split('\\\\')[-1].split('_')[3][:3]\n",
    "    \n",
    "    ls = []\n",
    "\n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "    except:\n",
    "        continue\n",
    "    trials =  int(tmp.shape[1]/trial_length)\n",
    "\n",
    "\n",
    "    csd_tmp = np.split(tmp, int(trials), axis = 1)\n",
    "    csd_input = np.dstack(csd_tmp)\n",
    "    \n",
    "\n",
    "    csd_tmp = csd_input.mean(axis = 2)\n",
    "\n",
    "    csd_tmp = pd.DataFrame(csd_tmp)\n",
    "    csd_tmp = csd_tmp - csd_tmp.median(axis = 0)\n",
    "\n",
    "    csd = oem.df_CSD_analysis( np.array(csd_tmp), ax, \n",
    "                      Channel_Number=np.shape(csd_tmp)[0], show_plot=False)\n",
    "    tmp_df = pd.DataFrame(csd).stack().reset_index()\n",
    "    tmp_df.columns = ['csd_step', 'samples', 'csd']\n",
    "    tmp_df['et'] = et\n",
    "#     tmp_df['paradigm'] = paradigm\n",
    "    tmp_df['stim1'] = stim1\n",
    "    tmp_df['fname'] = fname\n",
    "    tmp_df['times'] = tmp_df['samples']/1000\n",
    "    tmp_df['training'] = training \n",
    "    \n",
    "    ls.append(tmp_df)\n",
    "\n",
    "    \n",
    "    tmp_df2 = pd.concat(ls)\n",
    "    ls2.append(tmp_df2)\n",
    "    \n",
    "csd_df2 = pd.concat(ls2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_df2.et.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust time: stim1 at 1s. 380 pre, 377, 378 post\n",
    "# stim1 at 0.5s: 009, 010 pre, rest at 1s\n",
    "v = '378'\n",
    "csd_df2.loc[((csd_df2.et == v) & (csd_df2.training == 'post')), 'times'] = csd_df2.loc[((csd_df2.et == v) \n",
    "                                                    & (csd_df2.training == 'post')), 'times'] - 0.50\n",
    "csd_df2 = csd_df2.round({'times': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for probe 64DB, adjust times\n",
    "for i,v in enumerate((sorted(csd_df2[csd_df2.et.str.startswith('0')].et.unique()))[:]):\n",
    "    csd_df2.loc[((csd_df2.et == v) & (csd_df2.training == 'post')), 'times'] = csd_df2.loc[((csd_df2.et == v) \n",
    "                                                               & (csd_df2.training == 'post')), 'times'] - 0.50\n",
    "    csd_df2 = csd_df2.round({'times': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oddball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls2 = []\n",
    "times = np.linspace(0, 1.0, 1000)\n",
    "f, ax = plt.subplots(2, figsize = (1,1))\n",
    "for fname in matches:\n",
    "    if 'bad' in fname:\n",
    "        continue\n",
    "    elif 'sf' in fname:\n",
    "        print fname\n",
    "    else:\n",
    "        continue\n",
    "    tmp_df = 0\n",
    "    target_trials = ''\n",
    "    stim_type = ''\n",
    "    et = fname.split('\\\\')[-1].split()[0][1:]\n",
    "    side = fname.split('\\\\')[-1].split()[1].split('_')[0]\n",
    "    paradigm = fname.split('\\\\')[-1].split()[1].split('_')[2]\n",
    "    \n",
    "    ls = []\n",
    "\n",
    "    if 'nov' in paradigm:\n",
    "        trial_type = novel_oddball\n",
    "        idx_odd = np.where(trial_type==9)[0]\n",
    "        pre_odd = idx_odd-1\n",
    "        print 'novel oddball'\n",
    "    elif 'omis' in paradigm:\n",
    "        trial_type = loc_omission\n",
    "        idx_odd = np.where(trial_type==0)[0]\n",
    "        pre_odd = idx_odd-1\n",
    "        print 'loc_omission'\n",
    "    elif 'control' in paradigm:\n",
    "        trial_type = standard_control\n",
    "        print 'std control'\n",
    "    elif 'tuning' in paradigm:\n",
    "        trial_type = sf_tuning\n",
    "        print 'tuning'\n",
    "    else:\n",
    "        print 'no paradigm'\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "    except:\n",
    "        continue\n",
    "    trials =  int(tmp.shape[1]/1000)\n",
    "\n",
    "\n",
    "    csd_tmp = np.split(tmp, int(trials), axis = 1)\n",
    "    csd_input = np.dstack(csd_tmp)\n",
    "    \n",
    "    if 'control' in paradigm or 'tuning' in paradigm:\n",
    "        for j in np.unique(trial_type):\n",
    "            stim_type = 'ctr'\n",
    "            target_trials = np.where( trial_type == j)[0]\n",
    "            csd_tmp = csd_input[:,:, target_trials].mean(axis = 2)\n",
    "          \n",
    "            csd_tmp = pd.DataFrame(csd_tmp)\n",
    "            csd_tmp = csd_tmp - csd_tmp.median(axis = 0)\n",
    "\n",
    "            csd = oem.df_CSD_analysis( np.array(csd_tmp), ax, \n",
    "                              Channel_Number=np.shape(csd_tmp)[0], show_plot=False)\n",
    "            tmp_df = pd.DataFrame(csd).stack().reset_index()\n",
    "            tmp_df.columns = ['csd_step', 'samples', 'csd']\n",
    "            tmp_df['et'] = et\n",
    "            tmp_df['paradigm'] = paradigm\n",
    "            tmp_df['stim1'] = stim_type + str(j)\n",
    "            tmp_df['fname'] = fname\n",
    "            tmp_df['times'] = tmp_df['samples']/1000\n",
    "            tmp_df['side'] = side \n",
    "            ls.append(tmp_df)\n",
    "    else:\n",
    "        for j in np.unique(trial_type):\n",
    "            if j == 3:\n",
    "                stim_type = 'std'\n",
    "                csd_tmp = csd_input[:,:, pre_odd].mean(axis = 2)\n",
    "            elif j == 9 or j == 0:\n",
    "                stim_type = 'dev'\n",
    "                csd_tmp = csd_input[:,:, idx_odd].mean(axis = 2)\n",
    "            else:\n",
    "                continue\n",
    "            csd_tmp = pd.DataFrame(csd_tmp)\n",
    "            csd_tmp = csd_tmp - csd_tmp.median(axis = 0)\n",
    "\n",
    "            csd = oem.df_CSD_analysis( np.array(csd_tmp), ax, \n",
    "                              Channel_Number=np.shape(csd_tmp)[0], show_plot=False)\n",
    "            tmp_df = pd.DataFrame(csd).stack().reset_index()\n",
    "            tmp_df.columns = ['csd_step', 'samples', 'csd']\n",
    "            tmp_df['et'] = et\n",
    "            tmp_df['paradigm'] = paradigm\n",
    "            tmp_df['stim1'] = stim_type + str(j)\n",
    "            tmp_df['fname'] = fname\n",
    "            tmp_df['times'] = tmp_df['samples']/1000\n",
    "            tmp_df['side'] = side \n",
    "            ls.append(tmp_df)\n",
    "    \n",
    "    tmp_df2 = pd.concat(ls)\n",
    "    ls2.append(tmp_df2)\n",
    "    \n",
    "csd_df2 = pd.concat(ls2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_df2['group'] = csd_df2.et.map(sert_groups)\n",
    "csd_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_df.paradigm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = 'stim1'\n",
    "data = csd_df\n",
    "_data = data[(data.paradigm.str.contains('odd'))\n",
    "              | (data.stim1 == 'ctr2') \n",
    "               ] \n",
    "_data = _data[ (_data.times > 0.25)  & (_data.times < 0.75)]\n",
    "\n",
    "n = sorted(_data[cond].unique())[::-1]\n",
    "\n",
    "f,ax = plt.subplots(1, len(n), figsize = (12,6), sharey= True, sharex=True,facecolor = 'w')\n",
    "cbar_ax = f.add_axes([.97, .3, .03, .4])\n",
    "formatter = tkr.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-3, 3))\n",
    "\n",
    "for idx, val in enumerate(n):\n",
    "    hm_input = _data[_data[cond] == val].groupby(['csd_step', \n",
    "            'times']).mean().reset_index().pivot('csd_step', 'times', 'csd').values\n",
    "    sns.heatmap(hm_input, cmap = 'jet',  annot=False, xticklabels = 250,  yticklabels = 5,\n",
    "                ax = ax[idx], cbar=idx == 0,\n",
    "                vmin = -3.0e6, vmax = 3.0e6, robust = True,   \n",
    "                cbar_ax = None if idx else cbar_ax,  cbar_kws={\"format\": formatter}\n",
    "               )\n",
    "    ax[idx].invert_yaxis()\n",
    "    ax[idx].set_title(val)\n",
    "    \n",
    "# plt.savefig(\"hm-csd-cbar.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(_data[(_data.times == 0.4) & (_data.csd < -6e6)].csd_step.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = _data[(_data.csd_step > 6) \n",
    "            & (_data.csd_step<30) \n",
    "            & (_data.stim1 == 'ctr2')\n",
    "           ]\n",
    "tmp.groupby('times').mean().csd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_serta.stim1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_serta.groupby(['group', 'et', 'paradigm']).fname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of LFP\\fx_mmn_paper-lfp_master.pkl\"\n",
    "lfp_df = pd.read_pickle(path)\n",
    "\n",
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of LFP\\FX mmn paper\\fx_mmn_paper-csd_master.pkl\"\n",
    "csd_df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = csd_df[csd_df.stim1 == 'dev9']\n",
    "for i in sorted(tmp.csd_step.unique()[:]):\n",
    "    plt.plot((tmp[tmp.csd_step == i].groupby('times').mean().csd))\n",
    "    plt.show()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_df.loc[:, 'layer'] = 'un'\n",
    "csd_df.loc[csd_df['csd_step'] < 11, 'layer'] = 'l6'\n",
    "csd_df.loc[(csd_df['csd_step'] >= 11) & (csd_df['csd_step'] < 22), 'layer'] = 'l5'\n",
    "csd_df.loc[(csd_df['csd_step'] >= 22) & (csd_df['csd_step'] <= 30), 'layer'] = 'l4'\n",
    "# csd_df.loc[(csd_df['csd_step'] >= 24) & (csd_df['csd_step'] <= 32), 'layer'] = 'l5'\n",
    "csd_df.loc[csd_df['csd_step'] > 30, 'layer'] = 'l2/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of LFP\\SERT KO project\\sert_l4_lfp_master.pkl\"\n",
    "sert_df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_df.paradigm.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls2 = []\n",
    "times = np.linspace(0, 1.0, 1000)\n",
    "\n",
    "groups = {'ET#304': 1, 'ET#271': 1, 'ET#309': 2, 'ET#270': 2, 'ET#313': 1, 'ET#307': 2}\n",
    "sf_seq = [2, 0, 0, 4, 3, 3, 0, 4, 4, 3, 2, 5, 3, 2, 0, 4, 1, 0, 5, 2, 0, \n",
    "1, 0, 1, 3, 5, 2, 5, 1, 2, 0, 5, 2, 3, 5, 1, 0, 4, 3, 2, 5, 5, 3, 5, \n",
    "2, 0, 3, 3, 0, 3, 4, 5, 4, 1, 4, 0, 1, 5, 4, 1, 5, 3, 3, 5, 3, 3, 2, \n",
    "3, 2, 1, 1, 5, 1, 4, 1, 2, 3, 2, 4, 2, 1, 0, 5, 5, 2, 2, 4, 1, 4, 1, \n",
    "3, 1, 0, 4, 4, 4, 4, 0, 5, 4, 4, 0, 3, 5, 5, 2, 1, 3, 4, 1, 5, 0, 2, \n",
    "2, 0, 0, 0, 1, 2, 1]\n",
    "\n",
    "contrast_seq = [1, 0.25, 0.125, 1, 0.0625, 1, 1, 0.5, 0.0625, 0.125, 0.5, \n",
    "0.125, 0.5, 0.5, 0.125, 0.125, 0.5, 0.25, 0.5, 0.0625, 1, 0.0625, 0.5, 1, \n",
    "1, 0.125, 1, 0.125, 0.125, 0.25, 0.0625, 0.0625, 0.125, 0.5, 0.125, 0.5, \n",
    "0.0625, 1, 1, 0.25, 0.25, 1, 1, 0.25, 0.125, 1, 0.25, 0.5, 0.0625, 0.5, \n",
    "0.125, 0.0625, 0.25, 0.0625, 0.25, 0.5, 0.25, 0.5, 0.125, 0.125, 1, 1, \n",
    "0.5, 0.5, 0.125, 1, 0.25, 0.0625, 0.25, 0.25, 1, 1, 0.25, 0.5, 0.5, 0.125, \n",
    "1, 0.125, 0.0625, 0.125, 0.25, 0.0625, 0.25, 0.0625, 0.0625, 0.0625, 0.5, \n",
    "0.125, 0.25, 0.5, 0.0625, 0.25, 0.5, 0.0625, 0.25, 1, 0.0625, 0.0625, 0.25, 0.125]\n",
    "\n",
    "exp_type = dir_seq\n",
    "for fname in sorted(matches)[:3]:\n",
    "    print fname\n",
    "#     et = fname.split('\\\\')[-1].split('_')[1].split()[0]\n",
    "#     rec = fname.split('\\\\')[-1].split('_')[2]\n",
    "#     paradigm = fname.split('\\\\')[-1].split('_')[3]\n",
    "    \n",
    "    ls = []\n",
    "\n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "    except:\n",
    "        continue\n",
    "    trials =  int(tmp.shape[1]/1000)\n",
    "\n",
    "    df3_array=np.reshape(tmp.values,(np.shape(tmp)[0],trials, -1))\n",
    "    df3_avg=np.mean(df3_array,1)\n",
    "    df3_avg = pd.DataFrame(df3_avg).T        \n",
    "    df3_avg['times'] = times\n",
    "    negativity_ch_idx=df3_avg[(df3_avg['times']>=0.35) & (df3_avg['times']< 0.45)].ix[:,8:16].min().idxmin()\n",
    "#     negativity_ch_idx = negativity_ch_idx - 4\n",
    "    print negativity_ch_idx\n",
    "    # plotting the spectra and FFR for the maximum negativity channel, the first trial\n",
    "    ddf = tmp.values[negativity_ch_idx,:]\n",
    "    ddf2=np.reshape(ddf,(trials, 1000))\n",
    "    ddf2 = pd.DataFrame(ddf2)\n",
    "    ddf2['trial'] = exp_type[:ddf2.shape[0]]\n",
    "    for j in ddf2.trial.unique():\n",
    "    \n",
    "        lfp = ddf2[ddf2.trial==j].iloc[:,:].mean().drop('trial')\n",
    "#         if j==3 or j ==9:\n",
    "#             plt.plot(lfp)\n",
    "#             plt.show()\n",
    "        tmp_df = pd.DataFrame({ 'lfp':lfp, 'paradigm': '12-dir', 'stim1':j, 'abs_times': times + j*1,\n",
    "                               'fname':fname, 'times':times })\n",
    "        ls.append(tmp_df)\n",
    "    tmp_df2 = pd.concat(ls)\n",
    "    ls2.append(tmp_df2)\n",
    "    \n",
    "df_tuning = pd.concat(ls2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2.trial.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {'205':1, '206':1, '207':1, '212':3, '213':3, '214':3 }\n",
    "con_tuning['group'] = con_tuning.et.map(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(lfp_df.shape[0]/1000)*[np.arange(0,1,0.001)]\n",
    "lfp_df['times']= np.concatenate(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of LFP\\SERT KO project\\lfp_l4_sert_master.pkl\"\n",
    "df_sert = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = df_sert[df_sert.group == 'wt']\n",
    "df_tmp = pd.concat([tmp, df_hdac]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = sert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sert_master = pd.concat([df_sert_da, df_sert_db])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sert_df.groupby(['group', 'et', 'training']).fname.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting avg traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust time: stim1 at 1s. 380 pre, 378, 378 post\n",
    "# stim1 at 0.5s: 009, 010 pre, rest at 1s\n",
    "v = '380'\n",
    "df_serta.loc[((df_serta.et == v) & (df_serta.training == 'pre')), 'times'] = df_serta.loc[((df_serta.et == v) \n",
    "                                                    & (df_serta.training == 'pre')), 'times'] - 0.50\n",
    "df_serta = df_serta.round({'times': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.concat([df_sert, df_sert_l4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.to_pickle('sert_l4_lfp_master.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_training = ['gray', 'crimson']\n",
    "colors_group = [ 'k', 'blue', 'magenta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate((sorted(df_sert_l4.et.unique()))[:]):\n",
    "    df_sert_l4.loc[((df_sert_l4.et == v)), 'times'] = df_sert_l4.loc[((df_sert_l4.et == v)), 'times'] - 0.50\n",
    "    df_sert_l4 = df_sert_l4.round({'times': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate((sorted(df_tmp[df_tmp.group == 'hdac'].et.unique()))):\n",
    "    df_tmp.loc[((df_tmp.et == v) ), 'times'] = df_tmp.loc[((df_tmp.et == v)) , 'times'] - 0.50\n",
    "#     df_tmp.loc[((df_tmp.et == v) ), 'times'] = df_tmp.loc[((df_tmp.et == v) ), 'times'].round({'times': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), facecolor='w')\n",
    "data = df_master\n",
    "data = data[(data.stim1.str.contains('N') )\n",
    "#               & (data.stim1 == 'G-1') \n",
    "            & (data.training == 'post')\n",
    "               ] \n",
    "# data = data[(data.paradigm.str.contains('odd')) | (data.stim1 == 'ctr2')]\n",
    "# data = data[(data.times > 0) & (data.times < 1.5) ]\n",
    "\n",
    "colors_training = ['gray', 'crimson']\n",
    "colors_group = [ 'k', 'magenta', 'blue']\n",
    "# colors_group = ['k', 'coral']\n",
    "# colors_stim = ['gray', 'crimson', 'green']\n",
    "# data = data[(data.group != 'wat') & (data.layer == 'l4')]\n",
    "\n",
    "# # data = data[data.rec.str.contains('1')]\n",
    "\n",
    "data = data[(data.times > 0.05) & (data.times < 2.5)]\n",
    "# data['abs_times'] = data['stim1'].str.split('ctr', expand = True)[1].astype('int')*0.3 + data['times']\n",
    "# data = data[(data.stim1 == 1) | (data.stim1 == 3)]\n",
    "# data = data[data.et == data.et.unique()[3]]\n",
    "# print data.et.unique()\n",
    "# data = data.groupby(['group' ,'stim1', 'rec']).mean().reset_index()\n",
    "data = data.sort_values(by=['group', 'training'], ascending=[ False, False])\n",
    "\n",
    "sns.lineplot( x = 'times',  y = 'lfp', data = data,\n",
    "               legend = 'full', hue = 'group', \n",
    "             palette = colors_group,\n",
    "           estimator = np.nanmean, ci = None\n",
    "            )\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "sns.despine()\n",
    "# plt.axhline(y = 0)\n",
    "# plt.axvline(x = 0.5)\n",
    "plt.axvspan(0.5, 0.9, facecolor=\"gray\",alpha=0.2) \n",
    "# plt.xlim(0, 1)\n",
    "plt.ylim(-350, 250)\n",
    "# plt.savefig(\"line-sert-l4-gn-post.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots( 1, 3, sharey = True, figsize =(12,6))\n",
    "cbar_ax = f.add_axes([.91, .3, .03, .5])\n",
    "vmin, vmax = -500, 500\n",
    "data = lfp_df[(lfp_df.stim1.str.contains('ctr')) & (lfp_df.times > 0.3) \n",
    "              & (lfp_df.times < 0.6) & (lfp_df.layer == 'l2/3')]\n",
    "tmp = data.groupby(['group', 'stim1', 'times']).lfp.mean().reset_index()\n",
    "tmp2 = tmp[tmp.group == 'wt'].pivot('times', 'stim1', 'lfp')\n",
    "tmp3 = tmp[tmp.group == 'fx'].pivot('times', 'stim1', 'lfp')\n",
    "sns.heatmap(tmp2, cmap = 'jet', ax = ax[0], cbar = False, vmin = vmin, vmax = vmax)\n",
    "sns.heatmap(tmp3, cmap = 'jet', ax = ax[1], cbar = False,  vmin = vmin, vmax = vmax)\n",
    "sns.heatmap(tmp3.sub(tmp2), cmap = 'jet', ax = ax[2],  vmin = vmin, vmax = vmax, cbar_ax = cbar_ax)\n",
    "ax[1].set_ylabel('')\n",
    "ax[2].set_ylabel('')\n",
    "ax[0].invert_yaxis()\n",
    "# plt.savefig(\"hm-l23-sf_by_time.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_df.layer.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean = csd_df.groupby(['fname', 'csd_step']).csd.transform('mean')    \n",
    "Std = csd_df.groupby(['fname', 'csd_step']).csd.transform('std')\n",
    "csd_df['zscore'] = (csd_df.csd.values - Mean) / Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = csd_df[(csd_df.paradigm.str.contains('odd') )\n",
    "              | (csd_df.stim1.str.contains('ctr2'))\n",
    "             ]\n",
    "data = data[(data.times > 0.35) & (data.times < 0.7)]\n",
    "data = data.groupby(['group', 'rec', 'layer' ,'stim1', 'csd_step']).min().reset_index()\n",
    "tmp = data.groupby(['group', 'rec', 'layer' ,'stim1']).csd.mean().reset_index()\n",
    "tmp = tmp.sort_values(by=['group', 'stim1'], ascending=[ False, False])\n",
    "sns.factorplot(x = 'stim1', y = 'csd', data = tmp, palette = colors_group,\n",
    "              hue = 'group', height = 5, aspect = 2,\n",
    "                col = 'layer', col_wrap = 2, sharey = False,\n",
    "              )\n",
    "# plt.savefig(\"point-csd-across-layers.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_sert[(df_sert.stim1 == '1') & (df_sert.training == 'right1') \n",
    "              & (df_sert.times > 0) & (df_sert.times < 3)\n",
    "             ]\n",
    "for et in tmp.et.unique():\n",
    "    print et\n",
    "    tmp2 = tmp[tmp.et == et]\n",
    "    plt.plot(tmp2.groupby('times').lfp.mean())\n",
    "    plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp = data[(data.times > 0.35) & (data.times < 0.5)]\n",
    "colors = ['green', 'crimson', 'gray']\n",
    "fig_inp = fig_inp.groupby([ 'rec', 'stim1']).min().reset_index()\n",
    "fig_inp = fig_inp.sort_values(by = ['stim1', 'group'], ascending= [False, False])\n",
    "g = sns.factorplot(x = \"stim1\", y = \"lfp\",   data = fig_inp,  kind = 'point', ci = 68, \n",
    "                   palette = colors_group, hue = 'group',\n",
    "#                    capsize = 0.02, saturation = 1,\n",
    "                   height = 5, aspect = 1.5 )\n",
    "# plt.savefig(\"vep_bar_fx_mmn_l4.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats.multicomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp['lfp_tr'] = abs(fig_inp.lfp)**0.4\n",
    "model = ols('lfp_tr ~ C(group)*C(stim1)', fig_inp).fit()\n",
    "\n",
    "# Seeing if the overall model is significant\n",
    "print(\"Overall model F\", model.df_model,model.df_resid, model.fvalue, model.f_pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durban-Watson: autocorrelation: 2(no), 0-2 (positive), 2-4 (negative)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sm.stats.anova_lm(model, typ= 2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ols('lfp_tr ~ C(group)+ C(stim1)', fig_inp).fit()\n",
    "\n",
    "# Seeing if the overall model is significant\n",
    "print(\"Overall model F\", model2.df_model,model2.df_resid, model2.fvalue, model2.f_pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = sm.stats.anova_lm(model2, typ= 2)\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp['cond'] = fig_inp['stim1'] + fig_inp['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp.groupby(['group', 'et']).stim1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wt_std = fig_inp[(fig_inp.stim1 == 'std3') & (fig_inp.group == 'wt')].lfp.dropna().values\n",
    "wt_dev = fig_inp[(fig_inp.stim1 == 'dev9') & (fig_inp.group == 'wt')].lfp.dropna().values\n",
    "wt_ctr = fig_inp[(fig_inp.stim1 == 'ctr2') & (fig_inp.group == 'wt')].lfp.dropna().values\n",
    "\n",
    "fx_std = fig_inp[(fig_inp.stim1 == 'std3') & (fig_inp.group == 'fx')].lfp.dropna().values\n",
    "fx_dev = fig_inp[(fig_inp.stim1 == 'dev9') & (fig_inp.group == 'fx')].lfp.dropna().values\n",
    "fx_ctr = fig_inp[(fig_inp.stim1 == 'ctr2') & (fig_inp.group == 'fx')].lfp.dropna().values\n",
    "\n",
    "print np.mean(wt_std), sstat.sem(wt_std)\n",
    "print np.mean(wt_dev), sstat.sem(wt_dev)\n",
    "print np.mean(wt_ctr), sstat.sem(wt_ctr)\n",
    "\n",
    "\n",
    "print (len(wt_std), len(fx_std))\n",
    "\n",
    "# print sstat.kruskal(x1, x2, x3)\n",
    "\n",
    "p1 =  sstat.mannwhitneyu(wt_std, wt_dev)[1]\n",
    "p2 = sstat.mannwhitneyu(wt_std, wt_ctr)[1]\n",
    "p3 =  sstat.mannwhitneyu(wt_dev, wt_ctr)[1]\n",
    "\n",
    "p4 =  sstat.mannwhitneyu(fx_std, fx_dev)[1]\n",
    "p5 =  sstat.mannwhitneyu(fx_std, fx_ctr)[1]\n",
    "p6 =  sstat.mannwhitneyu(fx_dev, fx_ctr)[1]\n",
    "\n",
    "p7 =  sstat.mannwhitneyu(wt_std, fx_std)[1]\n",
    "p8 =  sstat.mannwhitneyu(wt_dev, fx_dev)[1]\n",
    "p9 =  sstat.mannwhitneyu(wt_ctr, fx_ctr)[1]\n",
    "\n",
    "pvals = np.array((p1, p2, p3, p4, p5, p6, p7, p8, p9))\n",
    "adj = statsmodels.stats.multitest.multipletests(pvals, method='holm')\n",
    "print pvals\n",
    "print adj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp = lfp_df[(lfp_df.times > 0.4) & (lfp_df.times < 0.45)]\n",
    "fig_inp = fig_inp[fig_inp.paradigm.str.contains('tun') & (fig_inp.layer == 'l4')]\n",
    "fig_inp = fig_inp.groupby(['group', 'stim1', 'fname' ]).lfp.min().reset_index()\n",
    "# fig_inp = fig_inp[(fig_inp.stim1.str.contains('ctr') )\n",
    "#               & (fig_inp.layer == 'l5/6') \n",
    "#                 & (data.paradigm.str.contains('025')) \n",
    "#                 & (data.et == data.et.unique()[4])\n",
    "#                ]   \n",
    "\n",
    "fig_inp = fig_inp.sort_values(by = ['stim1', 'group'], ascending = [True, False])\n",
    "g = sns.catplot(x=\"stim1\", y=\"lfp\",   data= fig_inp,  kind = 'point', ci = 68, \n",
    "                  hue = 'group', palette = ['k', 'coral'],\n",
    "#                    capsize = 0.02, saturation = 1,\n",
    "                   height = 5, aspect=1.5 )\n",
    "# plt.ylim(-1500, 0)\n",
    "plt.savefig('point-lfp-sf_tuning-04-045.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'lfp'\n",
    "cond = 'stim1'\n",
    "\n",
    "ls = []\n",
    "# for lay in sorted(fig_inp['layer'].unique()):\n",
    "#     print lay\n",
    "for l in sorted(fig_inp[cond].unique()):\n",
    "\n",
    "    stat_inp = fig_inp[fig_inp[cond] == l]\n",
    "    x1 = stat_inp[(stat_inp.group == 'wt')   ][val].dropna().values\n",
    "    x2 = stat_inp[(stat_inp.group == 'fx')   ][val].dropna().values\n",
    "\n",
    "#     x3 = stat_inp[(stat_inp.group == 'wt') & (stat_inp.n_type == 'fs')][val].dropna().values\n",
    "#     x4 = stat_inp[(stat_inp.group == 'fx') & (stat_inp.n_type == 'fs')][val].dropna().values\n",
    "\n",
    "    print '\\t', l\n",
    "    wt_mean, wt_sem = np.mean(x1), sstat.sem(x1)\n",
    "    fx_mean, fx_sem = np.mean(x2), sstat.sem(x2)\n",
    "#     print 'wt, fs', len(x3) ,np.mean(x3), sstat.sem(x3)\n",
    "#     print 'fx, fs', len(x4) ,np.mean(x4), sstat.sem(x4)\n",
    "\n",
    "    x1_x2 = sstat.mannwhitneyu(x1, x2)\n",
    "\n",
    "    data_tmp = pd.DataFrame({ 'stim1': l, \n",
    "'mean': [wt_mean, fx_mean], 'sem': [wt_sem, fx_sem], \n",
    "'n_ch': [len(x1), len(x2)], 'group': ['wt', 'fx'], 'p_val': [x1_x2[1], x1_x2[1]], \n",
    "            'u_stat': [x1_x2[0], x1_x2[0]]\n",
    "                })\n",
    "    ls.append(data_tmp)\n",
    "    print '------------------------'\n",
    "stat_table = pd.concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp['angle'] = fig_inp['stim1']*30\n",
    "fig_inp['lfp'] = abs(fig_inp['lfp'])\n",
    "tmp1 = fig_inp.groupby('angle').mean().reset_index()\n",
    "std = fig_inp.groupby('angle').sem().reset_index()\n",
    "tmp2 = tmp1.append(tmp1.iloc[0])\n",
    "std2 = std.append(std.iloc[0])\n",
    "ax = plt.subplot(111, projection='polar')\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_theta_direction(-1)\n",
    "ax.plot((tmp2.angle/360)*2*np.pi, abs(tmp2.lfp))\n",
    "tmp2['a'] = tmp2.lfp + std2.lfp\n",
    "tmp2['b'] = tmp2.lfp - std2.lfp\n",
    "ax.fill_between((tmp2.angle/360)*2*np.pi, tmp2.a, tmp2.b, alpha=0.2)\n",
    "ax.set_xticks(np.arange(0,2*np.pi,np.pi/6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of LFP\\FX mmn paper\\fx_var-odd_lfp_11-22-18.pkl\"\n",
    "set1 = pd.read_pickle(path)\n",
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of LFP\\FX mmn paper\\fx_var-odd_lfp_12-18-18.pkl\"\n",
    "set2 = pd.read_pickle(path)\n",
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of LFP\\FX mmn paper\\thy1-fx_var-odd_lfp.pkl\"\n",
    "set3 = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_seq.size/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_df = pd.concat([set1, set2, set3]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1, 0.001)\n",
    "for fname in sorted(mmn_df3.fname.unique()[160:]):\n",
    "    tmp = mmn_df3[(mmn_df3.fname == fname)]\n",
    "    try:\n",
    "        print tmp.et.values[0], tmp.group.values[0], tmp.side.values[0] , tmp.paradigm.values[0],tmp.l4_ch.values[0]\n",
    "    except:\n",
    "        print 'no'\n",
    "    ctr = tmp[tmp.stim1 == 'ctr2'].lfp.values\n",
    "    std = tmp[(tmp.stim1 == 'std3') & (tmp.paradigm == 'sf-nov-odd')].lfp.values\n",
    "    dev = tmp[tmp.stim1 == 'dev9'].lfp.values\n",
    "    if ctr.shape[0] > 0:\n",
    "        plt.plot(x, ctr, c = 'magenta')\n",
    "    if std.shape[0] > 1:\n",
    "        plt.plot(x, std, c = 'gray')\n",
    "        plt.plot(x, dev, c = 'r')\n",
    "    plt.axvline(x = 0.4)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff(tmp.T.dropna().T.iloc[0].index.values)*1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots()\n",
    "tmp = data[data.stim1 == 'std3'].dropna().pivot(index = 'fname', columns= 'times' ,values = 'lfp') \n",
    "# tf, time, frex, tf3d = oem.tf_cmw(ax, tmp, show=True, log_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_training = ['gray', 'crimson']\n",
    "colors_group = [ 'k', 'blue', 'magenta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = mmn_exp[mmn_exp.paradigm =='paired-oddball']\n",
    "data = df_master\n",
    "data = data[(data.stim1.str.contains('N'))\n",
    "#               & (data.layer == 'l4') \n",
    "            & (data.training == 'post')\n",
    "               ] \n",
    "\n",
    "vep1 = data[(data.times>0.53) & (data.times<0.63)]\n",
    "vep1 = vep1.ix[vep1.groupby([ 'stim1' ,'et']).lfp.idxmin().values]\n",
    "\n",
    "vep1.loc[:, 'vep'] = 1\n",
    "\n",
    "vep2 = data[(data.times > 0.73) & (data.times < 0.83)]\n",
    "vep2 = vep2.ix[vep2.groupby('et').lfp.idxmin().values]\n",
    "\n",
    "vep2.loc[:, 'vep'] = 2\n",
    "\n",
    "vep3 = data[(data.times > 0.93) & (data.times < 1.03)]\n",
    "vep3 = vep3.ix[vep3.groupby('et').lfp.idxmin().values]\n",
    "\n",
    "vep3.loc[:, 'vep'] = 3\n",
    "\n",
    "out = pd.concat([vep1, vep2, vep3])\n",
    "# _data = band_df\n",
    "g = sns.factorplot(x=\"vep\", y=\"lfp\", hue ='group',  data=out,  kind = 'bar', ci = 68, \n",
    "                   palette = colors_group, hue_order = ['wt', 'het', 'ko'],\n",
    "                   capsize = 0.02, saturation = 1,\n",
    "                   size = 5, aspect=1.5 )\n",
    "\n",
    "# plt.savefig('bar-sert-g1-pre-l4.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_out3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = 'variable'\n",
    "val = 'value'\n",
    "_inp = tf_out3\n",
    "for l in sorted(_inp[cond].unique()):\n",
    "    stat_inp = _inp[_inp[cond] == l]\n",
    "#     stat_inp = stat_inp.groupby(['cond', 'et']).mean().reset_index()\n",
    "    x1 = stat_inp[(stat_inp.cond == 'wt')   ][val].dropna().values\n",
    "    x2 = stat_inp[(stat_inp.cond == 'het')   ][val].dropna().values\n",
    "    x3 = stat_inp[(stat_inp.cond == 'ko')   ][val].dropna().values\n",
    "\n",
    "    print '\\t', l\n",
    "    print 'wt mean sem', np.mean(x1), sstat.sem(x1)\n",
    "    print 'het mean sem', np.mean(x2), sstat.sem(x2)\n",
    "    print 'ko mean sem', np.mean(x3), sstat.sem(x3)\n",
    "    print len(x1), len(x2), len(x3)\n",
    "    \n",
    "    print sstat.kruskal(x1, x2, x3)\n",
    "    print sstat.mannwhitneyu(x1, x2)\n",
    "    print sstat.mannwhitneyu(x1, x3)\n",
    "    print sstat.mannwhitneyu(x2, x3)\n",
    "    print '------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = master_df[master_df.training=='pre'].reset_index()\n",
    "test = data\n",
    "\n",
    "vep1 = test[(test.times>0.37) & (test.times<0.47)]\n",
    "vep1 = vep1.ix[vep1.groupby([ 'stim1' ,'fname']).lfp.idxmin().values]\n",
    "vep1 = vep1.groupby([ 'stim1' ,'fname']).min()\n",
    "vep1 = vep1.reset_index()\n",
    "vep1['vep'] = 'ssa'\n",
    "\n",
    "vep2 = test[(test.times>0.6) & (test.times<0.7)]\n",
    "vep2 = vep2.ix[vep2.groupby(['stim1' ,'fname']).lfp.idxmax().values]\n",
    "vep2 = vep2.groupby( ['stim1', 'fname']).max()\n",
    "vep2 = vep2.reset_index()\n",
    "vep2['vep'] = 'dd'\n",
    "\n",
    "# vep3 = test[(test.times>0.93) & (test.times<1.03)]\n",
    "# vep3 = vep3.ix[vep3.groupby('fname').lfp.idxmin().values]\n",
    "# vep3 = vep3.groupby('fname').min()\n",
    "# vep3['vep'] = 3\n",
    "\n",
    "# vep4 = test[(test.times>1.15) & (test.times<1.25)]\n",
    "# vep4 = vep4.ix[vep4.groupby('fname').lfp.idxmin().values]\n",
    "# vep4 = vep4.groupby('fname').min()\n",
    "# vep4['vep'] = 4\n",
    "\n",
    "# vep5 = test[(test.times>1.38) & (test.times<1.48)]\n",
    "# vep5 = vep5.ix[vep5.groupby('fname').lfp.idxmin().values]\n",
    "# vep5 = vep5.groupby('fname').min()\n",
    "# vep5['vep'] = 5\n",
    "\n",
    "\n",
    "\n",
    "out = pd.concat([vep1, vep2])\n",
    "# out = out.sort_values(by=['group'])\n",
    "# _data = band_df\n",
    "g = sns.factorplot(x=\"vep\", y=\"lfp\", hue ='stim1',  data=out,  kind = 'bar', ci = 95, \n",
    "#                     capsize = 0.02, saturation = 1,\n",
    "                   size = 5, aspect=1.5 )\n",
    "# plt.ylim(-700, 700)\n",
    "# plt.savefig('sf-vmmn-l4vep_quant-groupb.pdf', transparent=True)\n",
    "# plt.savefig('sert_ko_amp_vep_all.png', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sstat.wilcoxon(vep1[vep1.stim1 == 3].lfp, vep1[vep1.stim1 == 9].lfp )\n",
    "print sstat.wilcoxon(vep2[vep2.stim1 == 3].lfp, vep2[vep2.stim1 == 9].lfp )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdac3.to_pickle('lfp_l4_hdac.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of LFP\\fx-mmn-paper-tf_power.pkl\"\n",
    "tf_df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mmn_df\n",
    "data = data[(data.paradigm.str.contains('odd') )\n",
    "              | (data.stim1.str.contains('ctr2')) \n",
    "#                 | (data.stim1.str.contains('std')) \n",
    "#                 & (data.et == data.et.unique()[4])\n",
    "               ]   \n",
    "\n",
    "data = data[(data.group == 'fx') & (data.layer == 'l4')]\n",
    "data = data[(data.times>0) & (data.times<0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "ls2= []\n",
    "ls_perm = []\n",
    "sf_col = [0.01, 0.02, 0.04, 0.08, 0.14 ]\n",
    "data = df_master\n",
    "data = data[(data.stim1.str.contains('1'))\n",
    "#               & (data.layer == 'l4') \n",
    "            & (data.training == 'post')\n",
    "               ] \n",
    "# data = data[(data.paradigm.str.contains('odd')) | (data.stim1 == 'ctr2') ]\n",
    "data = data[(data.times > 0.05) & (data.times < 2.5)]\n",
    "data = data.groupby(['group' ,'et', 'times']).mean().reset_index()\n",
    "data = data.sort_values(by=['group'], ascending=[False])\n",
    "_data = data\n",
    "# _data = _data[_data.cnt == '25']\n",
    "param = 'group'\n",
    "n =sorted(_data[param].unique()) \n",
    "f, ax = plt.subplots( 1, len(n), sharex=True, sharey=True, figsize=(12,6), facecolor = 'w')\n",
    "t1, t2 = 500, 1500\n",
    "\n",
    "for idx, val in enumerate((_data[param].unique())):\n",
    "    \n",
    "    #tmp = _data[_data['stim1']==sf].pivot_table(index = ['fname', 'et'], columns= 'times' ,values = 'lfp').mean(level=1)\n",
    "    tmp = _data[_data[param]==val].pivot(index = 'et', columns= 'times' ,values = 'lfp') \n",
    "    tmp = tmp.T.dropna().T\n",
    "    tf, time, frex, tf3d = oem.tf_cmw(ax[idx], tmp, show=True, log_scale=False)\n",
    "    ax[idx].set_title('' + str(val))\n",
    "    \n",
    "    tf3_arr = np.dstack(tf3d)\n",
    "    tf3_arr = 10*np.log10(tf3_arr/tf3_arr[:,:500,:].mean(axis=1, keepdims=1))\n",
    "    \n",
    "#     mask = frex[(frex>12) & (frex<25)]\n",
    "#     ax[idx].plot(((tf[1, int(mask[0]):int(mask[-1]), :]).mean(axis=0)))\n",
    "    ls_perm.append(tf3_arr)\n",
    "    # this is because dB averaging does not equal to linear power averaging \n",
    "    tmp = np.dstack(tf3d)\n",
    "    tmp = tmp.swapaxes(0,2)\n",
    "    tmp = tmp/tmp[:,:500,:].mean(axis=1, keepdims=1)\n",
    "    #include only positve power, quanitfy bands\n",
    "   \n",
    "    alpha = np.mean(tmp[(frex>8) & (frex<=12)][:,t1:t2,:], axis=1)\n",
    "    alpha = np.mean(alpha,axis=0)\n",
    "\n",
    "    beta = np.mean(tmp[(frex > 12) & (frex <= 30)][:,t1:t2,:], axis=1)\n",
    "    beta = np.mean(beta, axis=0)\n",
    "\n",
    "    low_gamma = np.mean(tmp[(frex > 30) & (frex <= 50)][:,t1:t2,:], axis=1)\n",
    "    low_gamma = np.mean(low_gamma,axis=0)\n",
    "\n",
    "    high_gamma = np.mean(tmp[(frex>50)][:,t1:t2,:] , axis=1)\n",
    "    high_gamma = np.mean(high_gamma,axis=0)\n",
    "\n",
    "    theta = np.mean(tmp[(frex>=4) & (frex<=8)][:,t1:t2,:], axis=1)\n",
    "    theta = np.mean(theta,axis=0)\n",
    "\n",
    "    tf_tmp = pd.DataFrame({'cond': val,  'beta':beta, 'hg': high_gamma,\n",
    "                           'alpha': alpha,   'lg': low_gamma, 'theta': theta,\n",
    "                   'index': np.arange(0, _data[_data[param]==val].et.unique().size)\n",
    "                     })\n",
    "    ls2.append(tf_tmp)\n",
    "tf_tmp = pd.concat(ls2)\n",
    "# tf_tmp = pd.melt(tf_out, id_vars=['cond'], value_vars=['theta', 'alpha', 'beta', 'lg','hg' ])\n",
    "tf_tmp = pd.melt(tf_tmp, id_vars=['cond','index' ], \n",
    "                 value_vars=['theta', 'alpha', 'beta',  'lg','hg' ])\n",
    "tf_out3 = tf_tmp.groupby(['cond','variable', 'index']).mean().reset_index()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# plt.savefig(\"nat-mov_tf.png\", transparent=True)\n",
    "# plt.savefig(\"cbar.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_out3['ucond'] = 'postn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.dstack(tf3d)\n",
    "tmp = tmp.swapaxes(0,2)\n",
    "tmp = tmp/tmp[:,:300,:].mean(axis=1, keepdims=1)\n",
    "#include only positve power, quanitfy bands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_out3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "ls2= []\n",
    "ls_perm = []\n",
    "sf_col = [0.01, 0.02, 0.04, 0.08, 0.14 ]\n",
    "# _data = sert_df[(sert_df.training == 'post')\n",
    "#               & (sert_df.stim1 == '1')\n",
    "#               & (sert_df.stim2 == 'G')\n",
    "#                & (sert_df.times < 2.5)]\n",
    "_data = df_sf3\n",
    "param = 'training'\n",
    "n =sorted(_data[param].unique()) \n",
    "f, ax = plt.subplots( 1, len(n), sharex=True, sharey=True, figsize=(14,4), facecolor = 'w')\n",
    "t1, t2 = 400, 500\n",
    "ls_t2 = [120, 190, 260, 330, 400, 470, 540]\n",
    "#ls_high = []\n",
    "# _data = _data[  (_data.training_type == 'ori') \n",
    "#                 & (_data.training == 'oddball') \n",
    "#                & (_data.stim1==3)\n",
    "#                 & (_data.et != 'KIC5')\n",
    "#                ]  \n",
    "# _data = tf_exp\n",
    "# times = np.linspace(0,0.5,500)\n",
    "\n",
    "\n",
    "for idx, val in enumerate(sorted(_data[param].unique(), reverse=True)):\n",
    "    \n",
    "    #tmp = _data[_data['stim1']==sf].pivot_table(index = ['fname', 'et'], columns= 'times' ,values = 'lfp').mean(level=1)\n",
    "    tmp = _data[_data[param]==val].pivot(index = 'fname', columns= 'times' ,values = 'lfp') \n",
    " \n",
    "    tf, time, frex, tf3d = oem.tf_cmw(ax[idx], tmp, show=False, log_scale=False)\n",
    "    ax[idx].set_title('' + str(val))\n",
    "    \n",
    "#     tf3_arr = np.dstack(tf3d)\n",
    "#     tf3_arr = 10*np.log10(tf3_arr/tf3_arr[:,:500,:].mean(axis=1, keepdims=1))\n",
    "    \n",
    "#     mask = frex[(frex>12) & (frex<25)]\n",
    "#     ax[idx].plot(((tf[1, int(mask[0]):int(mask[-1]), :]).mean(axis=0)))\n",
    "#     ls_perm.append(tf3_arr)\n",
    "    \n",
    "    tmp = np.dstack(tf3d)\n",
    "    tmp = tmp.swapaxes(0,2)\n",
    "    tmp = 10*np.log10(tmp/tmp[:,:500,:].mean(axis=1, keepdims=1))\n",
    "    for t2 in ls_t2:\n",
    "        t1 = t2-70\n",
    "\n",
    "        alpha = np.max(tmp[(frex>8) & (frex<=12)], axis=0)[500+t1: 500+t2]\n",
    "        alpha = np.mean(alpha,axis=0)\n",
    "\n",
    "        beta = np.max(tmp[(frex>12) & (frex<=25)], axis=0)[500+t1: 500+t2]\n",
    "        beta = np.mean(beta,axis=0)\n",
    "\n",
    "        low_gamma = np.max(tmp[(frex>30) & (frex<=50)], axis=0)[500+t1: 500+t2]\n",
    "        low_gamma = np.mean(low_gamma,axis=0)\n",
    "\n",
    "        high_gamma = np.max(tmp[(frex>50) ], axis=0)[500+t1: 500+t2]\n",
    "        high_gamma = np.mean(high_gamma,axis=0)\n",
    "\n",
    "#         theta = np.mean(tmp[(frex>=4) & (frex<=8)], axis=0)[1000*tw+50+t1: 1000*tw+50+t2]\n",
    "#         theta = np.mean(theta,axis=0)\n",
    "\n",
    "\n",
    "        tf_tmp = pd.DataFrame({'cond': val,  'beta':beta, 'hg': high_gamma,'window':str(t2/1000),\n",
    "                               'alpha': alpha,   'lg': low_gamma, \n",
    "                       'index': np.arange(0, _data[_data[param]==val].fname.unique().size)\n",
    "                 })\n",
    "        ls.append(tf_tmp)\n",
    "    tf_tmp = pd.concat(ls)\n",
    "    ls2.append(tf_tmp)\n",
    "tf_out = pd.concat(ls2)\n",
    "tf_tmp = pd.melt(tf_out, id_vars=['cond'], value_vars=['alpha', 'beta', 'lg','hg' ])\n",
    "tf_out = pd.melt(tf_out, id_vars=['window' ,'cond','index' ], \n",
    "                 value_vars=['alpha', 'beta',  'lg','hg' ])\n",
    "tf_out = tf_out.groupby(['window' ,'cond','variable', 'index']).mean().reset_index()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"nat-mov_tf.png\", transparent=True)\n",
    "# plt.savefig(\"nat-mov_tf.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_out4 = pd.concat([ tf_out, tf_out2, tf_out3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_out.groupby(['cond', 'variable']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = ['green', 'crimson', 'gray']\n",
    "fig_inp = tf_out4\n",
    "# remove outliers\n",
    "# fig_inp = fig_inp[fig_inp.groupby(['variable', 'group']).value.\n",
    "#       transform(lambda x : (x<x.quantile(0.95))&(x>(x.quantile(0.05)))).eq(1)]\n",
    "fig_inp = fig_inp.sort_values(by = ['cond'], ascending = [False])\n",
    "g = sns.catplot(x=\"ucond\", y=\"value\", hue = 'cond',  data=fig_inp,  kind = 'bar', order = ['1pre', '1post', 'postn'],\n",
    "                    ci = 68,  sharey = False, legend = True, hue_order = ['wt', 'het', 'ko'],\n",
    "#                 order = reversed(['theta', 'alpha', 'beta', 'lg', 'hg']),\n",
    "                   palette = colors_group, \n",
    "                col = 'variable', col_wrap = 2,\n",
    "#                     capsize = 0.02, saturation = 1, col_wrap = 2,\n",
    "                   height = 4, aspect = 1.2 )\n",
    "plt.savefig(\"bar_tf-sert-l4-05-15.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = fig_inp[fig_inp.variable == 'hg']\n",
    "\n",
    "wt_std = tmp[(tmp.cond == 'std3') & (tmp.group == 'wt')].value.dropna().values\n",
    "wt_dev = tmp[(tmp.cond == 'dev9') & (tmp.group == 'wt')].value.dropna().values\n",
    "wt_ctr = tmp[(tmp.cond == 'ctr2') & (tmp.group == 'wt')].value.dropna().values\n",
    "\n",
    "fx_std = tmp[(tmp.cond == 'std3') & (tmp.group == 'fx')].value.dropna().values\n",
    "fx_dev = tmp[(tmp.cond == 'dev9') & (tmp.group == 'fx')].value.dropna().values\n",
    "fx_ctr = tmp[(tmp.cond == 'ctr2') & (tmp.group == 'fx')].value.dropna().values\n",
    "\n",
    "print np.mean(wt_std), sstat.sem(wt_std)\n",
    "print np.mean(wt_dev), sstat.sem(wt_dev)\n",
    "print np.mean(wt_ctr), sstat.sem(wt_ctr)\n",
    "\n",
    "\n",
    "print (len(wt_std), len(fx_std))\n",
    "\n",
    "# print sstat.kruskal(x1, x2, x3)\n",
    "\n",
    "p1 =  sstat.mannwhitneyu(wt_std, wt_dev)[1]\n",
    "p2 = sstat.mannwhitneyu(wt_std, wt_ctr)[1]\n",
    "p3 =  sstat.mannwhitneyu(wt_dev, wt_ctr)[1]\n",
    "\n",
    "p4 =  sstat.mannwhitneyu(fx_std, fx_dev)[1]\n",
    "p5 =  sstat.mannwhitneyu(fx_std, fx_ctr)[1]\n",
    "p6 =  sstat.mannwhitneyu(fx_dev, fx_ctr)[1]\n",
    "\n",
    "p7 =  sstat.mannwhitneyu(wt_std, fx_std)[1]\n",
    "p8 =  sstat.mannwhitneyu(wt_dev, fx_dev)[1]\n",
    "p9 =  sstat.mannwhitneyu(wt_ctr, fx_ctr)[1]\n",
    "\n",
    "pvals = np.array((p1, p2, p3, p4, p5, p6, p7, p8, p9))\n",
    "adj = statsmodels.stats.multitest.multipletests(pvals, method='fdr_bh')\n",
    "print pvals\n",
    "print adj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_band = []\n",
    "bands = ['theta', 'alpha', 'beta', 'lg', 'hg']\n",
    "for idx, var in enumerate(bands):\n",
    "    \n",
    "    tmp = fig_inp[fig_inp.variable == var]\n",
    "    ls_layer = []\n",
    "    for lidx, lval in enumerate(sorted(tmp.layer.unique())):\n",
    "        tmp2 = tmp[tmp.layer == lval]\n",
    "        std = tmp2[tmp2.cond == 'std3'].value.dropna().values\n",
    "        dev = tmp2[tmp2.cond == 'dev9'].value.dropna().values\n",
    "        ctr = tmp2[tmp2.cond == 'ctr2'].value.dropna().values\n",
    "        print var, lval\n",
    "        std_mean, std_sem = np.mean(std), sstat.sem(std)\n",
    "        dev_mean, dev_sem = np.mean(dev), sstat.sem(dev)\n",
    "        ctr_mean, ctr_sem = np.mean(ctr), sstat.sem(ctr)\n",
    "        \n",
    "        \n",
    "        print len(std), len(dev), len(ctr)\n",
    "#         print sstat.kruskal(std, dev, ctr)\n",
    "\n",
    "        std_dev = sstat.mannwhitneyu(std, dev)\n",
    "        std_ctr = sstat.mannwhitneyu(std, ctr)\n",
    "        dev_ctr = sstat.mannwhitneyu(dev, ctr)\n",
    "\n",
    "        data_tmp = pd.DataFrame({'layer': lval, 'band': var, 'stim1': ['std', 'dev', 'ctr'], \n",
    "        'mean': [std_mean, dev_mean, ctr_mean ], 'sem': [std_sem, dev_sem, ctr_sem ], \n",
    "        'n_ch': [len(std), len(dev), len(ctr)], 'group': 'wt', 'comparison': ['std_dev', 'std_ctr', 'dev_ctr'],\n",
    "        'p_val': [std_dev[1], std_ctr[1], dev_ctr[1]], 'u_stat': [std_dev[0], std_ctr[0], dev_ctr[0]],\n",
    "                                 'test': 'MWU' })\n",
    "        ls_layer.append(data_tmp)\n",
    "        \n",
    "        print \"--------------------------\"\n",
    "    print '========================'\n",
    "    ls_band.append(pd.concat(ls_layer))\n",
    "stat_table = pd.concat(ls_band)\n",
    "stat_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['group', 'band', 'layer', 'comparison', 'test', 'u_stat', 'p_val', 'stim1', 'n_ch', 'mean', 'sem' ]\n",
    "stat_table[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tf_df[tf_df.groupby(['variable', 'group']).value.\n",
    "      transform(lambda x : (x<x.quantile(0.95))&(x>(x.quantile(0.05)))).eq(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSD Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.groupby(['variable', 'group'])[\"value\"].quantile([0.05, 0.95]).unstack(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.dstack(tf3d)\n",
    "tmp = tmp.swapaxes(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf3 = np.array_split(tf3d.as_matrix(), 108)\n",
    "# make 3d array from list of 2d arrays\n",
    "\n",
    "tf3_arr = np.dstack(tf3d)\n",
    "tf3_arr = tf3_arr.swapaxes(0,2)\n",
    "tf3_pre = 10*np.log10(tf3_arr/tf3_arr[:,:400,:].mean(axis=1, keepdims=1))\n",
    "# reshape to 2d array for dataframe\n",
    "f,s,tr = tf3_pre.shape\n",
    "tf3d_pre = pd.DataFrame(tf3_pre.transpose(2,0,1).reshape(f*tr,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(5,1, sharex=True, sharey=True, figsize=(8,12))\n",
    "# time = np.linspace(0, 4, 4000 )\n",
    "diff_map = post_map-pre_map\n",
    "sf_col = [0.01, 0.02, 0.04, 0.08, 0.14 ]\n",
    "contour_levels = np.arange(-10, 30, 1)\n",
    "for i, val in enumerate(sf_col):\n",
    "    ax[i].set_yscale('log')\n",
    "    ax[i].set_yticks(np.logspace(np.log10(2),np.log10(80),6))\n",
    "    ax[i].set_yticklabels(np.round(np.logspace(np.log10(2),np.log10(80),6)))\n",
    "    \n",
    "    hm = ax[i].contourf(time, frex, post_map[i], 40,  cmap = 'jet' , extend = 'both' )\n",
    "    ax[i].set_ylabel('Frequency')\n",
    "    cb_tf = plt.colorbar(hm, ax=ax[i])\n",
    "    cb_tf.set_label('dB from baseline')\n",
    "    ax[i].set_title(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T_obs, clusters, cluster_p_values, H0 = mne.stats.permutation_cluster_test([ls_perm[1], ls_perm[0]],\n",
    "                             n_permutations=256, threshold = 15.0,  tail=0)\n",
    "T_obs_plot = np.nan * np.ones_like(T_obs)\n",
    "for c, p_val in zip(clusters, cluster_p_values):\n",
    "    if p_val <= 0.05:\n",
    "        T_obs_plot[c] = T_obs[c]\n",
    "\n",
    "f, ax = plt.subplots(figsize = (5,5))\n",
    "# ax.set_xlim(0,2.5)\n",
    "# ax.set_yscale('log')\n",
    "# hm = ax.set_yticks(np.logspace(np.log10(2),np.log10(80),6))\n",
    "# ax.set_yticklabels(np.round(np.logspace(np.log10(2),np.log10(80),6)))\n",
    "# ax.contourf(time, frex, T_obs.T, \n",
    "           \n",
    "#            aspect='auto', origin='lower', cmap='gray')\n",
    "hm = ax.contourf(time, frex, T_obs_plot.T, \n",
    "           \n",
    "           aspect='auto', origin='lower', cmap='jet')\n",
    "# plt.colorbar(hm)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "# plt.savefig(\"tf_permutation.png\", transparent=True)\n",
    "# plt.savefig(\"tf_permutation_sf_low2.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Papers\\Impaired Visual Familiarity Circuit in Fmr1 KO Mice\\Sam\\Data analysis for figures\\RAW\\Phase Shift\\wtfx_all.hdf5\"\n",
    "df_units = pd.read_hdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurodsp\n",
    "x = lfp\n",
    "Fs = 1000\n",
    "all_pac_methods = ['ozkurt', 'plv', 'glm', 'tort', 'canolty']\n",
    "f_range_lo = (13, 30)\n",
    "f_range_hi = (50, 200)\n",
    "N_seconds_lo = .25\n",
    "N_seconds_hi = .2\n",
    "\n",
    "np.random.seed(0) # The tort and canolty methods uses random methods\n",
    "pacs = np.zeros(len(all_pac_methods))\n",
    "for i, m in enumerate(all_pac_methods):\n",
    "    pacs[i] = neurodsp.compute_pac(x, x, Fs, f_range_lo, f_range_hi,\n",
    "                               N_seconds_lo=N_seconds_lo, N_seconds_hi=N_seconds_hi,\n",
    "                               pac_method=m)\n",
    "    print('PAC strength = {:.4f}, {:s} method'.format(pacs[i], m))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters for comodulogram function\n",
    "f_pha_bin_edges = np.arange(3, 22, 2)\n",
    "f_amp_bin_edges = np.arange(20, 120, 4)\n",
    "N_cycles_pha = 3\n",
    "N_cycles_amp = 11\n",
    "\n",
    "# Compute comodulogram\n",
    "pac = neurodsp.compute_pac_comodulogram(x, x, Fs,\n",
    "                                        f_pha_bin_edges, f_amp_bin_edges,\n",
    "                                        N_cycles_pha=N_cycles_pha,\n",
    "                                        N_cycles_amp=N_cycles_amp,\n",
    "                                        pac_method='ozkurt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comodulogram\n",
    "neurodsp.plot_pac_comodulogram(pac, f_pha_bin_edges, f_amp_bin_edges,\n",
    "                               clim=(0,.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SF decoding from LFP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score, StratifiedKFold\n",
    "from collections import defaultdict\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['group'] = master_df.et.map(groups)\n",
    "master_df['trial_n'] = master_df.index//1e3\n",
    "master_df['rect_LFP'] = abs(master_df.lfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = svm.SVC(kernel='linear', C=1)\n",
    "ls_cond = []\n",
    "rskf = RepeatedStratifiedKFold(n_splits = 4, n_repeats = 5,\n",
    "     random_state = 3)\n",
    "for gr in df2.group.unique():\n",
    "    for w in time_w:\n",
    "        df2_sub = df2[(df2.trial_spikes >= w[0]) & (df2.trial_spikes < w[1]) & (df2.group == gr)]\n",
    "        resp_units = df2_sub.groupby('cluster_id').times.count().reset_index()\n",
    "        resp_units = resp_units[resp_units.times > 10].cluster_id.unique()\n",
    "        tmp = df2_sub[df2_sub.cluster_id.isin(resp_units)].groupby(['cluster_id', \n",
    "                                                'trial_n']).trial_spikes.count().reset_index()\n",
    "        tmp2 = tmp.pivot('trial_n', 'cluster_id', 'trial_spikes')\n",
    "        tmp2 = tmp2.fillna(0)\n",
    "\n",
    "        ls = []\n",
    "# coef_arr = np.zeros((100,5, _data.cuid.unique().size ))\n",
    "        print gr, w\n",
    "\n",
    "        x_ = tmp2.values\n",
    "        y_ = sf_tuning.values()\n",
    "\n",
    "        scores = cross_val_score(lr, x_, y_, cv = rskf)\n",
    "#         print scores\n",
    "        tmp_df = pd.DataFrame({'cv': i, 'group': gr, 'window': w[0], 'acc': scores})\n",
    "        ls_cond.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rskf = RepeatedStratifiedKFold(n_splits = 4, n_repeats = 5,\n",
    "     random_state = 3) \n",
    "lda = LinearDiscriminantAnalysis(solver = 'lsqr', shrinkage = 'auto')\n",
    "# svm = SVC(C=1)\n",
    "y_ = sf_tuning\n",
    "time_w = [(0.35, 0.8), (0.35, 0.45), (0.45, 0.55), (0.55, 0.65), (0.65, 0.75)]\n",
    "ls_cond = []\n",
    "for gr in master_df.group.unique():\n",
    "    for w in time_w:\n",
    "        \n",
    "        df_sub = master_df[(master_df.group == gr) & (master_df.times > w[0]) & (master_df.times < w[1])]\n",
    "#         df_sub = df_sub.groupby(['group', 'rec', 'trial_n', 'times']).mean().reset_index()\n",
    "        df_sub2 = df_sub.groupby(['fname', 'trial_n']).rect_LFP.mean().reset_index()\n",
    "        x_ = df_sub2.pivot('trial_n', 'fname', 'rect_LFP')\n",
    "#         X_scaled = preprocessing.scale(x_)\n",
    "        x_ -= x_.min() \n",
    "        x_ /= x_.max()\n",
    "        scores = cross_val_score(lda, x_, y_, cv = rskf)\n",
    "        \n",
    "        tmp_df = pd.DataFrame({'cv': i, 'group': gr, 'window': w[0], 'acc': scores})\n",
    "        ls_cond.append(tmp_df)\n",
    "out = pd.concat(ls_cond)\n",
    "out.loc[:,'error'] = 1 - out.acc\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = 'window', y = 'error', data = out, hue = 'group', \n",
    "            kind = 'point', height = 6, aspect = 1.5, palette = colors_group , ci = 95)\n",
    "plt.ylim(0, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

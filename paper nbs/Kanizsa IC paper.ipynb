{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-down modulation of illusory contours paper\n",
    "### see https://www.jneurosci.org/content/40/3/648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import scipy.stats as sstat\n",
    "import scipy.signal as ssig\n",
    "import h5py\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import re\n",
    "import ephys_unit_analysis as ena\n",
    "import fnmatch\n",
    "# from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "mpl.rcParams['pdf.fonttype'] = 42 \n",
    "mpl.rcParams['font.sans-serif']=['Arial', 'Helvetica','Bitstream Vera Sans', 'DejaVu Sans', 'Lucida Grande', \n",
    "                                 'Verdana', 'Geneva', 'Lucid', 'Avant Garde', 'sans-serif']  \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# pal=sns.blend_palette([\"black\", \"crimson\"], 2)\n",
    "sns.despine()\n",
    "# current_palette = sns.color_palette(\"colorblind\", 10)\n",
    "# sns.set_palette(current_palette)\n",
    "\n",
    "# for publication quality plots, not bar graphs, use this: \n",
    "def set_pub_plots(pal=sns.blend_palette([\"crimson\",\"gray\", 'cyan',  'purple', 'magenta' ],5)):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_palette(pal)\n",
    "    sns.set_context(\"poster\", font_scale=1.5, rc={\"lines.linewidth\": 2.5, \"axes.linewidth\":2.5, 'figure.facecolor': 'white'}) \n",
    "    sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n",
    "    # optional, makes markers bigger, too, axes.linewidth doesn't seem to work\n",
    "    plt.rcParams['axes.linewidth'] = 2.5\n",
    "\n",
    "rc_pub={'font.size': 25, 'axes.labelsize': 25, 'legend.fontsize': 25.0, \n",
    "    'axes.titlesize': 25, 'xtick.labelsize': 25, 'ytick.labelsize': 25, \n",
    "    #'axes.color_cycle':pal, # image.cmap - rewritesd the default colormap\n",
    "    'axes.linewidth':2.5, 'lines.linewidth': 2.5,\n",
    "    'xtick.color': 'black', 'ytick.color': 'black', 'axes.edgecolor': 'black','axes.labelcolor':'black','text.color':'black'}\n",
    "# to restore the defaults, call plt.rcdefaults() \n",
    "\n",
    "#set_pub_bargraphs()\n",
    "set_pub_plots()\n",
    "\n",
    "\n",
    "\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_immediate_subdirectories(a_dir):\n",
    "    return [name for name in os.listdir(a_dir)\n",
    "            if os.path.isdir(os.path.join(a_dir, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define color palette\n",
    "my_pal = np.zeros((8,3))\n",
    "colors = sns.color_palette(\"Paired_r\", 12)\n",
    "colors = np.array(colors)*0.9\n",
    "my_pal[0] = colors[6]\n",
    "my_pal[1] = colors[7]\n",
    "my_pal[2] = colors[8]\n",
    "my_pal[3] = colors[9]\n",
    "my_pal[4] = colors[10]\n",
    "my_pal[5] = colors[11]\n",
    "my_pal[6] = colors[2]\n",
    "my_pal[7] = colors[3]\n",
    "colors = my_pal[:]\n",
    "# colors = colors[::2]\n",
    "sns.palplot(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract frame indices from frames\n",
    "# generate STA RF maps\n",
    "def avg_rf_on_off(_input, d_rf, et, rec):\n",
    "    d_frames_idx = {}\n",
    "    df_rez = _input\n",
    "    for unit in sorted(df_rez.cluster_id.unique()[:]):\n",
    "        cluster_id = str(unit) + '005' + 'et' + str(et) + str(rec) \n",
    "        tmp = df_rez[df_rez.cluster_id == unit]\n",
    "        # account for latency and 120 empty frames, convert to frames idx\n",
    "        tmp =  np.rint((tmp.times.values[:] + 0.05 -2)*60)\n",
    "\n",
    "    #     frame_idx = [round_to_frame(x) for x in tmp]\n",
    "        frame_idx = ((tmp//15))\n",
    "        frame_idx = frame_idx[frame_idx>-1]\n",
    "        frame_idx = frame_idx[frame_idx < 3002]\n",
    "        frame_idx = frame_idx.astype(int)\n",
    "\n",
    "        unique, counts = np.unique(frame_idx, return_counts=True)\n",
    "        d_frames_idx = dict(zip(unique, counts))\n",
    "        avg_frame_on = np.average(frames_arr_on[d_frames_idx.keys()],axis = 0, weights=d_frames_idx.values())\n",
    "        avg_frame_off = np.average(frames_arr_off[d_frames_idx.keys()],axis = 0, weights=d_frames_idx.values())\n",
    "        d_rf['on'][cluster_id] = avg_frame_on\n",
    "        d_rf['off'][cluster_id] = avg_frame_off\n",
    "    return d_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor specs\n",
    "from math import atan2, degrees\n",
    "mon_resolution = (1080,1920) # vert, horiz\n",
    "mon_width_cm = 48.1 #enter your monitors width in cm\n",
    "mon_height_cm = 28.5 #enter your monitors height in cm\n",
    "\n",
    "mon_width_cm = 59.7 #VS big monitor\n",
    "mon_height_cm = 33.5 #VS big monitor\n",
    "\n",
    "monitor_distance = 17 # cm\n",
    "\n",
    "probe_in_px = 48 #lsn probe/square size\n",
    "cir_r = 100 #opto experiments\n",
    "sqr = 600 # opto exp\n",
    "\n",
    "# sqr = 400 # kic-sqr-ori\n",
    "# cir_r = 80 # kic-sqr-ori experiments\n",
    "\n",
    "line_w = 10\n",
    "kic = sqr - 2*cir_r\n",
    "deg_per_px = degrees(atan2(.5*mon_height_cm, monitor_distance)) / (.5*mon_resolution[0])\n",
    "print '%s degrees correspond to a single pixel' % deg_per_px\n",
    "probe_in_deg = probe_in_px * deg_per_px\n",
    "cir_r_deg = cir_r * deg_per_px\n",
    "sqr_deg = sqr * deg_per_px\n",
    "kic_deg = kic * deg_per_px\n",
    "line_w_deg = line_w * deg_per_px\n",
    "print 'The size of the stimulus is %s pixels and %s visual degrees' \\\n",
    "    % (probe_in_px, probe_in_deg)\n",
    "print 'rad of circular disc', cir_r_deg\n",
    "print 'sqr', sqr_deg, 'kic', kic_deg\n",
    "print 'line width', line_w_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dics to map stimuli to trials\n",
    "trial_seq = [3, 3, 1, 4, 3, 4, 4, 1, 4, 4, 2, 1, 4, 4, 3, 4, 4, 3, 2, 4, 3, 2, 3, 4, 4, 2, 2, 3, 4, 4, 4, 4, 2, 2, 3, 1, 2, 3, 1, 1, 2, 4, 2, 4, 1, 2, 4, 2, 4, 4, 3, 2, 4, 3, 3, 2, 3, 2, 1, 1, 4, 4, 2, 4, 4, 3, 2, 3, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 4, 2, 4, 2, 3, 3, 2, 1, 3, 3, 2, 1, 3, 1, 2, 3, 4, 4, 2, 4, 1, 3, 2, 1, 1, 4, 4, 3, 1, 3, 3, 4, 1, 1, 3, 3, 3, 2, 3, 1, 1, 4, 2, 1, 4, 4, 4, 2, 2, 2, 4, 3, 4, 3, 2, 1, 1, 2, 1, 4, 3, 4, 3, 2, 3, 1, 3, 1, 2, 3, 2, 1, 1, 4, 1, 1, 1, 1, 3, 2, 1, 1, 3, 3, 3, 2, 1, 1, 1, 3, 2, 2, 3, 1, 4, 2, 1, 2, 4, 1, 3, 2, 4, 4, 3, 2, 4, 2, 3, 2, 1, 4, 3, 3, 1, 3, 2, 2, 1, 2, 2, 4]\n",
    "opto_idx = [1, 2, 3, 5, 9, 10, 12, 15, 21, 24, 25, 28, 29, 31, 32, 34, 35, 36, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 54, 57, 62, 64, 67, 68, 71, 76, 79, 80, 81, 84, 86, 87, 88, 91, 92, 96, 98, 99, 102, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 120, 121, 122, 128, 129, 131, 133, 138, 139, 140, 142, 143, 144, 147, 148, 149, 150, 151, 153, 155, 158, 159, 161, 162, 163, 165, 170, 172, 173, 177, 181, 183, 186, 188, 189, 190, 192, 194, 196, 197, 198]\n",
    "trial_seq = np.array(trial_seq)\n",
    "short_seq = np.array(trial_seq[opto_idx])\n",
    "opto_seq = [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
    "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
    "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
    "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
    "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
    "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
    "       0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
    "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
    "       1, 0]\n",
    "opto_seq = np.array(opto_seq)\n",
    "dir_12 = np.array([10, 7, 3, 2, 4, 8, 9, 5, 7, 3, 4, 8, 3, 2, 1, 8, 0, 4, 9, 11, \n",
    "10, 9, 1, 11, 4, 0, 7, 1, 2, 8, 2, 9, 11, 9, 6, 5, 10, 4, 9, 0, 7, 11, 9, \n",
    "5, 9, 10, 11, 6, 8, 9, 5, 4, 2, 8, 11, 2, 10, 3, 5, 1, 7, 0, 4, 9, 1, 5, \n",
    "11, 3, 5, 10, 1, 2, 9, 6, 2, 2, 11, 5, 10, 7, 3, 7, 4, 6, 8, 4, 1, 8, 0, \n",
    "11, 0, 6, 2, 11, 1, 10, 3, 8, 3, 1, 2, 10, 5, 3, 11, 1, 7, 3, 4, 7, 8, 4, 6, \n",
    "7, 11, 7, 0, 8, 6, 10, 4, 5, 7, 2, 10, 3, 5, 9, 8, 6, 3, 2, 0, 11, 0, 6, 10, \n",
    "0, 7, 4, 5, 0, 10, 6, 8, 10, 3, 11, 9, 0, 5, 1, 3, 7, 0, 6, 9, 1, 6, 10, 5, \n",
    "6, 11, 7, 0, 5, 1, 4, 1, 6, 8, 2, 9, 2, 8, 3, 0, 4, 6, 1])\n",
    "d_dir = {}\n",
    "for i, v in enumerate(dir_12):\n",
    "    d_dir[i] = v \n",
    "kic_ori_seq = ['a1', 'b2', 'a1', 'd2', 'd4', 'b2', 'a1', 'b1', 'd3', 'a2', 'b3', 'a4', 'd1', 'd2', 'd1', 'a4', 'd4', 'd1', 'a1', 'c4', 'c1', 'b1', 'c1', 'a3', 'd4', 'b2', 'd1', 'd2', 'a3', 'd4', 'd3', 'b3', 'b3', 'b3', 'a3', 'a3', 'a3', 'a4', 'd1', 'd3', 'a2', 'b1', 'c3', 'a4', 'b4', 'd3', 'b4', 'c4', 'c4', 'b2', 'c2', 'b2', 'a4', 'd4', 'c4', 'b3', 'd4', 'c4', 'b2', 'b1', 'c3', 'd1', 'd4', 'b3', 'c2', 'b1', 'c4', 'a1', 'a3', 'd3', 'a4', 'd1', 'a3', 'c4', 'a2', 'a3', 'b4', 'd4', 'c3', 'c2', 'b4', 'b2', 'd4', 'c1', 'a4', 'a2', 'c4', 'b2', 'b4', 'c3', 'd3', 'c3', 'c4', 'a2', 'd3', 'a4', 'a1', 'b1', 'c2', 'd4', 'd2', 'a4', 'd3', 'a3', 'c2', 'a1', 'a2', 'a3', 'b2', 'd1', 'b4', 'a3', 'd4', 'b4', 'a2', 'a2', 'd2', 'c1', 'd1', 'b3', 'c1', 'b3', 'a3', 'd2', 'b1', 'c4', 'd1', 'b2', 'a3', 'c3', 'c2', 'a4', 'd1', 'c3', 'd3', 'b2', 'c1', 'a2', 'b4', 'c1', 'd3', 'c1', 'b2', 'c2', 'd4', 'c1', 'c1', 'b1', 'c3', 'b3', 'a1', 'b4', 'c2', 'a4', 'd2', 'a1', 'b4', 'b4', 'c3', 'b3', 'c2', 'c1', 'b4', 'b1', 'b2', 'b3', 'b3', 'b3', 'b4', 'd2', 'b3', 'c3', 'c4', 'c2', 'd2', 'd3', 'a2', 'd1', 'b2', 'a2', 'b4', 'a4', 'a1', 'c3', 'c2', 'b3', 'd2', 'b2', 'd3', 'b4', 'c1', 'd4', 'd1', 'd4', 'b1', 'c2', 'b3', 'a2', 'c3', 'c4', 'd4', 'a2', 'a1', 'b2', 'd1', 'd4', 'a2', 'a4', 'c3', 'a2', 'b3', 'a2', 'd3', 'b2', 'b1', 'a3', 'c2', 'd2', 'd3', 'c4', 'd3', 'c4', 'd4', 'b1', 'c4', 'a1', 'c1', 'a4', 'd2', 'c3', 'a1', 'c4', 'a3', 'c4', 'a1', 'd1', 'd3', 'a3', 'a1', 'c4', 'd1', 'a3', 'c3', 'c2', 'a2', 'c4', 'b1', 'd3', 'a3', 'd4', 'd2', 'd2', 'b2', 'b3', 'c1', 'c2', 'b3', 'd2', 'a3', 'c3', 'a4', 'c2', 'a1', 'a2', 'a1', 'c4', 'c1', 'c2', 'c1', 'c2', 'b1', 'a1', 'd1', 'd1', 'b4', 'd2', 'd4', 'd4', 'c4', 'd4', 'a2', 'c4', 'b1', 'b1', 'a1', 'b4', 'b3', 'b4', 'c3', 'd2', 'c4', 'c1', 'a2', 'b3', 'c3', 'a4', 'b3', 'd1', 'b1', 'b1', 'd2', 'b3', 'b2', 'a3', 'a4', 'c3', 'b1', 'a4', 'd4', 'd1', 'b1', 'd2', 'd3', 'c1', 'b1', 'a3', 'a1', 'c1', 'd1', 'b2', 'c3', 'b1', 'b3', 'c1', 'a4', 'd2', 'd4', 'b4', 'd2', 'b2', 'a4', 'd1', 'd2', 'b4', 'b4', 'd4', 'c3', 'a3', 'a4', 'd4', 'b2', 'a2', 'b2', 'c2', 'a2', 'a3', 'c2', 'b2', 'a4', 'd3', 'a4', 'b1', 'a2', 'c1', 'b2', 'd3', 'c1', 'd1', 'd3', 'd2', 'a4', 'c2', 'a1', 'c3', 'd1', 'c3', 'b1', 'c1', 'd2', 'd3', 'b4', 'a4', 'c4', 'a2', 'c2', 'd3', 'a1', 'c1', 'b1', 'c3', 'b4', 'c4', 'a2', 'a3', 'c1', 'b4', 'c2', 'c2', 'd3', 'c3', 'a3', 'a1', 'd3', 'd2', 'b3', 'a1', 'd1', 'c2', 'b4', 'a1']\n",
    "d_kic_ori = {}\n",
    "for i, v in enumerate(kic_ori_seq):\n",
    "    d_kic_ori[i] = v \n",
    "# dir8_seq = np.array([2, 0, 7, 5, 6, 3, 3, 1, 4, 5, 4, 7, 4, 7, 5, 2, 2, 2, 2, 1, 1, 7,\n",
    "#        5, 3, 1, 1, 3, 4, 2, 3, 6, 6, 0, 5, 1, 2, 7, 6, 5, 3, 5, 4, 5, 6,\n",
    "#        5, 4, 4, 6, 6, 4, 0, 2, 7, 7, 0, 4, 5, 1, 2, 4, 3, 3, 4, 5, 7, 5,\n",
    "#        4, 7, 2, 4, 5, 7, 0, 1, 2, 2, 3, 6, 1, 3, 5, 1, 5, 1, 2, 6, 0, 1,\n",
    "#        7, 1, 0, 6, 7, 6, 5, 3, 3, 2, 5, 7, 0, 4, 3, 0, 1, 4, 1, 3, 5, 2,\n",
    "#        0, 1, 6, 0, 5, 7, 1, 7, 3, 0, 6, 7, 0, 2, 5, 2, 3, 7, 4, 3, 3, 6,\n",
    "#        3, 1, 2, 1, 5, 7, 6, 0, 3, 3, 0, 4, 0, 7, 1, 2, 0, 3, 0, 0, 6, 5,\n",
    "#        2, 5, 4, 2, 7, 0, 1, 3, 2, 7, 6, 6, 1, 0, 1, 5, 6, 0, 3, 2, 6, 4,\n",
    "#        7, 0, 7, 4, 6, 5, 4, 1, 6, 2, 4, 4, 2, 4, 7, 6, 0, 4, 3, 1, 6, 6,\n",
    "#        0, 7])\n",
    "# for i, v in enumerate(dir8_seq):\n",
    "#     d_dir[i] = v "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [] # list of experiment folders\n",
    "source_folder = r\"U:\\Data\\pak6\\OpenEphys\\probe_64DB\\WT\\illusory_contours\\Abutting lines\\set1\\right2\"\n",
    "#for source_folder in source_folders:\n",
    "for root, dirnames, filenames in os.walk(source_folder):\n",
    "    for filename in fnmatch.filter(filenames, '*rez.mat'):\n",
    "        matches.append(os.path.join(root, filename))\n",
    "print matches[:5]\n",
    "print matches[-5:]\n",
    "print len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "labels = []\n",
    "for path in matches:\n",
    "    path = os.path.split(path)[0]\n",
    "    path_cluster_groups = os.path.join(path, 'cluster_groups.csv')\n",
    "    cluster_groups = pd.read_csv(path_cluster_groups, sep = '\\t')\n",
    "    spike_times = np.load(os.path.join(path, 'spike_times.npy'))\n",
    "    spike_clusters = np.load(os.path.join(path, 'spike_clusters.npy'))\n",
    "    templates = np.load(os.path.join(path, 'templates.npy'))\n",
    "    spike_templates = np.load(os.path.join(path, 'spike_templates.npy'))\n",
    "    df = pd.DataFrame({'sample':spike_times.flatten(), \n",
    "                   'cluster_id':spike_clusters.flatten(), \n",
    "                   'templates':spike_templates.flatten() })\n",
    "    \n",
    "    for idx, i in enumerate(df.cluster_id.unique()):\n",
    "        tmt_id = df[df.cluster_id==i].templates.unique().tolist()\n",
    "        tmt_arr = templates[tmt_id]\n",
    "        tmt_arr = np.mean(tmt_arr, axis=0)\n",
    "        tmp = tmt_arr.flatten()\n",
    "        ls.append(tmp)\n",
    "        labels.append(cluster_groups[cluster_groups.cluster_id==i].group.values[0])\n",
    "out = np.matrix(ls)   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [ r\"U:\\Data\\pak6\\OpenEphys\\probe_64DA\\WT\\illusory contours\\KIC-sqr-ori\\set1\\KICsqrori2-left1\\ksort2\",\n",
    "           r\"U:\\Data\\pak6\\OpenEphys\\probe_64DA\\WT\\illusory contours\\KIC-sqr-ori\\set1\\KICsqrori2-right2\\ksort2\",\n",
    "           r\"U:\\Data\\pak6\\OpenEphys\\probe_64DA\\WT\\illusory contours\\KIC-sqr-ori\\set2\\right\\KICsqrori6-right2\\ksort2\",\n",
    "           r\"U:\\Data\\pak6\\OpenEphys\\probe_64DA\\WT\\illusory contours\\KIC-sqr-ori\\set2\\left\\KICsqrori6-left1\\ksort2\",\n",
    "           r\"U:\\Data\\pak6\\OpenEphys\\probe_64DA\\WT\\illusory contours\\KIC-sqr-ori\\set2\\left\\KICsqrori8-left1\\ksort2\",\n",
    "]\n",
    "for path in matches[:]:\n",
    "    et = path.split('\\\\')[-2].split()[0]\n",
    "    print et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally sparse noise rf mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data\\pak6\\OpenEphys\\probe_64DA\\WT\\illusory contours\\KIC-sqr-ori\\set1\\KICsqrori1-left1\\ksort2\"\n",
    "# path = r\"u:\\Data\\tang232\\Visual_field\\09.30.2017 RF+Size\\openephys\\10.07.2017_CC#_ET#05_post\\001 4x4 RF_2017-10-07_14-09-47\"\n",
    "# path = r\"U:\\Data\\tang232\\Receptive_field\\4X4 RF Mapping+ Size\\07.01.2017_CC#_ET#400_pre\\conc\"\n",
    "path_cluster_groups = os.path.join(path, 'cluster_groups.csv')\n",
    "cluster_groups = pd.read_csv(path_cluster_groups, sep = '\\t')\n",
    "# good_units = cluster_groups[cluster_groups.group != 'noise'].cluster_id.values\n",
    "noise_units = cluster_groups[(cluster_groups['group'] == 'noise') | (cluster_groups['group'] == 'mua')].cluster_id.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in matches[:]:\n",
    "    et = fname.split('\\\\')[-2].split()[0]\n",
    "    rec = fname.split('\\\\')[-2].split()[1]\n",
    "    print et, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receptive field mapping\n",
    "# d_rf = {}\n",
    "# d_rf['on'] = {}\n",
    "# d_rf['off'] = {}\n",
    "\n",
    "ls_psth = []\n",
    "ls_spikes = []\n",
    "for path in matches[:]:\n",
    "    path = r\"U:\\Data\\pak6\\OpenEphys\\probe_64DA\\WT\\illusory contours\\KIC-ori\\right\\KICori2-right1\\conc\"\n",
    "    et = path.split('\\\\')[-2].split()[0]\n",
    "    \n",
    "    path_cluster_groups = os.path.join(path, 'cluster_groups.csv')\n",
    "    cluster_groups = pd.read_csv(path_cluster_groups, sep = '\\t')\n",
    "    noise_units = cluster_groups[(cluster_groups['group'] == 'noise') | \n",
    "                                 (cluster_groups['group'] == 'mua')].cluster_id.values\n",
    "    spike_times = np.load(os.path.join(path, 'spike_times.npy'))\n",
    "    spike_clusters = np.load(os.path.join(path, 'spike_clusters.npy'))\n",
    "    templates = np.load(os.path.join(path, 'templates.npy'))\n",
    "    spike_templates = np.load(os.path.join(path, 'spike_templates.npy'))\n",
    "    df = pd.DataFrame({'times':spike_times.flatten()/30000.0, \n",
    "                       'cluster_id':spike_clusters.flatten(), \n",
    "                       'templates':spike_templates.flatten() })\n",
    "\n",
    "    df = df[~df.cluster_id.isin(noise_units)]\n",
    "    df_rez = df\n",
    "#     break\n",
    "#     df_rez['trial_n'] = df_rez.times//4\n",
    "    \n",
    "    df2 = df[df.times <= 20]\n",
    "    \n",
    "    df3 = df[(df.times > 20)   & (df.times<= 220)]\n",
    "    df3.times = df3.times - 20\n",
    "\n",
    "    df4 = df[ (df.times > 220) & (df.times <= 1000)]\n",
    "    df4.times = df4.times - 220\n",
    "    \n",
    "    df5 = df[ (df.times > 1000)]\n",
    "    df5.times = df5.times - 1000\n",
    "    break\n",
    "\n",
    "#     df6 = df[df.times > 1000 ]\n",
    "#     df6.times = df6.times - 1000\n",
    "\n",
    "# #     df2['trial_n'] = df2.times//4.0\n",
    "# #     df3['trial_n'] = df3.times//60.0\n",
    "#     df5['trial_n'] = df5.times//2.0\n",
    "#     df6['trial_n'] = df6.times//2.0\n",
    "    \n",
    "#     df5['stim1'] = df5['trial_n'].map(d_stim)\n",
    "#     df6['stim1'] = df6['trial_n'].map(d_stim)\n",
    "    \n",
    "#     df_g1 = df_rez\n",
    "#     df_g1['et'] = '476'\n",
    "#     df_g1['rec'] = 'left-1'\n",
    "#     df_g1['fname'] = path\n",
    "    df5['trial_n'] = df5.times//2.0\n",
    "    df5['stim1'] = df5['trial_n'].map(d_kic_ori)\n",
    "    df5['trial_spikes'] = df5.times - df5['trial_n']*2\n",
    "    tmp_psth0, _ ,_ = g1_psth(df5)\n",
    "    break\n",
    "#     tmp_psth0['et'] = et\n",
    "#     tmp_psth0['cluster_id'] = tmp_psth0['cluster_id'].astype('str') + \"-\" + et\n",
    "#     tmp_psth0['fname'] = path\n",
    "# #     ls_spikes.append(tmp_spikes0)\n",
    "#     ls_psth.append(tmp_psth0)\n",
    "    \n",
    "#     tmp_psth1, _, _ = g1_psth(df6)\n",
    "#     ls_spikes.append(tmp_spikes1)\n",
    "#     ls_psth.append(tmp_psth1)\n",
    "    \n",
    "#     df_rf = df3\n",
    "#     d_rf = avg_rf_on_off(df_rf, d_rf , et, rec)\n",
    "    \n",
    "#     df_kic = df4\n",
    "#     df_kic2 = df5\n",
    "# g1_ms_psth_v1 = pd.concat(ls_psth)\n",
    "# g1_ms_spikes_v1 = pd.concat(ls_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kic_sqr_ori_psth.to_pickle('kic_sqr_ori_psth.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSTH for Direction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_tuning(data, et, angle_step, paradigm, d_dir):\n",
    "    \n",
    "    data['trial_n'] = data.times//1.0\n",
    "    data['stim1'] = data['trial_n'].map(d_dir)\n",
    "    data['trial_spikes'] = data.times - data['trial_n']*1\n",
    "    data['angle'] = data.stim1*30\n",
    "\n",
    "    for unit in data['cluster_id'].unique():\n",
    "        tmp = data[data.cluster_id == unit]\n",
    "        for idx, val in enumerate(sorted(tmp.stim1.unique())):\n",
    "            tmp2 = tmp[tmp.stim1 == val]\n",
    "            df = ena.getRaster_kilosort(tmp2, unit, trial_length) \n",
    "    #         if len(df.times) < 100:\n",
    "    #             continue\n",
    "            cluster_id = str(unit) + '-' + str(et) \n",
    "            cuid = str(unit) + str('et') + str(et) +  str(val)\n",
    "            h, ttr = ena.PSTH(df.times, th_bin, trial_length, trials_number) # all times rescaled to 0-4 this is why trias number 1.0\n",
    "            zscore = sstat.mstats.zscore(h)\n",
    "            mean = np.mean(h[:30])\n",
    "            std = np.std(h[:30])\n",
    "            if mean==0:\n",
    "                std=1\n",
    "            ztc = (h - mean)/std\n",
    "\n",
    "            df_psth_tmp = pd.DataFrame({     'times':ttr,    'Hz':h,  'stim1': val , 'paradigm': paradigm,\n",
    "                                  'et': et,   'cluster_id':cluster_id, 'abs_times': ttr + val*1, 'angle': val*angle_step,\n",
    "                                    'zscore':zscore, 'ztc':ztc, 'cuid':cuid  })\n",
    "            ls_psth.append(df_psth_tmp)\n",
    "\n",
    "    df_tuning = pd.concat(ls_psth)\n",
    "    return df_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_number = 15.0\n",
    "trial_length = 1.0\n",
    "th_bin = 0.01\n",
    "ls_psth = []\n",
    "ls_tuning = []\n",
    "\n",
    "for path in sorted(matches)[1:]:\n",
    "    path = os.path.split(path)[0]\n",
    "\n",
    "    et = path.split('\\\\')[-2].split()[0]\n",
    "    \n",
    "    path_cluster_groups = os.path.join(path, 'cluster_groups.csv')\n",
    "    cluster_groups = pd.read_csv(path_cluster_groups, sep = '\\t')\n",
    "    noise_units = cluster_groups[(cluster_groups['group'] == 'noise') | \n",
    "                                 (cluster_groups['group'] == 'mua')].cluster_id.values\n",
    "    spike_times = np.load(os.path.join(path, 'spike_times.npy'))\n",
    "    spike_clusters = np.load(os.path.join(path, 'spike_clusters.npy'))\n",
    "    templates = np.load(os.path.join(path, 'templates.npy'))\n",
    "    spike_templates = np.load(os.path.join(path, 'spike_templates.npy'))\n",
    "    df = pd.DataFrame({'times':spike_times.flatten()/30000.0, \n",
    "                       'cluster_id':spike_clusters.flatten(), \n",
    "                       'templates':spike_templates.flatten() })\n",
    "\n",
    "    df = df[~df.cluster_id.isin(noise_units)]\n",
    "\n",
    "    df1 = df[ (df.times > 20) & (df.times <= 200)]\n",
    "    df1.times = df1.times - 20\n",
    "    tmp_tun = dir_tuning(df1, et ,45, 'lg1', d_dir)\n",
    "    \n",
    "    df2 = df[ (df.times > 200) & (df.times <= 380)]\n",
    "    df2.times = df2.times - 200\n",
    "    tmp_tun2 = dir_tuning(df2, et ,45, 'lg2', d_dir)\n",
    "    \n",
    "    df3 = df[ (df.times > 380) & (df.times <= 560)]\n",
    "    df3.times = df3.times - 380\n",
    "    tmp_tun3 = dir_tuning(df3, et, 45, 'lg3', d_dir)\n",
    "    \n",
    "    \n",
    "    df4 = df[ (df.times > 560) & (df.times <= 740)]\n",
    "    df4.times = df4.times - 560\n",
    "    tmp_tun4 = dir_tuning(df4, et ,30, 'ic1', d_dir)\n",
    "    \n",
    "    df5 = df[ (df.times > 740) & (df.times <= 920)]\n",
    "    df5.times = df5.times - 740\n",
    "    tmp_tun5 = dir_tuning(df5, et, 30, 'ic2', d_dir)\n",
    "    \n",
    "    df6 = df[ (df.times > 920) & (df.times <= 1100)]\n",
    "    df6.times = df6.times - 920\n",
    "    tmp_tun6 = dir_tuning(df6, et, 30, 'ic3', d_dir)\n",
    "    \n",
    "    df_tun = pd.concat([ tmp_tun, tmp_tun2, tmp_tun3, tmp_tun4, tmp_tun5, tmp_tun6 ])\n",
    "    ls_psth.append(df_tun)\n",
    "\n",
    "master_tun = pd.concat(ls_psth)\n",
    "master_tun = master_tun.dropna()\n",
    "master_tun['ori'] = master_tun['angle'] % 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\Kanizsa paper ephys\\kic_sqr_ori_tuning.pkl\"\n",
    "kic_sqr_ori_tuning = pd.read_pickle(path)\n",
    "kic_sqr_ori_tuning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp2.paradigm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_tun['cuid'] = master_tun.cluster_id.astype('str') + master_tun.paradigm\n",
    "_inp = master_tun[(master_tun.times > 0.35) & (master_tun.times < 0.8)]\n",
    "_inp2 = _inp.groupby(['paradigm', 'cluster_id', 'cuid' , 'ori']).mean().reset_index()\n",
    "base = master_tun[(master_tun.times > 0.05) & (master_tun.times < 0.35)].groupby(['paradigm', \n",
    "                        'cluster_id', 'cuid' , 'ori']).Hz.mean().values\n",
    "# _inp2['bs_fr'] = _inp2.Hz - base\n",
    "df_pref = _inp2.sort_values('Hz', ascending=False).groupby(['cuid'], as_index=False).first()\n",
    "pref_ori = dict(zip(df_pref.cuid, df_pref.ori))\n",
    "_inp2['pref_ori'] = _inp2.cuid.map(pref_ori)\n",
    "# _inp2 = _inp2.groupby('cluster_id').apply(get_osi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(4,3)\n",
    "B = np.sum(A, axis = 1, keepdims = True)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot single unit responses\n",
    "_inp = df_tuning[(df_tuning.times > 0.35) & (df_tuning.times < 0.8)]\n",
    "_inp = _inp.groupby(['cluster_id', 'angle']).mean().reset_index()\n",
    "sem = _inp.groupby(['cluster_id', 'angle']).sem().reset_index()\n",
    "for unit in sorted(_inp.cluster_id.unique())[10:20]:\n",
    "    tmp = _inp[_inp.cluster_id == unit]\n",
    "    sem2 = sem[sem.cluster_id == unit]\n",
    "    \n",
    "    tmp2 = tmp.append(tmp.iloc[0])\n",
    "    sem3 = sem2.append(sem2.iloc[0])\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    \n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.plot((tmp2.angle/360)*2*np.pi, abs(tmp2.Hz), label = unit, color = 'k')\n",
    "    tmp2['a'] = tmp2.Hz + sem3.Hz\n",
    "    tmp2['b'] = tmp2.Hz - sem3.Hz\n",
    "    ax.fill_between((tmp2.angle/360)*2*np.pi, tmp2.a, tmp2.b, alpha=0.2, color = 'gray')\n",
    "    ax.set_xticks(np.arange(0,2*np.pi,np.pi/4))\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "#     plt.savefig('polar_dir_tuning-261-kicsqrori2-left1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(_inp2.groupby('cluster_id').osi.mean().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute osi\n",
    "def get_osi(data):\n",
    "    tmp = data[data.paradigm == 'lg']\n",
    "    pref_ori = tmp.pref_ori.unique()[0]\n",
    "    orth_ori = (pref_ori + 90)%180\n",
    "    pref_fr = tmp[tmp.ori == pref_ori].Hz.values\n",
    "    orth_fr = tmp[tmp.ori == orth_ori].Hz.values\n",
    "    osi = (pref_fr - orth_fr)/(pref_fr + orth_fr)\n",
    "    data['osi'] = osi[0]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _inp = df_tun[(df_tun.times > 0.35) & (df_tun.times < 0.6)]\n",
    "# _inp = _inp.groupby(['paradigm', 'ori' ,'cluster_id']).mean().reset_index()\n",
    "# sem = _inp.groupby(['cluster_id', 'angle']).sem().reset_index()\n",
    "for unit in sorted(_inp2.cluster_id.unique())[:]:\n",
    "    tmp = _inp2[_inp2.cluster_id == unit]\n",
    "    tmp2 = tmp[tmp.paradigm == 'lg3']\n",
    "    tmp3 = tmp[tmp.paradigm == 'ic3']\n",
    "#     tmp4 = tmp[tmp.paradigm == 'lg3']\n",
    "#     osi = tmp2.osi.values[0]\n",
    "    f, ax = plt.subplots()\n",
    "    x1 = tmp2.ori.unique()\n",
    "    x2 = tmp3.ori.unique()\n",
    "    x3 = tmp4.ori.unique()\n",
    "    \n",
    "    ax.plot( x1, tmp2.Hz, label = tmp2.pref_ori.values[0], color = 'k')\n",
    "    ax.plot( x2, tmp3.Hz, label = tmp3.pref_ori.values[0], color = 'red')\n",
    "#     ax.plot( x3, tmp4.Hz, label = tmp4.pref_ori.values[0], color = 'blue')\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "#     plt.savefig('polar_dir_tuning-261-kicsqrori2-left1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_fr_units = _inp.groupby('cluster_id').Hz.mean().reset_index()\n",
    "high_fr_units = high_fr_units[high_fr_units.Hz < 5].cluster_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _inp = _inp[_inp.cluster_id.isin(high_fr_units)]\n",
    "# _inp = _inp[_inp.layer == 'l2_3']\n",
    "idx = _inp.groupby('cluster_id').Hz.transform(max) == _inp['Hz']\n",
    "grat_pref = dict(zip(_inp[idx].cluster_id, (_inp[idx].angle) ))\n",
    "grat_opp = {} \n",
    "for k in grat_pref.keys():\n",
    "    pref = grat_pref[k]\n",
    "    if pref >= 180:\n",
    "        opp_dir = pref - 180\n",
    "    else:\n",
    "        opp_dir = pref + 180\n",
    "    grat_opp[k] = opp_dir\n",
    "df_tun['pref_dir'] = df_tun.cluster_id.map(grat_pref)\n",
    "df_tun['opp_dir'] = df_tun.cluster_id.map(grat_opp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g1_psth(data):\n",
    "    ls = []\n",
    "    ls_tmt = []\n",
    "    ls_spikes = []\n",
    "#     exp  = fname.split('_')[0]\n",
    "#     data['opto'] = data.trial_n.map(d_opto)\n",
    "    data['opto'] = 0\n",
    "#     data['stim1'] = data.trial_n.map(d_stim)\n",
    "    data['trial_spikes'] = data.times - data.trial_n*2\n",
    "    for i, v in enumerate(sorted(data.opto.unique())): \n",
    "        tmp = data[data['opto'] == v ]\n",
    "        opto_time = 0\n",
    "        if v == 1:\n",
    "            opto_time = 8\n",
    "        for idx, val in enumerate(sorted(tmp.stim1.unique())): \n",
    "            tmp2 = tmp[tmp['stim1'] == val ]\n",
    "            \n",
    "            for unit in tmp2['cluster_id'].unique():\n",
    "                df = ena.getRaster_kilosort(tmp2, unit, 2.0) \n",
    "                if len(df.times) < 10:\n",
    "                    continue\n",
    "        \n",
    "#                 cluster_id = str(unit) + str(exp) + 'et' + str(et) + str(rec) \n",
    "#                 cuid =  str(cluster_id) + 'stim' + str(val) + 'opto' + str(v)\n",
    "                \n",
    "#                 tmt, depth, ch_idx = ena.ksort_get_tmt(tmp2, unit, templates, channel_groups)\n",
    "#                 df_tmt_tmp = pd.DataFrame({'tmt':tmt,  'cluster_id' : cluster_id, 'path': path })\n",
    "#                 ls_tmt.append(df_tmt_tmp)\n",
    "#                 tmp3 = tmp2[tmp2.cluster_id == unit]\n",
    "#                 tmp3['cluster_id'] = cluster_id\n",
    "#                 tmp3['stim1'] = str(val) + str(v)\n",
    "#                 ls_spikes.append(tmp3)\n",
    "                \n",
    "                #df = tmp[tmp.cluster_id==unit]\n",
    "                #df.trial_st = df.trial_st-t_idx[0]\n",
    "     \n",
    "                h, ttr = ena.PSTH(df.times, 0.01, 2.0, 25.0) # all times rescaled to 0-4 this is why trias number 1.0\n",
    "                zscore = sstat.mstats.zscore(h)\n",
    "                mean = np.mean(h[:50])\n",
    "                std = np.std(h[:50])\n",
    "                if mean==0:\n",
    "                    std=1\n",
    "                ztc = (h - mean)/std\n",
    "\n",
    "                df_psth = pd.DataFrame({     'times':ttr,    'Hz':h, \n",
    "                                            'stim1': str(val) + str(v),  'opto':v, 'abs_times': idx*2.0 + ttr + opto_time,\n",
    "                                        'zscore':zscore, 'ztc':ztc,   'cluster_id':unit,  })\n",
    "                ls.append(df_psth)\n",
    "            \n",
    "\n",
    "    \n",
    "    df_psth = pd.concat(ls)\n",
    "  \n",
    "    df_spikes = 0\n",
    "    df_tmt = 0\n",
    "    return df_psth, df_spikes, df_tmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_probe_idx(fname):\n",
    "    searchObj=re.search(r'probes_[\\d]+', fname)\n",
    "    return int(searchObj.group()[7:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot neural responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "f, ax = plt.subplots(figsize = (8, 4), facecolor = 'w')\n",
    "tmp = test[test.fname.isin(good_recs)].dropna()\n",
    "data = tmp[(tmp.kic_sig == False)]\n",
    "# data = data[data.cluster_id.isin(data2.cluster_id.unique())]\n",
    "# data = data[data.et.str.startswith('7')]\n",
    "# data = data[data.depth < 500]\n",
    "# data = data[data.cluster_id.isin(result[result.n_type == 'un'].cluster_id.unique())]\n",
    "data = data[(data.stim1.str.contains('0'))]\n",
    "data = data.dropna()\n",
    "# data = data[(data.stim1.str.startswith('1')) | (data.stim1.str.startswith('2')) ]\n",
    "# data = data[ (data.times >= 0.5) & (data.times < 3)]\n",
    "# data = data.sort_values(by = ['kic_ind'])\n",
    "hm = data.pivot('cluster_id', 'abs_times', 'zscore')\n",
    "hm = hm.dropna()\n",
    "hm2 = hm.values[ np.argsort(np.mean(hm.values[:,100:150], axis = 1) - np.mean(hm.values[:,50:100], axis = 1) )]\n",
    "\n",
    "sns.heatmap(hm2, cmap = 'jet',  annot=False, xticklabels=  200, vmax = 7, cbar = False,\n",
    "            vmin = -1, robust = True, yticklabels=False, ax = ax )\n",
    "ax.set(xlabel='', ylabel= hm.shape[0])\n",
    "# plt.savefig('hm_g1_lm_opto_n2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (8, 4), facecolor = 'w')\n",
    "colors = ['k', 'g']\n",
    "sns.tsplot(data.reset_index(), time = 'times', value = zsc,  ci = 68,\n",
    "   unit = 'cluster_id', condition = 'stim1', legend = False,   estimator=np.nanmean)\n",
    "sns.despine()\n",
    "plt.axvspan(1, 1.5, alpha=0.2, color='gray')\n",
    "plt.axvspan(0.9, 1.7,ymin = 0.95, ymax = 1, alpha=0.5, color='green')\n",
    "# plt.savefig('line_g1_lm_n2_opto.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open .gif files using pil\n",
    "fname = r\"U:\\Visual Stimulation\\pak6\\Vis Stim\\RF mapping\\probes1.gif\"\n",
    "from PIL import Image\n",
    "frames_ls = []\n",
    "im = Image.open(fname)\n",
    "# skip to the second frame\n",
    "tmp = im.convert('L')\n",
    "frame_arr = np.array(tmp)\n",
    "frames_ls.append(frame_arr)\n",
    "\n",
    "try:\n",
    "    while 1:\n",
    "        im.seek(im.tell()+1)\n",
    "        tmp = im.convert('L')\n",
    "        frame_arr = np.array(tmp)\n",
    "        frames_ls.append(frame_arr)\n",
    "        #plt.imshow(im, cmap = plt.cm.gray, vmin=0, vmax=255)\n",
    "        \n",
    "except EOFError:\n",
    "    pass # end of sequence\n",
    "frames_arr = np.stack(frames_ls, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['rec', 'et', 'exp']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "matches = [] # list of experiment folders\n",
    "source_folder = r\"U:\\Visual Stimulation\\pak6\\Vis Stim\\RF mapping\\test_probes_on\"\n",
    "#for source_folder in source_folders:\n",
    "file1 = glob.glob(source_folder + '\\\\probes_*.bmp')\n",
    "file_list=sorted(file1, key=lambda x: extract_probe_idx(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "frames_ls = []\n",
    "for fname in file_list[:]:\n",
    "    img = Image.open(fname).convert('L')\n",
    "    tmp = np.array(img)\n",
    "    frames_ls.append(tmp)\n",
    "\n",
    "#     plt.imshow(img, cmap = plt.cm.gray, vmin=0, vmax=255)\n",
    "#     plt.show()\n",
    "frames_arr = np.stack(frames_ls, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.mean(frames_arr, axis = 0), cmap = plt.cm.gray)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Visual Stimulation\\pak6\\Vis Stim\\RF mapping\\probes_off.npy\"\n",
    "frames_arr_off = np.load(path)\n",
    "path = r\"U:\\Visual Stimulation\\pak6\\Vis Stim\\RF mapping\\probes_on.npy\"\n",
    "frames_arr_on = np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract frame indices from frames\n",
    "# plot single STA frames\n",
    "fig = plt.subplots(facecolor='white')\n",
    "d_frames_idx = {}\n",
    "df_rez = df4\n",
    "for unit in sorted(df_rez.cluster_id.unique())[80:90]:\n",
    "    tmp = df_rez[df_rez.cluster_id == unit]\n",
    "    # account for latency and 120 empty frames, convert to frames idx\n",
    "    tmp =  np.rint((tmp.times.values[:] + 0.05 -2)*60)\n",
    "    \n",
    "#     frame_idx = [round_to_frame(x) for x in tmp]\n",
    "    frame_idx = ((tmp//15))\n",
    "    frame_idx = frame_idx[frame_idx>-1]\n",
    "    frame_idx = frame_idx[frame_idx < 3002]\n",
    "    frame_idx = frame_idx.astype(int)\n",
    "    \n",
    "    unique, counts = np.unique(frame_idx, return_counts=True)\n",
    "    d_frames_idx = dict(zip(unique, counts))\n",
    "    avg_frame_on = np.average(frames_arr_on[d_frames_idx.keys()],axis = 0, weights=d_frames_idx.values())\n",
    "    avg_frame_off = np.average(frames_arr_off[d_frames_idx.keys()],axis = 0, weights=d_frames_idx.values())\n",
    "    print unit\n",
    "    plt.imshow(avg_frame_on, cmap = plt.cm.gray)\n",
    "    plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    plt.tick_params(axis='y', which='both', right=False, left=False, labelleft=False)\n",
    "    for pos in ['right','top','bottom','left']:\n",
    "        plt.gca().spines[pos].set_visible(False)\n",
    "    plt.show()\n",
    "#     plt.savefig('rf_map_on-12-kicsqr2-right1.pdf')\n",
    "    plt.imshow(avg_frame_off, cmap = plt.cm.gray)\n",
    "    plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    plt.tick_params(axis='y', which='both', right=False, left=False, labelleft=False)\n",
    "    for pos in ['right','top','bottom','left']:\n",
    "        plt.gca().spines[pos].set_visible(False)\n",
    "    plt.show()\n",
    "# plt.savefig('rf_map_off-12-kicori2-right1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for unit in data.cluster_id.unique():\n",
    "    print unit\n",
    "    try:\n",
    "        plt.imshow(d_rf['on'][unit], cmap = plt.cm.gray)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        plt.imshow(d_rf['off'][unit], cmap = plt.cm.gray)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_type = sorted(data.stim1.unique())\n",
    "colors = my_pal[:]\n",
    "psth_tmp = data[data.cluster_id == '199005et715left']\n",
    "psth_tmp = psth_tmp[(psth_tmp.stim1.str.startswith('1')) | (psth_tmp.stim1.str.startswith('2'))]\n",
    "for unit in sorted(psth_tmp.cluster_id.unique())[:]:\n",
    "#     print unit\n",
    "    f, ax = plt.subplots(1 , figsize = (10,4), sharey=True, sharex=True, facecolor = 'w')\n",
    "    tmp = psth_tmp[psth_tmp.cluster_id == unit]\n",
    "    sns.despine()\n",
    "    plt.suptitle(unit)\n",
    "    for idx, stim in enumerate(sorted(tmp.stim1.unique())):\n",
    "        ax.plot(tmp[(tmp.stim1 == stim)].Hz, label = stim_type[idx], color = colors[idx])\n",
    "#         plt.xlim(100,200)\n",
    "    \n",
    "#     for idx, stim in enumerate(sorted(tmp[tmp.opto == 1].stim1.unique())):\n",
    "#         ax[1].plot(tmp[(tmp.stim1 == stim)].Hz, label = stim_type[idx])\n",
    "# #         plt.xlim(100,200)\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#     plt.show()\n",
    "plt.axvspan(100, 150, alpha = 0.1, color='k')\n",
    "plt.axvspan(50, 100, alpha = 0.05, color='k')\n",
    "sns.despine()\n",
    "# plt.savefig('line_199005et715left_opto_12.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ic_mod_units = ['10-KICsqrori6-right2', '11-KICsqrori2-right2', '171-KICsqrori2-left1', '176-KICsqrori6-right2',\n",
    "               '207-KICsqrori2-right2', '208-KICsqrori2-right2', '214-KICsqrori8-left1', '217-KICsqrori2-right2',  \n",
    "                '232-KICsqrori6-right2', '257-KICsqrori6-left1', '261-KICsqrori2-left1', '31-KICsqrori8-left1',\n",
    "                '70-KICsqrori6-right2', '79-KICsqrori2-right2', '86-KICsqrori2-left1'\n",
    "               ]\n",
    "ori_ls = ['a', 'b', 'c', 'd']\n",
    "colors = my_pal[::2]\n",
    "tmp_psth0 = kic_sqr_ori_psth[kic_sqr_ori_psth.cluster_id.isin(ic_mod_units)]\n",
    "stim_type = sorted(kic_sqr_ori_psth.stim1.unique())\n",
    "for unit in sorted(tmp_psth0.cluster_id.unique())[74:75]:\n",
    "#     print unit\n",
    "#     unit = '261-KICsqrori2-left1'\n",
    "    f, ax = plt.subplots(4, 1 , figsize = (6,8), sharey=True, sharex=True, facecolor = 'w')\n",
    "    plt.suptitle(unit)\n",
    "    tmp = tmp_psth0[tmp_psth0.cluster_id == unit]\n",
    "    sns.despine()\n",
    "    for plot_idx, ori in enumerate(sorted(ori_ls)):\n",
    "        tmp2 = tmp[tmp.stim1.str.contains(ori)]\n",
    "        for idx, stim1 in enumerate(sorted(tmp2.stim1.unique())[:]):\n",
    "            ax[plot_idx].plot(tmp2[tmp2.stim1 == stim1].Hz, label = stim_type[idx], color = colors[idx] )\n",
    "       \n",
    "        for i in range(2):\n",
    "            ax[plot_idx].axvspan(50+50*i, 100+50*i, alpha=0.1, color='k')    \n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#     plt.show()\n",
    "# plt.axvspan(100, 150, alpha = 0.1, color='k')\n",
    "# plt.axvspan(50, 100, alpha = 0.05, color='k')\n",
    "\n",
    "# plt.savefig('line_119-KICori2-right1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp = df_tuning[(df_tuning.times > 0.35) & (df_tuning.times < 0.85)]\n",
    "_inp['ori'] = _inp.angle%180\n",
    "_inp = _inp.groupby(['cluster_id', 'ori']).mean().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp = tmp_psth0\n",
    "arr = _inp.stim1.values\n",
    "ori = [x[0] for x in arr]\n",
    "stim = [x[1] for x in arr]\n",
    "_inp['ori'] = ori\n",
    "_inp['stim'] = stim\n",
    "ind_fr = _inp[(_inp.times > 0.55) & (_inp.times < 1.05)].groupby(['stim' ,'ori', 'cluster_id']).mean().reset_index().Hz\n",
    "ic_df = _inp[(_inp.times > 1.05) & (_inp.times < 1.5)].groupby(['stim', 'ori' , 'cluster_id']).mean().reset_index()\n",
    "ic_df['im_cir'] = (ic_df.Hz - ind_fr)/(ic_df.Hz + ind_fr)\n",
    "ori_2deg = {'a':0, 'b':1, 'c':2, 'd':3}\n",
    "ic_df['deg'] = ic_df['ori'].map(ori_2deg)\n",
    "\n",
    "ic_df = ic_df[ic_df.stim == '1']\n",
    "idx = ic_df.groupby('cluster_id')['im_cir'].transform(max) == ic_df['im_cir']\n",
    "ic_pref = dict(zip(ic_df[idx].cluster_id, (ic_df[idx].deg) ))\n",
    "ic_df['ic_pref'] = ic_df.cluster_id.map(ic_pref)\n",
    "ic_df['rel_deg'] = ic_df['deg'].astype(int) - ic_df['ic_pref'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp['norm'] = _inp.groupby('cluster_id')['Hz'].apply(lambda x: (x-x.min())/(x.max()-x.min()))\n",
    "_inp = _inp[_inp.cluster_id == '119-KICori2-right1']\n",
    "# ic_df = ic_df[ic_df.im_cir > 0]\n",
    "sns.factorplot(x = 'ori', y = 'norm', data = _inp, height = 4, \n",
    "               aspect = 1.5, color = 'gray')\n",
    "# plt.ylim(0, 1)\n",
    "# plt.savefig('point_119-KICori2-right1-grating-tuning.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grat_tun = kic_sqr_ori_tuning[kic_sqr_ori_tuning.cluster_id.isin(ic_mod_units)]\n",
    "grat_tun = grat_tun[(grat_tun.times > 0.35) & (grat_tun.times < 0.8)]\n",
    "grat_tun['ori'] = grat_tun.angle%180\n",
    "_inp = grat_tun.groupby(['cluster_id', 'ori']).Hz.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = _inp.groupby('cluster_id').Hz.transform(max) == _inp['Hz']\n",
    "grat_pref = dict(zip(_inp[idx].cluster_id, (_inp[idx].ori) ))\n",
    "_inp['grat_pref'] = _inp.cluster_id.map(grat_pref)\n",
    "_inp['rel_deg'] = _inp['ori'].astype(int) - _inp['grat_pref'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grat_tun.ori.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "inp_stat = ic_df\n",
    "for i in sorted(inp_stat.rel_deg.unique()):\n",
    "    tmp = inp_stat[inp_stat.rel_deg == i].im_cir.values\n",
    "    ls.append(tmp)\n",
    "    print np.mean(tmp), sstat.sem(tmp)\n",
    "    print len(tmp)\n",
    "sstat.kruskal(ls[0], ls[1], ls[2], ls[3], ls[4], ls[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ic_df[ic_df.deg == '0'].groupby('grat_pref').cluster_id.count().reset_index()\n",
    "df2 = ic_df[ic_df.deg == '0'].groupby('ic_pref').cluster_id.count().reset_index()\n",
    "df1 = pd.melt(df1, id_vars = ['cluster_id'], value_vars = ['grat_pref'])\n",
    "df2 = pd.melt(df2, id_vars = ['cluster_id'], value_vars = ['ic_pref'])\n",
    "df3 = pd.concat([df1, df2])\n",
    "sns.factorplot(x = 'value', y = 'cluster_id', hue = 'variable', data = df3, \n",
    "               kind = 'bar', palette = ['gray', 'crimson'], height = 4, aspect = 1, legend = False)\n",
    "# plt.savefig('bar_counts_pref-ori.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = tmp.stim1.values\n",
    "ori = [x[0] for x in arr]\n",
    "stim = [x[1] for x in arr]\n",
    "tmp['ori'] = ori\n",
    "tmp['stim'] = stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp2 = tmp[tmp.stim1.str.contains('1')]\n",
    "fig_inp_stim = tmp[(tmp.times > 1.05) & (tmp.times < 1.5)].groupby(['stim', 'ori']).mean().reset_index()\n",
    "fig_inp_ind = tmp[(tmp.times > 0.55) & (tmp.times < 1.05)].groupby(['stim', 'ori']).mean().reset_index().Hz\n",
    "fig_inp['im_cir'] = (fig_inp_stim.Hz - fig_inp_ind)/(fig_inp_stim.Hz + fig_inp_ind)\n",
    "sns.factorplot(x = 'ori', y = 'im_cir', data = fig_inp, palette = colors,\n",
    "               height = 4, aspect = 1.5, hue = 'stim',)\n",
    "# plt.savefig('point_ic-tuning_119-KICori2-right1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['trial_n'] = df5.times//2.0\n",
    "df5['stim1'] = df5['trial_n'].map(d_kic_ori)\n",
    "df5['trial_spikes'] = df5.times - df5['trial_n']*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = my_pal[:]\n",
    "colors = colors[::2]\n",
    "ori_ls = ['a', 'b', 'c', 'd']\n",
    "\n",
    "for unit in sorted(df5.cluster_id.unique())[84:85]:\n",
    "    tmp = df5[df5.cluster_id == unit]\n",
    "    f, ax = plt.subplots( 4, 1, figsize = (8, 9),  sharex=True, sharey=True, facecolor = 'w')\n",
    "    plt.suptitle(unit)\n",
    "    for plot_idx, ori in enumerate(ori_ls):    \n",
    "#         print ori\n",
    "        tmp2 = tmp[tmp.stim1.str.contains(ori)]\n",
    "        stim_type = sorted(tmp2.stim1.unique())\n",
    "        for idx, stim in enumerate(sorted(tmp2.stim1.unique()[:])):\n",
    "            un_trials = np.unique(tmp2[(tmp2.stim1 == stim)].trial_n.values, return_counts=True)[0].size\n",
    "            counts = np.unique(tmp2[(tmp2.stim1 == stim)].trial_n.values, return_counts=True)[1]\n",
    "            y = np.arange(1, (un_trials+1))\n",
    "            y = np.repeat(y, counts)\n",
    "            x = tmp2[(tmp2.stim1 == stim)].trial_spikes + idx*2\n",
    "            ax[plot_idx].plot(x, y, '.', label = stim_type[idx], color = colors[idx], markersize = 6 )\n",
    "        #         plt.xlim(100,200)\n",
    "        # plt.gca().invert_yaxis()\n",
    "    #     for idx, stim in enumerate(sorted(tmp[tmp.opto == 1].stim1.unique()[:])):\n",
    "    #         un_trials = np.unique(tmp[(tmp.stim1 == stim)].trial_n.values, return_counts=True)[0].size\n",
    "    #         counts = np.unique(tmp[(tmp.stim1 == stim)].trial_n.values, return_counts=True)[1]\n",
    "    #         y = np.arange(1, (un_trials+1))\n",
    "    #         y = np.repeat(y, counts)\n",
    "    #         x = tmp[(tmp.stim1 == stim)].trial_spikes + idx*2\n",
    "    #         ax[1].plot(x, y, '.' ,  label = stim_type[idx], color = colors[idx])\n",
    "        #         plt.xlim(100,200)\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "        for i in range(4):\n",
    "            ax[plot_idx].axvspan(1+2*i, 1.5+2*i, alpha=0.2, color='k')\n",
    "#         ax[i].axvspan(0.5 + 2*i, 1 + 2*i, alpha=0.2, color='gray')\n",
    "    sns.despine()\n",
    "    \n",
    "#     plt.savefig('raster_119-kicori2-right1.png')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = my_pal[4:]\n",
    "# colors = colors[::2]\n",
    "sp_tmp = df_spikes[df_spikes.cluster_id == '199005et715left']\n",
    "sp_tmp = sp_tmp[(sp_tmp.stim1.str.startswith('3')) | (sp_tmp.stim1.str.startswith('4'))]\n",
    "for unit in sorted(sp_tmp.cluster_id.unique())[:]:\n",
    "    tmp2 = sp_tmp[sp_tmp.cluster_id == unit]\n",
    "    f, ax = plt.subplots(  1, figsize = (10, 4),  sharex=True, sharey=True, facecolor = 'w')\n",
    "    plt.suptitle(unit)\n",
    "\n",
    "    stim_type = sorted(tmp2.stim1.unique())\n",
    "    for idx, stim in enumerate(sorted(tmp2.stim1.unique()[:])):\n",
    "        un_trials = np.unique(tmp2[(tmp2.stim1 == stim)].trial_n.values, return_counts=True)[0].size\n",
    "        counts = np.unique(tmp2[(tmp2.stim1 == stim)].trial_n.values, return_counts=True)[1]\n",
    "        y = np.arange(1, (un_trials+1))\n",
    "        y = np.repeat(y, counts)\n",
    "        x = tmp2[(tmp2.stim1 == stim)].trial_spikes + idx*2\n",
    "        ax.plot(x, y, '.', label = stim_type[idx], color = colors[idx] )\n",
    "\n",
    "    for i in range(4):\n",
    "        ax.axvspan(1+2*i, 1.5+2*i, alpha=0.2, color='k')\n",
    "#         ax[i].axvspan(0.5 + 2*i, 1 + 2*i, alpha=0.2, color='gray')\n",
    "    sns.despine()\n",
    "    \n",
    "# plt.savefig('raster_199005et715left_opto_34.pdf')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_psth0['et'] = 'KIC13'\n",
    "# tmp_psth0['exp'] = '004'\n",
    "# tmp_psth0['cluster_id'] = '004' + tmp_psth0['cluster_id'].astype('str') + 'KIC13' \n",
    "# tmp_psth1['et'] = 'KIC13'\n",
    "# tmp_psth1['exp'] = '005'\n",
    "# tmp_psth1['cluster_id'] = '005' + tmp_psth1['cluster_id'].astype('str') + 'KIC13' \n",
    "kic_psth = pd.concat([kic_psth0, kic_psth1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kanizsa SC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [] # list of experiment folders\n",
    "source_folder = r\"C:\\Users\\Chub_lab\\Desktop\\phy\\rf_tmp\\conc\"\n",
    "#for source_folder in source_folders:\n",
    "for root, dirnames, filenames in os.walk(source_folder):\n",
    "    for filename in fnmatch.filter(filenames, '*rez.mat'):\n",
    "        matches.append(os.path.join(root, filename))\n",
    "\n",
    "print matches[:5]\n",
    "print matches[-5:]\n",
    "print len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [] # list of experiment folders\n",
    "source_folder = r\"U:\\Data\\pak6\\OpenEphys\\probe_64DB\\WT\\illusory_contours\\LM-Arch\"\n",
    "for root, dirnames, filenames in os.walk(source_folder):\n",
    "    for dirname in fnmatch.filter(dirnames, '*conc'):\n",
    "        if 'phy' in dirname or 'bad' in dirname:\n",
    "            continue\n",
    "        matches.append(os.path.join(root, dirname)) \n",
    "print matches[:5]\n",
    "print len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in matches if '201' in f or 'KIC' in f]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in matches[:]:\n",
    "    \n",
    "    et = path.split('\\\\')[-2].split()[0]\n",
    "    rec = path.split('\\\\')[-2].split()[1]\n",
    "    stims = sorted([x for x in  get_immediate_subdirectories(os.path.split(path)[0]) if '201' in x])\n",
    "    print et, rec, stims[4]\n",
    "    for fname in stims[:]:\n",
    "\n",
    "        if 'kic-opto' in fname:\n",
    "\n",
    "            exp  = fname.split('_')[0]\n",
    "            stim1 = fname.split('_')[1]\n",
    "        else:\n",
    "            continue\n",
    "       \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trial_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f, axs = plt.subplots(3,2, sharex=True, sharey = True, figsize =(14,10))\n",
    "data = df_rez\n",
    "trial_length = 4\n",
    "plt.figure()\n",
    "for i in sorted(data['cluster_id'].unique()[:]):\n",
    "    print i\n",
    "    df_r = ena.getRaster_kilosort(data, i, trial_length)\n",
    "    \n",
    "    plt.plot( df_r.times, df_r.trial, '.')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlim(0.5,2)\n",
    "    plt.show()\n",
    "sns.despine()\n",
    "\n",
    "# plt.ylabel('Trials')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('rasrter2_123.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = '64DB'\n",
    "channel_groups = ena.get_channel_depth(probe)\n",
    "def make_kic_psth(data, fname, et, rec, d_stim, d_opto, templates, channel_groups ):\n",
    "    ls = []\n",
    "    ls_tmt = []\n",
    "    ls_spikes = []\n",
    "    exp  = fname.split('_')[0]\n",
    "#     data['opto'] = data.trial_n.map(d_opto)\n",
    "    data['opto'] = 0\n",
    "    data['stim1'] = data.trial_n.map(d_stim)\n",
    "    data['trial_spikes'] = data.times - data.trial_n*2\n",
    "    for i, v in enumerate(sorted(data.opto.unique())): \n",
    "        tmp = data[data['opto'] == v ]\n",
    "        opto_time = 0\n",
    "        if v == 1:\n",
    "            opto_time = 8\n",
    "        for idx, val in enumerate(sorted(tmp.stim1.unique())): \n",
    "            tmp2 = tmp[tmp['stim1'] == val ]\n",
    "            \n",
    "            for unit in tmp2['cluster_id'].unique():\n",
    "                df = ena.getRaster_kilosort(tmp2, unit, 2.0) \n",
    "                if len(df.times) < 100:\n",
    "                    continue\n",
    "        \n",
    "                cluster_id = str(unit) + str(exp) + 'et' + str(et) + str(rec) \n",
    "                cuid =  str(cluster_id) + 'stim' + str(val) + 'opto' + str(v)\n",
    "                \n",
    "                tmt, depth, ch_idx = ena.ksort_get_tmt(tmp2, unit, templates, channel_groups)\n",
    "#                 df_tmt_tmp = pd.DataFrame({'tmt':tmt,  'cluster_id' : cluster_id, 'path': path })\n",
    "#                 ls_tmt.append(df_tmt_tmp)\n",
    "#                 tmp3 = tmp2[tmp2.cluster_id == unit]\n",
    "#                 tmp3['cluster_id'] = cluster_id\n",
    "#                 tmp3['stim1'] = str(val) + str(v)\n",
    "#                 ls_spikes.append(tmp3)\n",
    "                \n",
    "                #df = tmp[tmp.cluster_id==unit]\n",
    "                #df.trial_st = df.trial_st-t_idx[0]\n",
    "     \n",
    "                h, ttr = ena.PSTH(df.times, 0.01, 2.0, 25.0) # all times rescaled to 0-4 this is why trias number 1.0\n",
    "                zscore = sstat.mstats.zscore(h)\n",
    "                mean = np.mean(h[:50])\n",
    "                std = np.std(h[:50])\n",
    "                if mean==0:\n",
    "                    std=1\n",
    "                ztc = (h - mean)/std\n",
    "\n",
    "                df_psth = pd.DataFrame({     'times':ttr,    'Hz':h, 'cuid': cuid  ,  'et':et, 'fname':fname, 'rec':rec,\n",
    "                                            'stim1': str(val) + str(v),  'opto':v, 'abs_times': idx*2.0 + ttr + opto_time,\n",
    "                                        'zscore':zscore, 'ztc':ztc,   'cluster_id':cluster_id, 'exp': exp, 'depth': depth   })\n",
    "                ls.append(df_psth)\n",
    "            \n",
    "\n",
    "    \n",
    "    df_psth = pd.concat(ls)\n",
    "  \n",
    "    df_spikes = 0\n",
    "    df_tmt = 0\n",
    "    return df_psth, df_spikes, df_tmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_stim = {}\n",
    "for i, v in enumerate(short_seq):\n",
    "    d_stim[i] = v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Receptive field mapping\n",
    "ls_psth = []\n",
    "ls_spikes = []\n",
    "ls_tmt = []\n",
    "d_stim = {}\n",
    "d_opto = {}\n",
    "for i, v in enumerate(short_seq):\n",
    "    d_stim[i] = v \n",
    "for i, v in enumerate(opto_seq):\n",
    "    d_opto[i] = v \n",
    "    \n",
    "for path in matches[:]:\n",
    "\n",
    "    print path\n",
    "    if 'bad' in path:\n",
    "        continue\n",
    "    et = path.split('\\\\')[-2].split()[0]\n",
    "    rec = path.split('\\\\')[-2].split()[1]\n",
    "    stims = sorted([x for x in  get_immediate_subdirectories(os.path.split(path)[0]) if '201' in x])\n",
    "    \n",
    "    path_cluster_groups = os.path.join(path, 'cluster_groups.csv')\n",
    "    cluster_groups = pd.read_csv(path_cluster_groups, sep = '\\t')\n",
    "    # good_units = cluster_groups[cluster_groups.group != 'noise'].cluster_id.values\n",
    "    noise_units = cluster_groups[cluster_groups['group'] == 'noise'].cluster_id.values\n",
    "    spike_times = np.load(os.path.join(path, 'spike_times.npy'))\n",
    "    spike_clusters = np.load(os.path.join(path, 'spike_clusters.npy'))\n",
    "    templates = np.load(os.path.join(path, 'templates.npy'))\n",
    "    spike_templates = np.load(os.path.join(path, 'spike_templates.npy'))\n",
    "    df = pd.DataFrame({'times':spike_times.flatten()/30000.0, \n",
    "                       'cluster_id':spike_clusters.flatten(), \n",
    "                       'templates':spike_templates.flatten() })\n",
    "\n",
    "    df = df[~df.cluster_id.isin(noise_units)]\n",
    "#     df2 = df[df.times <= 160]\n",
    "#     df3 = df[(df.times > 160)   & (df.times<=940)]\n",
    "#     df3.times = df3.times - 160\n",
    "\n",
    "    df4 = df[ (df.times > 940) & (df.times <= 1140)]\n",
    "    df4.times = df4.times - 940\n",
    "\n",
    "    df5 = df[df.times > 1340 ]\n",
    "    df5.times = df5.times - 1340\n",
    "\n",
    "#     df2['trial_n'] = df2.times//4.0\n",
    "#     df3['trial_n'] = df3.times//60.0\n",
    "    df4['trial_n'] = df4.times//2.0\n",
    "    df5['trial_n'] = df5.times//2.0\n",
    "#     df_g1 = df2\n",
    "#     df_rf = df3\n",
    "    df_kic = df4\n",
    "    df_kic2 = df5\n",
    "    \n",
    "    df_kic_psth, df_kic_spikes, df_kic_tmt  = make_kic_psth(df4, stims[3], et, rec,  d_stim, d_opto, templates, channel_groups)\n",
    "\n",
    "    ls_psth.append(df_kic_psth)\n",
    "    \n",
    "    df_kic2_psth, df_kic2_spikes, df_kic2_tmt  = make_kic_psth(df5, stims[4], et, rec, d_stim, d_opto, templates, channel_groups)\n",
    "    ls_spikes.append(df_kic2_spikes)\n",
    "    ls_tmt.append(df_kic2_tmt)\n",
    "    ls_psth.append(df_kic2_psth)\n",
    "# df_tmt = pd.concat(ls_tmt)\n",
    "# df_spikes = pd.concat(ls_spikes)\n",
    "master_kic = pd.concat(ls_psth)\n",
    "# master_kic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\Kanizsa paper ephys\\kic-archt-all-recs.pkl\"\n",
    "# df_psth = pd.read_pickle(path)\n",
    "\n",
    "# path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\Kanizsa paper ephys\\kic2-archt-880unit_spikes.pkl\"\n",
    "# df_spikes = pd.read_pickle(path)\n",
    "\n",
    "# path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\Kanizsa paper ephys\\rf_maps_kic.pkl\"\n",
    "# rf_df = pd.read_pickle(path)\n",
    "\n",
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\Kanizsa paper ephys\\kic_5km.pkl\"\n",
    "kic_5km = pd.read_pickle(path)\n",
    "\n",
    "# path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\Kanizsa paper ephys\\kic2-archt-880unit_tmt.pkl\"\n",
    "# df_templates = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kic_5km.cluster_id.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kic_index(df, val = 'Hz'):\n",
    "    tmp_10 = df[df.stim1 == '10']\n",
    "    tmp_30 = df[df.stim1 == '30']\n",
    "    tmp_40 = df[df.stim1 == '40']\n",
    "    base = np.mean(tmp_10[val][:50])\n",
    "    ind_kic_fr = np.mean(tmp_10[val][55:100])\n",
    "    kic_fr = np.mean(tmp_10[val][105:150])\n",
    "    try:    \n",
    "        kic_ind = (kic_fr - ind_kic_fr)/ (kic_fr + ind_kic_fr)\n",
    "    except:\n",
    "        kic_ind = np.nan\n",
    "    df['kic_ind'] = kic_ind\n",
    "    \n",
    "    tmp_20 = df[df.stim1 == '20']\n",
    "    ind_rot_fr = np.mean(tmp_20[val][55:100])\n",
    "    rot_fr = np.mean(tmp_20[val][105:150])\n",
    "    try:\n",
    "        kic_rot = (kic_fr - rot_fr) / (rot_fr + kic_fr)\n",
    "    except:\n",
    "        kic_rot = np.nan\n",
    "    df['kic_rot'] = kic_rot\n",
    "    \n",
    "    tmp_11 = df[df.stim1 == '11']\n",
    "    archt_fr = np.mean(tmp_11[val][105:150])\n",
    "    try:\n",
    "        opto_mod = (kic_fr - archt_fr) / (kic_fr + archt_fr)\n",
    "    except:\n",
    "        opto_mod = np.nan\n",
    "    df['opto_mod'] = opto_mod\n",
    "    kic_sig = False\n",
    "\n",
    "    p_ind = sstat.wilcoxon(tmp_10[val][5:50], tmp_10[val][55:100])\n",
    "#         p_rot = sstat.wilcoxon(tmp_10[val][105:150], tmp_20[val][105:150])\n",
    "    if p_ind[1] < 0.05:\n",
    "        kic_sig = True\n",
    "    else:\n",
    "        kic_sig = False\n",
    "\n",
    "    df['ind_sig'] = kic_sig\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kic_ori_index(df, val = 'Hz'):\n",
    "    tmp_ori = df[df.stim1.str.contains('10')]\n",
    "    for stim1 in sorted(tmp_ori.stim1.unique()):\n",
    "        tmp = df[df.stim1 == stim1]\n",
    "        base = np.mean(tmp[val][5:35])\n",
    "        ind_fr = np.mean(tmp[val][55:85])\n",
    "        ic_fr = np.mean(tmp[val][105:135])\n",
    "\n",
    "        p_ind = sstat.wilcoxon(tmp[val][5:50], tmp[val][55:100])\n",
    "        if p_ind[1] < 0.05:\n",
    "            ind_sig = True\n",
    "        else:\n",
    "            ind_sig = False\n",
    "            \n",
    "        try:    \n",
    "            ind_idx = (ic_fr - ind_fr)/ (ic_fr + ind_fr)\n",
    "        except:\n",
    "            ind_idx = np.nan\n",
    "        if ind_sig == False and ind_idx > 0:\n",
    "            df[stim1 + \"_\" + 'ic_mod'] = True\n",
    "        else:\n",
    "            df[stim1 + \"_\" + 'ic_mod'] = False\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kic_sqr_ori_psth = kic_sqr_ori_psth.groupby('cluster_id').apply(add_kic_ori_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = kic_5km.groupby('cluster_id').apply(add_kic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_inj_et = ['433', '434', '437']\n",
    "good_recs = test[(test.abs_times == 0) \n",
    "     & (test.ind_sig == True)\n",
    "    & (test.kic_ind > 0)\n",
    "    & test.kic_rot > 0].groupby(['et', 'rec', 'fname']).cluster_id.count().reset_index()\n",
    "# good_recs = good_recs[~good_recs.et.isin(bad_inj_et)]\n",
    "# good_recs = good_recs[good_recs.cluster_id>3]\n",
    "good_recs = good_recs[good_recs.fname.str.startswith('005')].fname.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kic_5km.to_pickle('kic_paper_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pca = 'r_groups'\n",
    "time_idx = np.concatenate((np.arange(100), np.arange(200, 300), np.arange(400, 500), np.arange(600, 700)))\n",
    "# time_idx = np.arange(50,100)\n",
    "kic_5km = ena.unit_kmeans(test[test.fname.isin(good_recs)].dropna(), 6, 'abs_times', 'cluster_id', key_pca, time_idx)\n",
    "# kic_5km = ena.unit_kmeans(kic_psth[kic_psth.exp == '005'], 5, 'abs_times', 'cluster_id', key_pca, time_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = test[test.fname.isin(good_recs)].dropna()\n",
    "df_new = tmp.pivot(index= 'abs_times', columns= 'cluster_id', values= 'zscore')\n",
    "time_idx = np.arange(0, 100)\n",
    "df_new = df_new.reset_index().drop( 'abs_times',1)\n",
    "# df_new = df_new.dropna()\n",
    "df_new = df_new.T\n",
    "df_new = df_new.dropna()\n",
    "X = df_new.ix[:,time_idx].values #0.5-2 second interval\n",
    "y = df_new.index.values.tolist() # corresponding cuid\n",
    "\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "# plt.savefig('elbow_method_kmeans.pdf', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(4,12), metric='calinski_harabaz', timings=False\n",
    ")\n",
    "\n",
    "visualizer.fit(X)    # Fit the data to the visualizer\n",
    "visualizer.poof()    # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data\n",
    "tmp = tmp[(tmp.abs_times == 0)]\n",
    "# tmp = tmp[\n",
    "# #     (tmp.kic_ind > 0) \n",
    "# #             & (tmp.kic_rot > 0)\n",
    "#              (tmp.kic_sig == False)\n",
    "# #             & (test.opto_mod < 0)\n",
    "#            ]\n",
    "tmp = tmp[['cluster_id', 'kic_rot', 'kic_ind']]\n",
    "g = sns.jointplot(x = 'kic_ind', y = 'kic_rot', data = tmp, color = 'k', \n",
    "                  s = 20, xlim= (-1,1), ylim = (-1, 1))\n",
    "# g.plot_joint(sns.kdeplot, n_levels = 1, z_order = 0)\n",
    "# plt.axhline(y=0, linestyle = '--')\n",
    "# plt.axvline(x=0, linestyle = '--')\n",
    "# plt.savefig('scatter_im_opto.pdf', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gr = kic_5km[:\n",
    "#              (df_psth.stim1 == 1) \n",
    "#             & (df_psth.depth < 600) \n",
    "#             & (df_psth.et == '494') \n",
    "            ]\n",
    "# gr = gr[gr.times<2.5]\n",
    "gr = gr.dropna()\n",
    "bad_units = gr[(gr.ztc>100) | (gr.ztc < -10)].cluster_id.unique()\n",
    "# gr = gr[gr.times < 2.5]\n",
    "gr = gr[~gr.cluster_id.isin(bad_units)]\n",
    "n = int(gr.r_groups.max()+1)\n",
    "gr = gr[(gr.stim1.str.contains('0'))]\n",
    "# gr = gr[gr.cluster_id.isin(result[result.n_type == 'fs'].cluster_id.unique())]\n",
    "f, ax = plt.subplots(n, sharex=True,figsize = (8,12))\n",
    "# gr = gr[(gr.stim1.str.startswith('3')) | (gr.stim1.str.startswith('4')) ]\n",
    "# cbar_ax = f.add_axes([.91, .3, .03, .5])\n",
    "sns.set_style(\"ticks\")\n",
    "for idx, val in enumerate(sorted(gr.r_groups.unique())):\n",
    "    tmp = gr[gr.r_groups==val].pivot('cluster_id', 'abs_times', 'zscore').dropna()\n",
    "    tmp2 = tmp.values[ np.argsort(np.mean(tmp.values[:,50:100], axis = 1) )]\n",
    "    g = sns.heatmap(tmp2, cmap = 'jet',  \n",
    "                 ax = ax[idx], xticklabels=200, yticklabels=False, vmax= 7,  vmin = -1, robust = True,\n",
    "                cbar = False\n",
    "                   )\n",
    "    ax[idx].set(xlabel='', ylabel=tmp.index.size )\n",
    "\n",
    "#     ax[i].set_title('Cluster group ' + str (i+1), loc = 'left') \n",
    "f.subplots_adjust(hspace=0.2) \n",
    "# ax[0].set_title(str(gr.stim1.unique()[0])  )\n",
    "# plt.savefig('hm_kic_5km_all_groups.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zsc = 'Hz'\n",
    "# gr = df_out\n",
    "# gr = df_out\n",
    "f, ax = plt.subplots(n, sharex=True,figsize = (6,12))\n",
    "colors = my_pal[:]\n",
    "colors = colors[::2]\n",
    "ts_input = gr[(gr.times > 0.1) & (gr.times<1.9)].sort_values(by=['stim1'])\n",
    "ts_input = ts_input.sort_values(by = 'stim1')\n",
    "# colors = ['k', 'g']\n",
    "for idx, val in enumerate(sorted(gr.r_groups.unique())):\n",
    "    tmp = ts_input[ts_input.r_groups==val].pivot('cluster_id', 'abs_times', 'zscore').dropna()\n",
    "    tmp2 = ts_input[ts_input.cluster_id.isin(tmp.index.values)]\n",
    "    sns.tsplot(tmp2[tmp2.r_groups==val].reset_index(), time = 'times', value = zsc, color = colors, ci = 68,\n",
    "       unit = 'cluster_id', condition = 'stim1',   estimator=np.nanmean, ax = ax[idx], legend= False,\n",
    "              )\n",
    "\n",
    "# plt.axhline(y=0.002,xmin=0,xmax=3,c=\"black\",linewidth=1,zorder=0, ls='dashed')\n",
    "# plt.xlim(0, 1)\n",
    "# plt.xlabel('Time (s)')\n",
    "plt.axvspan(0.4, 1.2,ymin = 0.95, ymax = 1, alpha=0.5, color='green')\n",
    "plt.axvspan(0.5, 1, alpha = 0.1, color='k')\n",
    "plt.ylabel(zsc)\n",
    "sns.despine()\n",
    "# sns.despine(offset=5, trim=True);\n",
    "# plt.savefig('line_5km_all_groups.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "f, ax = plt.subplots(figsize = (12, 6), facecolor = 'w')\n",
    "# data = gr\n",
    "# data = kic_5km[\n",
    "#             #(test.kic_ind > 0) \n",
    "#             #& (test.kic_rot > 0)\n",
    "#             #& (test.kic_sig == True)\n",
    "#             #&  (test.opto_mod < 0)\n",
    "#      kic_5km.r_groups > -1\n",
    "#            ]\n",
    "\n",
    "data = test[test.fname.isin(good_recs)]\n",
    "data = tmp_psth0[tmp_psth0.deg == tmp_psth0.pref]\n",
    "data['uet'], data['side'] = data['et'].str.split('-', 1).str\n",
    "data = data[data.uet == data.uet.unique()[2]]\n",
    "print data.uet.unique()\n",
    "# data = data[data.cluster_id.isin(result[result.n_type == 'fs'].cluster_id.unique())]\n",
    "\n",
    "# data = test[test.fname.isin(good_recs)].dropna()\n",
    "# bad_units = data[(data.ztc>100) | (data.ztc < -10)].cluster_id.unique()\n",
    "# data = data[~data.cluster_id.isin(bad_units)]\n",
    "# data = data[(data.depth < 500)]\n",
    "data = data[(data.stim1.str.contains('1')) \n",
    "            | (data.stim1.str.contains('2')) \n",
    "           ]\n",
    "data['stim_times'] = data['times'] + data['stim'].astype(int)*2\n",
    "\n",
    "# data = data[(data['kic_ind'] > 0) \n",
    "           \n",
    "# #             & (data.kic_rot > 0.1)\n",
    "#             & (data['ind_sig'] == False)\n",
    "#            ]\n",
    "hm = data.pivot('cluster_id', 'stim_times', 'zscore')\n",
    "hm = hm.dropna()\n",
    "hm2 = hm.values[ np.argsort(np.mean(hm.values[:,100:150], axis = 1) - np.mean(hm.values[:,50:100], axis = 1) )]\n",
    "# hm2 = hm2[-100:]\n",
    "# plt.figure\n",
    "# plt.figure(figsize=(20,20))\n",
    "sns.heatmap(hm2, cmap = 'jet',  annot=False, xticklabels=  200, vmax = 7, cbar = False,\n",
    "            vmin = -1, robust = True, yticklabels=False, ax = ax )\n",
    "ax.set(xlabel='', ylabel= hm2.shape[0])\n",
    "# plt.savefig('hm_ic_ind-sig_12opto.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "# gr = df_psth[\n",
    "#             (df_psth.stim1 == 'sqr-line') |\n",
    "# #             & (df_psth.cluster_id == '100et495pre') \n",
    "#              (df_psth.stim1 == 'sqr') \n",
    "# #             & (df_psth.depth > 500)\n",
    "#             ]\n",
    "zsc = 'Hz'\n",
    "colors = my_pal[::2]\n",
    "# colors = colors[4:]\n",
    "\n",
    "data_tmp = data[data.cluster_id.isin(hm.index.values)]\n",
    "# data_tmp = data_tmp[(data_tmp.stim1.str.startswith('1')) | (data_tmp.stim1.str.startswith('2')) ]\n",
    "# data_tmp = data_tmp[(data_tmp.stim1.str.contains('0'))]\n",
    "\n",
    "\n",
    "ts_input = data_tmp[(data_tmp.times > 0.1) & (data_tmp.times<1.9)].sort_values(by=['stim1'])\n",
    "ts = sns.tsplot(ts_input.reset_index(), time = 'times', value = zsc, color  = colors, legend = False,\n",
    "           unit = 'cluster_id',   estimator=np.nanmean, condition = 'stim', ci = 68,\n",
    "          )\n",
    "\n",
    "# plt.axvspan(0.9, 1.7,ymin = 0.97, ymax = 1, alpha=0.5, color='green')\n",
    "# plt.axvspan(0.9, 1.7, alpha = 0.05, color='green')\n",
    "plt.axvspan(1, 1.5, alpha = 0.1, color='k')\n",
    "plt.axvspan(0.5, 1, alpha = 0.05, color='k')\n",
    "\n",
    "# ts.set(xlim=(0.4, 0.5))\n",
    "sns.despine()\n",
    "# plt.savefig('line_ind_sig_12opto.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_inp = data_tmp[\n",
    "                 (data_tmp.times > 1.05) \n",
    "                & (data_tmp.times < 1.5)].dropna()\n",
    "\n",
    "fig_inp = fig_inp.groupby([ 'stim1' ,'cluster_id']).mean().reset_index().dropna() \n",
    "\n",
    "g = sns.factorplot(  x = 'stim1', y= \"Hz\", data= fig_inp, kind = 'bar', ci = 68,\n",
    "                   palette = colors, \n",
    "#                    capsize = 0.02, saturation = 1,\n",
    "                   size = 6, aspect = 1 )\n",
    "# g = sns.swarmplot(x = 'stim1', y = 'Hz', data = fig_inp, color = '0.25', alpha = 0.5)\n",
    "# plt.ylim(0, 20)\n",
    "# plt.savefig('bar_kic_ind-sig_12opto.pdf', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = _inp[ (_inp.layer == 'l2/3')].opto_mod.dropna().values\n",
    "x2 = _inp[ (_inp.layer == 'l4')].opto_mod.dropna().values\n",
    "x3 = _inp[ (_inp.layer == 'l5/6')].opto_mod.dropna().values\n",
    "print np.mean(x1), sstat.sem(x1)\n",
    "print np.mean(x2), sstat.sem(x2)\n",
    "print np.mean(x3), sstat.sem(x3)\n",
    "print len(x1), len(x2), len(x3)\n",
    "\n",
    "print sstat.mannwhitneyu(x1, x2)\n",
    "print sstat.mannwhitneyu(x1, x3)\n",
    "print sstat.mannwhitneyu(x2, x3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = fig_inp[(fig_inp.r_groups == 0) & (fig_inp.opto == 0)].Hz.values\n",
    "x2 = fig_inp[(fig_inp.r_groups == 1) & (fig_inp.opto == 0)].Hz.values\n",
    "x3 = fig_inp[(fig_inp.r_groups == 2) & (fig_inp.opto == 0)].Hz.values\n",
    "x4 = fig_inp[(fig_inp.r_groups == 3) & (fig_inp.opto == 0)].Hz.values\n",
    "\n",
    "y1 = fig_inp[(fig_inp.r_groups == 0) & (fig_inp.opto == 1)].Hz.values\n",
    "y2 = fig_inp[(fig_inp.r_groups == 1) & (fig_inp.opto == 1)].Hz.values\n",
    "y3 = fig_inp[(fig_inp.r_groups == 2) & (fig_inp.opto == 1)].Hz.values\n",
    "y4 = fig_inp[(fig_inp.r_groups == 3) & (fig_inp.opto == 1)].Hz.values\n",
    "\n",
    "print np.mean(x1), sstat.sem(x1)\n",
    "print np.mean(y1), sstat.sem(y1)\n",
    "\n",
    "print np.mean(x2), sstat.sem(x2)\n",
    "print np.mean(y2), sstat.sem(y2)\n",
    "print np.mean(x3), sstat.sem(x3)\n",
    "print np.mean(y3), sstat.sem(y3)\n",
    "print np.mean(x4), sstat.sem(x4)\n",
    "print np.mean(y4), sstat.sem(y4)\n",
    "\n",
    "print len(x1), len(x2), len(x3), len(x4)\n",
    "print len(y1), len(y2), len(y3), len(y4)\n",
    "# print sstat.kruskal(x1, x2, x3, x4)\n",
    "\n",
    "print sstat.wilcoxon(x1, y1, zero_method='pratt', correction=True)\n",
    "print sstat.wilcoxon(x2, y2, zero_method='pratt', correction=True)\n",
    "print sstat.wilcoxon(x3, y3, zero_method='pratt', correction=True)\n",
    "print sstat.wilcoxon(x4, y4, zero_method='pratt', correction=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = fig_inp[fig_inp.stim1 == '10'].dropna().Hz.values\n",
    "x2 = fig_inp[fig_inp.stim1 == '11'].dropna().Hz.values\n",
    "x3 = fig_inp[fig_inp.stim1 == '20'].dropna().Hz.values\n",
    "x4 = fig_inp[fig_inp.stim1 == '21'].dropna().Hz.values\n",
    "\n",
    "print np.mean(x1), sstat.sem(x1)\n",
    "print np.mean(x2), sstat.sem(x2)\n",
    "print np.mean(x3), sstat.sem(x3)\n",
    "print np.mean(x4), sstat.sem(x4)\n",
    "print len(x1), len(x2), len(x3), len(x4)\n",
    "n = min(len(x1), len(x2), len(x3), len(x4))\n",
    "\n",
    "print sstat.kruskal(x1, x2, x3, x4)\n",
    "\n",
    "print len(x1)\n",
    "\n",
    "print sstat.mannwhitneyu(x1, x2)\n",
    "print sstat.mannwhitneyu(x1, x3)\n",
    "print sstat.mannwhitneyu(x1, x4)\n",
    "print sstat.mannwhitneyu(x2, x3)\n",
    "print sstat.mannwhitneyu(x2, x4)\n",
    "print sstat.mannwhitneyu(x3, x4)\n",
    "\n",
    "print sstat.wilcoxon(x1[:n], x2[:n], zero_method='pratt', correction=True)\n",
    "print sstat.wilcoxon(x1[:n], x3[:n], zero_method='pratt', correction=True)\n",
    "print sstat.wilcoxon(x1[:n], x4[:n], zero_method='pratt', correction=True)\n",
    "print sstat.wilcoxon(x2[:n], x3[:n], zero_method='pratt', correction=True)\n",
    "print sstat.wilcoxon(x2[:n], x4[:n], zero_method='pratt', correction=True)\n",
    "print sstat.wilcoxon(x3, x4, zero_method='pratt', correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(x1), len(x2), len(x3), len(x4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp = fig_inp.groupby([ 'cluster_id', 'stim1']).mean().reset_index()\n",
    "_data = _inp.pivot(index='cluster_id', columns='stim1', values='Hz').reset_index()\n",
    "#_data = _data[['cluster_id', '10', '11']]\n",
    "lims = [0, 20]\n",
    "_data = _data[(_data>lims[0]) & (_data<lims[1])]\n",
    "g = sns.jointplot(x = '41', y = '40', data = _data, color = 'k', \n",
    "                  s = 30, stat_func = None,\n",
    "                  xlim= lims, ylim = lims\n",
    "                 )\n",
    "g.plot_joint(sns.kdeplot, n_levels = 2, z_order = 0)\n",
    "\n",
    "x0, x1 = g.ax_joint.get_xlim()\n",
    "y0, y1 = g.ax_joint.get_ylim()\n",
    "lims = [max(x0, y0), min(x1, y1)]\n",
    "g.ax_joint.plot(lims, lims, '--k', linewidth = 3 )    \n",
    "# plt.savefig('scatter_ind_sig_opto_41.pdf', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (data_tmp['depth'] <= 600) & (data_tmp['depth'] >= 400),\n",
    "    (data_tmp['depth'] < 400) ,\n",
    "    (data_tmp['depth'] > 600)]\n",
    "choices = ['l4', 'l5/6', 'l2/3']\n",
    "data_tmp['layer'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp.cluster_id.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots()\n",
    "bins = np.arange(0, 1001, 50)\n",
    "labels = bins[1:]\n",
    "_inp = data_tmp[(data_tmp.opto_mod > -10) & (data_tmp.abs_times == 4.5)]\n",
    "_inp['bin_depth'] = pd.cut(_inp['depth'], bins=bins, labels=labels)\n",
    "_inp = _inp.sort_values(by = 'layer')\n",
    "sns.despine()\n",
    "sns.catplot(data = _inp, x = 'layer', y = 'kic_rot',  color = 'gray',\n",
    "            kind= 'box', height = 5, aspect = 1 )\n",
    "\n",
    "plt.savefig('box_kic_rot_layer.pdf', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_seq = [10, 7, 3, 2, 4, 8, 9, 5, 7, 3, 4, 8, 3, 2, 1, 8, 0, 4, 9, 11, \n",
    "10, 9, 1, 11, 4, 0, 7, 1, 2, 8, 2, 9, 11, 9, 6, 5, 10, 4, 9, 0, 7, 11, 9, \n",
    "5, 9, 10, 11, 6, 8, 9, 5, 4, 2, 8, 11, 2, 10, 3, 5, 1, 7, 0, 4, 9, 1, 5, \n",
    "11, 3, 5, 10, 1, 2, 9, 6, 2, 2, 11, 5, 10, 7, 3, 7, 4, 6, 8, 4, 1, 8, 0, \n",
    "11, 0, 6, 2, 11, 1, 10, 3, 8, 3, 1, 2, 10, 5, 3, 11, 1, 7, 3, 4, 7, 8, 4, 6, \n",
    "7, 11, 7, 0, 8, 6, 10, 4, 5, 7, 2, 10, 3, 5, 9, 8, 6, 3, 2, 0, 11, 0, 6, 10, \n",
    "0, 7, 4, 5, 0, 10, 6, 8, 10, 3, 11, 9, 0, 5, 1, 3, 7, 0, 6, 9, 1, 6, 10, 5, \n",
    "6, 11, 7, 0, 5, 1, 4, 1, 6, 8, 2, 9, 2, 8, 3, 0, 4, 6, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize = (8,12))\n",
    "sns.despine()\n",
    "ax.plot(b[0]/a[0], bins[1:])\n",
    "# plt.savefig('line_norm_depth_dist.pdf', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, 1001, 50)\n",
    "tmp = kic_5km.groupby('cluster_id').depth.mean().values\n",
    "a = np.histogram(tmp, bins= bins)\n",
    "dep_gr0 = data_tmp.groupby('cluster_id').depth.mean().values\n",
    "b = np.histogram(dep_gr0, bins= bins)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dep_gr0 = data_tmp.groupby('cluster_id').depth.mean().values\n",
    "sns.distplot(dep_gr0, color = 'k', bins = np.arange(0, 1025, 50), kde = False)\n",
    "# sns.distplot(dep2, color = 'cyan')\n",
    "sns.despine()\n",
    "plt.gca().invert_xaxis()\n",
    "#     plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "plt.xlim(1200, -200)\n",
    "\n",
    "# plt.savefig('dist_depth.pdf', transparent = True)\n",
    "# plt.xlim(-150, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dep_gr0 = df_rf_size[(df_rf_size.on > 0) & (df_rf_size.r_groups == 0)].on.values\n",
    "dep_gr1 = df_rf_size[(df_rf_size.on > 0) & (df_rf_size.r_groups == 1)].on.values\n",
    "dep_gr2 = df_rf_size[(df_rf_size.on > 0) & (df_rf_size.r_groups == 2)].on.values\n",
    "dep_gr3 = df_rf_size[(df_rf_size.on > 0) & (df_rf_size.r_groups == 3)].on.values\n",
    "dep_gr4 = df_rf_size[(df_rf_size.on > 0) & (df_rf_size.r_groups == 4)].on.values\n",
    "\n",
    "print len(dep_gr0), len(dep_gr1), len(dep_gr2), len(dep_gr3), len(dep_gr4)\n",
    "print np.mean(dep_gr0), np.mean(dep_gr1), np.mean(dep_gr2), np.mean(dep_gr3), np.mean(dep_gr3)\n",
    "print sstat.kruskal(dep_gr0, dep_gr1, dep_gr2, dep_gr3, dep_gr3)\n",
    "\n",
    "print sstat.ks_2samp(dep_gr0, dep_gr1)\n",
    "print sstat.ks_2samp(dep_gr0, dep_gr2)\n",
    "print sstat.ks_2samp(dep_gr0, dep_gr3)\n",
    "print sstat.ks_2samp(dep_gr0, dep_gr4)\n",
    "\n",
    "print sstat.ks_2samp(dep_gr1, dep_gr2)\n",
    "print sstat.ks_2samp(dep_gr1, dep_gr3)                                                                                                                      \n",
    "print sstat.ks_2samp(dep_gr1, dep_gr4)\n",
    "\n",
    "print sstat.ks_2samp(dep_gr2, dep_gr3)\n",
    "print sstat.ks_2samp(dep_gr2, dep_gr4)\n",
    "\n",
    "print sstat.ks_2samp(dep_gr3, dep_gr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "from detect_peaks import detect_peaks\n",
    "_data = data\n",
    "idx = 0\n",
    "for unit in _data.cluster_id.unique():\n",
    "    tmp = _data[_data.cluster_id == unit]\n",
    "    \n",
    "    for stim in tmp.stim1.unique():\n",
    "        tmp2 = tmp[tmp.stim1 == stim].Hz.values\n",
    "        \n",
    "        thresh = np.mean(tmp2[:50]) + 2 * np.std(tmp2[:50])\n",
    "        idx=+1\n",
    "        try:\n",
    "            peak_0 = detect_peaks(tmp2[50:80], mph = thresh, mpd = 30)[0]\n",
    "        except:\n",
    "            peak_0 = np.nan\n",
    "        try:\n",
    "            peak_1 = detect_peaks(tmp2[100:130], mph = thresh, mpd = 30)[0]\n",
    "        except:\n",
    "            peak_1 = np.nan    \n",
    "   \n",
    "        out = pd.DataFrame({'cluster_id': unit, 'stim1':stim, 'lnc_4c':peak_0,\n",
    "                           'lnc_stim': peak_1 }, index = [idx])\n",
    "        ls.append(out)\n",
    "\n",
    "#         plt.plot(tmp2)\n",
    "\n",
    "#         plt.axvline(x=peak_1+100)\n",
    "#         plt.show()\n",
    "lnc_df = pd.concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lnc_df['lnc_stim'] = lnc_df['lnc_stim'] + 10\n",
    "g = sns.factorplot(x = \"stim1\", y =\"lnc_stim\", data = lnc_df,  kind = 'point', ci = 68,\n",
    "                  palette = colors, \n",
    "#                    capsize = 0.02, saturation = 1,\n",
    "                   size = 4, aspect=1 \n",
    "                  )\n",
    "print lnc_df.groupby('stim1').lnc_stim.mean()\n",
    "plt.ylim(12, 22)\n",
    "# g.invert_yaxis()  \n",
    "# plt.savefig('point_peak_time.pdf'.format(stim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_palette(colors)\n",
    "for idx, stim in enumerate(lnc_df.stim1.unique()[:]):\n",
    "    _inp = lnc_df[(lnc_df.stim1 == stim)].lnc_stim.dropna()\n",
    "    sns.kdeplot(_inp.values, cumulative=True, color = colors[idx])\n",
    "#     plt.title( stim + 'n' + str(_inp.values.size))\n",
    "    plt.xlim(0, 35)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "# plt.savefig('cdf_peak_time.pdf'.format(stim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1 = lnc_df[(lnc_df.stim1 == '10')].lnc_stim.dropna()\n",
    "pt2 = lnc_df[(lnc_df.stim1 == '20')].lnc_stim.dropna()\n",
    "pt3 = lnc_df[(lnc_df.stim1 == '30')].lnc_stim.dropna()\n",
    "pt4 = lnc_df[(lnc_df.stim1 == '40')].lnc_stim.dropna()\n",
    "\n",
    "print np.mean(pt1)/100, sstat.sem(pt1)/100\n",
    "print np.mean(pt2)/100, sstat.sem(pt2)/100\n",
    "print np.mean(pt3)/100, sstat.sem(pt3)/100\n",
    "print np.mean(pt4)/100, sstat.sem(pt4)/100\n",
    "\n",
    "print len(pt1), len(pt2), len(pt3), len(pt4)\n",
    "\n",
    "print sstat.kruskal(pt1, pt2, pt3, pt4)\n",
    "\n",
    "print sstat.mannwhitneyu(pt1, pt2)\n",
    "print sstat.mannwhitneyu(pt1, pt3)\n",
    "print sstat.mannwhitneyu(pt1, pt4)\n",
    "print sstat.mannwhitneyu(pt2, pt3)\n",
    "print sstat.mannwhitneyu(pt2, pt4)\n",
    "print sstat.mannwhitneyu(pt3, pt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_templates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resampy\n",
    "from copy import deepcopy\n",
    "import scipy.optimize as opt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "def gaus(x,a,x0,sigma):\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_templates\n",
    "spk_width = {}\n",
    "d_fwhm = {}\n",
    "tr2peak = {}\n",
    "neuron_type = {}\n",
    "ls = []\n",
    "ls2 = []\n",
    "ls3 = []\n",
    "# f, ax = plt.subplots(1,2)\n",
    "for ii in result.cluster_id.unique()[::]:\n",
    "#trough-to-peak    \n",
    "    #tp =  result[(result['cuid'] == ii)  & (result.index > 18)  ].tmt.idxmax() - result[(result['cuid'] == ii)  & (result.index == 18)  ].index\n",
    "    tmt_data = np.array(result[(result['cluster_id'] == ii)   ].tmt)\n",
    "\n",
    "    y = resampy.resample( tmt_data[::-1] , 1 ,10,  filter='sinc_window',\n",
    "                                    num_zeros=10, precision=5,\n",
    "                                    window=ssig.hann)\n",
    "    trough_idx = y.argmin()\n",
    "    peak_idx = y[:y.argmin()].argmax()\n",
    "#     plt.plot(y)\n",
    "#     plt.axvline(x= y.argmin(), color = 'k', linestyle = '--')\n",
    "#     plt.axvline(x= y[:y.argmin()].argmax(), color = 'r',linestyle = '--')\n",
    "#     plt.show()\n",
    "\n",
    "    tp = abs((y.argmin() - y[:y.argmin()].argmax())/300.0)\n",
    "    \n",
    "    x = np.arange(y.size)\n",
    "    y_gaus = y*(-1)\n",
    "    popt,pcov = opt.curve_fit(gaus,x,y_gaus,p0=[0.2, y.argmin(), 10])\n",
    "    fwhm = popt[-1]/300*2.355\n",
    "    \n",
    "#     plt.plot(x,y*(-1),'b+:')\n",
    "#     plt.plot(x,gaus(x,*popt),'r--')\n",
    "#     plt.show()\n",
    "\n",
    "    f,pxx = ssig.welch(tmt_data, fs=3e4,  nfft=5096,  nperseg=48,\n",
    "                          return_onesided=True, scaling='spectrum')\n",
    "\n",
    "    df = np.vstack((f, pxx))\n",
    "    df = pd.DataFrame(df)\n",
    "    idx = df.T[1].idxmax()\n",
    "    w = df.T[0][idx]\n",
    "    w = 1/w*1000.0\n",
    "\n",
    "    ls2.append(w)\n",
    "    ls.append(tp)\n",
    "    \n",
    "    spk_width[ii] = w\n",
    "    tr2peak[ii] = tp\n",
    "    d_fwhm[ii] = fwhm\n",
    "    if tp<0.4:\n",
    "        neuron_type[ii] = 'fs'\n",
    "    else:\n",
    "        neuron_type[ii] = 'rs'\n",
    "\n",
    "#p/t ratio\n",
    "    edge = 100\n",
    "    if peak_idx < 100:\n",
    "        edge = peak_idx\n",
    "    y_slope = y[peak_idx-edge:peak_idx]\n",
    "    x_slope = np.arange(y_slope.size)\n",
    "    slope, intercept, r_value, p_value, std_err = sstat.linregress(x_slope, y_slope)\n",
    "    ls3.append(1e3*(slope))\n",
    "    \n",
    "result['n_type'] = result.cluster_id.map(neuron_type)\n",
    "result['sp_w'] = result.cluster_id.map(spk_width)\n",
    "result['tp'] = result.cluster_id.map(tr2peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (sns.jointplot(np.array(ls2), np.array(ls), stat_func= None,\n",
    "             color=\"k\", s = 20)\n",
    "       .plot_joint(sns.kdeplot, zorder=0, n_levels=6))\n",
    "g.set_axis_labels('Spike width [ms]', 't-p width [ms]')\n",
    "# plt.axhline(y= 0.45, linestyle = '--')\n",
    "plt.axvline(x= 1.1,  linestyle = '--')\n",
    "plt.axhline(y= 0.4, linestyle = '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = result.cluster_id.unique().size\n",
    "print total\n",
    "print result[result.index == 0].groupby('n_type').cluster_id.count()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reset_index()\n",
    "result[result.n_type == 'rs'].groupby('index').tmt.mean().plot()\n",
    "result[result.n_type == 'fs'].groupby('index').tmt.mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF map analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\rf_maps_kic.pkl\"\n",
    "# rf_df = pd.read_pickle(path)\n",
    "d_rf = rf_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp = kic_5km\n",
    "for unit in _inp[(_inp.et == '434') & (_inp.rec == 'left')].cluster_id.unique()[:6]:\n",
    "    print unit\n",
    "    print _inp[_inp.cluster_id == unit].depth[0].mean()-1000\n",
    "    try:\n",
    "        plt.imshow(d_rf['on'][unit], cmap = plt.cm.gray)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        plt.imshow(d_rf['off'][unit], cmap = plt.cm.gray)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    except:\n",
    "        continue\n",
    "# plt.savefig('rf_map_145005et435right.pdf'.format(stim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rf quanitification using opt.curve_fit to 2d-gaus\n",
    "# d_rf_size = {}\n",
    "# d_rf_size['on'] = {}\n",
    "# d_rf_size['off'] = {}\n",
    "for unit in kic_5km.cluster_id.unique()[14:15]:    \n",
    "    data_fitted_on = data_fitted_off = data_on = data_off = threshold = popt_on = popt_off = 0\n",
    "    try: \n",
    "        _inp_on = deepcopy(d_rf['on'][unit])[:,30:120]\n",
    "        _inp_off = deepcopy(d_rf['off'][unit])[:,30:120]\n",
    "        if np.std(_inp_on) > 2 or np.std(_inp_off)>2:\n",
    "            continue\n",
    "            \n",
    "        f, ax = plt.subplots(2, 3, sharex = True, sharey = True, figsize = (9,6))\n",
    "        \n",
    "        ax[0][0].imshow(_inp_on, cmap = 'gray' )\n",
    "        ax[1][0].imshow(_inp_off, cmap = 'gray' )\n",
    "\n",
    "#         tmp = np.array(sstat.zscore(_inp,ddof=1))\n",
    "#         _inp_on = gaussian_filter(_inp_on, sigma=1)\n",
    "#         _inp_off = gaussian_filter(_inp_off, sigma=1)\n",
    "        \n",
    "        tmp_on, threshold = threshold_rf(_inp_on, 0.975)\n",
    "        tmp_off, threshold = threshold_rf(_inp_off, 0.025)\n",
    "        \n",
    "\n",
    "#         ax[1].imshow(tmp, cmap = 'gray')\n",
    "        if threshold == 0:\n",
    "            continue\n",
    "        \n",
    "        ax[0][1].imshow(tmp_on, cmap='Reds_r' )\n",
    "        ax[1][1].imshow(tmp_off, cmap='Blues')\n",
    "\n",
    "        data_on = deepcopy(tmp_on)/255\n",
    "        data_off = deepcopy(tmp_off)/255\n",
    "\n",
    "#         data_on = ssig.medfilt2d(data_on, kernel_size=(3,3))\n",
    "        \n",
    "        x = np.linspace(0, 89, 90)\n",
    "        y = np.linspace(0, 89, 90)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "       \n",
    "        ind_on = np.unravel_index(np.argmin(data_on, axis=None), data_on.shape)\n",
    "        initial_guess_on = (data_on.min(), ind_on[1], ind_on[0], 20, 20, 0, 1)\n",
    "        popt_on, pcov_on = opt.curve_fit(twoD_Gaussian, (x,y), data_on.ravel(),absolute_sigma=True,\n",
    "                  p0 = initial_guess_on, bounds = (-np.inf, [1, 90, 90, 40, 40, np.inf, np.inf]))\n",
    "        perr_on = np.sqrt(np.diag(pcov_on))  \n",
    "        \n",
    "        ind_off = np.unravel_index(np.argmax(data_off, axis=None), data_off.shape)\n",
    "        initial_guess_off = (data_off.max(), ind_off[1], ind_off[0], 20, 20, 0, 1)\n",
    "        popt_off, pcov_off = opt.curve_fit(twoD_Gaussian, (x,y), data_off.ravel(), absolute_sigma=True,\n",
    "                       p0 = initial_guess_off, bounds = (-np.inf, [1, 90, 90, 40, 40, np.inf, np.inf]))\n",
    "        perr_off = np.sqrt(np.diag(pcov_off))\n",
    "\n",
    "        data_fitted_on = twoD_Gaussian((x, y), *popt_on)\n",
    "        data_fitted_off = twoD_Gaussian((x, y), *popt_off)\n",
    "\n",
    "        ax[0][2].imshow(data_fitted_on.reshape(90, 90), cmap='Reds_r')\n",
    "        ax[1][2].imshow(data_fitted_off.reshape(90, 90), cmap='Blues')\n",
    "        \n",
    "        if abs(popt_on[3]) < 1.9 or abs(popt_on[3]) > 20 or abs(popt_on[4]) < 1.9 or abs(popt_on[4]) > 20:\n",
    "            hwhm_on = np.nan\n",
    "        else:\n",
    "            hwhm_on = 2.355*0.5*(abs(popt_on[3]) + abs(popt_on[4]))*0.5*0.96\n",
    "            ax[0][2].text(0.95, 0.05, \"\"\"\n",
    "            err: %.1f\n",
    "            x : %.1f\n",
    "            y : %.1f\n",
    "            width_x : %.1f\n",
    "            width_y : %.1f\"\"\" %(perr_on[3]+perr_on[4], \n",
    "                                popt_on[1], popt_on[2], popt_on[3], popt_on[4]),\n",
    "                    fontsize=14, horizontalalignment='right',\n",
    "                    verticalalignment='bottom', transform=ax[0][2].transAxes)\n",
    "\n",
    "        if abs(popt_off[3]) < 1.9 or abs(popt_off[3]) > 20 or abs(popt_off[4]) < 1.9 or abs(popt_off[4]) > 20:\n",
    "            hwhm_off = np.nan\n",
    "        else:\n",
    "            # compute flhm/2 and scale by degree in pixel for donwsa,ple images (12 times)\n",
    "            hwhm_off = 2.355*0.5*(abs(popt_off[3]) + abs(popt_off[4]))*0.5*0.96\n",
    "            ax[1][2].text(0.95, 0.05, \"\"\"\n",
    "            err: %.1f\n",
    "            x : %.1f\n",
    "            y : %.1f\n",
    "            width_x : %.1f\n",
    "            width_y : %.1f\"\"\" %(perr_off[3]+perr_off[4], popt_off[1], popt_off[2], popt_off[3], popt_off[4]),\n",
    "                    fontsize=14, horizontalalignment='right',\n",
    "                    verticalalignment='bottom', transform=ax[1][2].transAxes)\n",
    "#         d_rf_size['on'][unit] = hwhm_on\n",
    "#         d_rf_size['off'][unit] = hwhm_off\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# plt.savefig('rf_size_analysis2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_size = pd.DataFrame.from_dict(d_rf_size).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowOpen = np.ones((5,5),np.uint8)\n",
    "tmp = np.interp(_inp_off, (_inp_off.min(), _inp_off.max()), (0, 255))\n",
    "equ = cv2.equalizeHist(tmp.astype(np.uint8))\n",
    "blur = cv2.GaussianBlur(equ, (5,5), 3)\n",
    "retval, thresh = cv2.threshold(blur.astype(np.uint8), 30, 255, cv2.THRESH_BINARY_INV)\n",
    "pupilFrame = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, windowOpen)\n",
    "# threshold = cv2.inRange(pupilFrame,10,255)\t\t#get the blobs\n",
    "_, contours, hierarchy = cv2.findContours(pupilFrame,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = sorted(contours, key = cv2.contourArea, reverse = True)\n",
    "cnt = cnts[0]\n",
    "# cv2.drawContours(pupilFrame, [cnt], 0, 120, 1)\n",
    "try:\n",
    "    ellipse = cv2.fitEllipse(cnt)\n",
    "    cv2.ellipse(pupilFrame, ellipse, 120,1)\n",
    "except:\n",
    "    print 'no fit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pupilFrame, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\Kanizsa paper ephys\\kic_rf_size.pkl\"\n",
    "# df_rf_size = pd.read_pickle(path)\n",
    "# df_rf_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(figsize = (6,6))\n",
    "_inp = df_rf_size\n",
    "g = sns.jointplot(x=\"on\", y=\"depth\", data=_inp[_inp.on > 0], kind = 'kde', \n",
    "                  color = 'r', stat_func=None, xlim = (0, 15))\n",
    "g.plot_marginals(sns.rugplot,height = 0.25, color = 'k' )\n",
    "\n",
    "# plt.savefig('kde_rf_size_on_depth.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_size[df_rf_size.off > 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_d = dict(zip(kic_5km.cluster_id, kic_5km.depth))\n",
    "gr_d = dict(zip(kic_5km.cluster_id, kic_5km.r_groups))\n",
    "df_rf_size['depth'] = df_rf_size['index'].map(dep_d)\n",
    "df_rf_size['r_groups'] = df_rf_size['index'].map(gr_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_rf_size[(df_rf_size.on > 0) & (df_rf_size.r_groups == 4)].on, color = 'r')\n",
    "sns.distplot(df_rf_size[(df_rf_size.off > 0) & (df_rf_size.r_groups == 4)].off, color = 'b')\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 0.5)\n",
    "# plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "sns.despine()\n",
    "plt.savefig('dist_rf_size_group4.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_size.to_pickle('kic_rf_size.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_rf(_inp, a):\n",
    "    hist, bin_edges = np.histogram(_inp.flatten(), density=True)\n",
    "    for idx, val in enumerate(np.cumsum(hist/hist.sum())):\n",
    "    #             if val >= 0.03:\n",
    "        if val >= a:\n",
    "            threshold = bin_edges[idx]-0.0001\n",
    "            break\n",
    "\n",
    "    tmp = deepcopy(_inp)\n",
    "    tmp = gaussian_filter(tmp, sigma=2)\n",
    "    idx = tmp > threshold\n",
    "    tmp[idx] = False\n",
    "    return tmp, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp = deepcopy(d_rf['off'][unit])\n",
    "_inp = _inp[:,30:120]\n",
    "plt.imshow(_inp, cmap = 'gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "tmp = deepcopy(_inp)\n",
    "idx = tmp > threshold\n",
    "tmp[idx] = False\n",
    "tmp = tmp\n",
    "# tmp = sstat.zscore(np.ravel(tmp))\n",
    "plt.imshow(tmp, cmap = 'gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "#         tmp = ssig.medfilt2d(_inp, kernel_size=(7,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(_inp.flatten(), density=True)\n",
    "for idx, val in enumerate(np.cumsum(hist/hist.sum())):\n",
    "    if val >= 0.015:\n",
    "        threshold = bin_edges[idx]-0.0001\n",
    "        break\n",
    "print threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(height, center_x, center_y, width_x, width_y):\n",
    "    \"\"\"Returns a gaussian function with the given parameters\"\"\"\n",
    "    width_x = float(width_x)\n",
    "    width_y = float(width_y)\n",
    "    return lambda x,y: height*np.exp(\n",
    "                -(((center_x-x)/width_x)**2+((center_y-y)/width_y)**2)/2)\n",
    "\n",
    "def moments(data):\n",
    "    \"\"\"Returns (height, x, y, width_x, width_y)\n",
    "    the gaussian parameters of a 2D distribution by calculating its\n",
    "    moments \"\"\"\n",
    "    total = data.sum()\n",
    "    X, Y = np.indices(data.shape)\n",
    "    x = (X*data).sum()/total\n",
    "    y = (Y*data).sum()/total\n",
    "    col = data[:, int(y)]\n",
    "    width_x = np.sqrt(np.abs((np.arange(col.size)-y)**2*col).sum()/col.sum())\n",
    "    row = data[int(x), :]\n",
    "    width_y = np.sqrt(np.abs((np.arange(row.size)-x)**2*row).sum()/row.sum())\n",
    "    height = data.max()\n",
    "    return height, x, y, width_x, width_y\n",
    "\n",
    "def fitgaussian(data):\n",
    "    \"\"\"Returns (height, x, y, width_x, width_y)\n",
    "    the gaussian parameters of a 2D distribution found by a fit\"\"\"\n",
    "    params = 100, 5, 5, 10,10\n",
    "    errorfunction = lambda p: np.ravel(gaussian(*p)(*np.indices(data.shape)) -\n",
    "                                 data)\n",
    "    p, success = opt.curve_fit(gaussian, errorfunction, params)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gaussian data\n",
    "\n",
    "data = tmp\n",
    "plt.matshow(data, cmap= plt.cm.gist_earth_r)\n",
    "\n",
    "params = fitgaussian(data)\n",
    "fit = gaussian(*params)\n",
    "\n",
    "plt.contour(fit(*np.indices(data.shape)),1, cmap = plt.cm.copper)\n",
    "ax = plt.gca()\n",
    "(height, x, y, width_x, width_y) = params\n",
    "\n",
    "plt.text(0.95, 0.05, \"\"\"\n",
    "x : %.1f\n",
    "y : %.1f\n",
    "width_x : %.1f\n",
    "width_y : %.1f\"\"\" %(x, y, width_x, width_y),\n",
    "        fontsize=16, horizontalalignment='right',\n",
    "        verticalalignment='bottom', transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y indices\n",
    "x = np.linspace(0, 89, 90)\n",
    "y = np.linspace(0, 89, 90)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "#create data\n",
    "data = twoD_Gaussian((x,y), data.max(), ind[1], ind[0], 10, 10, 0, 1)\n",
    "\n",
    "# plot twoD_Gaussian data generated above\n",
    "plt.figure()\n",
    "plt.imshow(data.reshape(90, 90))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xo,y0 center, \n",
    "def twoD_Gaussian((x, y), amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)    \n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = offset + amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) \n",
    "                            + c*((y-yo)**2)))\n",
    "    return g.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some noise to the data and try to fit the data generated beforehand\n",
    "initial_guess = (data.max()/2, ind[1], ind[0], 10, 10, 0, 1)\n",
    "\n",
    "popt, pcov = opt.curve_fit(twoD_Gaussian, (x,y), data.ravel(), p0=initial_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fitted = twoD_Gaussian((x, y), *popt)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hold(True)\n",
    "ax.imshow(data.reshape(90, 90), cmap=plt.cm.gray, origin = 'bot',\n",
    "    extent=(x.min(), x.max(), y.min(), y.max()))\n",
    "ax.contour(x, y, data_fitted.reshape(90,90), 1, colors='r')\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_keyphrases=['Bad Reports','bad', 'MSF', 'phy']\n",
    "directory = r\"U:\\Data\\pak6\\figures\\LFP\\probe_64DB\\wt\\illusory_contours\\LM-ArchT\"\n",
    "matches = [os.path.join(dirpath, f)\n",
    "            for dirpath, dirnames, files in os.walk(directory)\n",
    "            for f in files \n",
    "            if (f.endswith('.h5') and not any(filter(lambda bad: bad in f, exclude_keyphrases)))]\n",
    "matches = [f for f in matches if 'bad' not in f and '201' in f]\n",
    "matches[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in matches[:]:\n",
    "    et = fname.split('\\\\')[-1].split()[0][1:]\n",
    "    rec = fname.split('\\\\')[-1].split()[1].split('_')[0]\n",
    "    paradigm = fname.split('\\\\')[-1].split()[-1].split('_')[-6]\n",
    "    exp =  fname.split('\\\\')[-1].split()[-1].split('_')[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls2 = []\n",
    "\n",
    "for fname in matches[:]:\n",
    "    et = fname.split('\\\\')[-1].split()[0][1:]\n",
    "    rec = fname.split('\\\\')[-1].split()[1].split('_')[0]\n",
    "    paradigm = fname.split('\\\\')[-1].split()[-1].split('_')[-6]\n",
    "    exp =  fname.split('\\\\')[-1].split()[-1].split('_')[1]\n",
    "    print fname\n",
    "    ls = []\n",
    "       \n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "#         trial_length = pd.read_hdf(fname, key = 'trial_duration') \n",
    "    except:\n",
    "        continue\n",
    "    trial_length = 2000\n",
    "    trials =  200\n",
    "    times = np.linspace(0, trial_length/1e3, trial_length)\n",
    "    df3_array = np.reshape(tmp.values,(np.shape(tmp)[0],trials, -1))\n",
    "    df3_array = df3_array[:,np.where((opto_seq==0)  & (trial_seq == 1 ))[0],:]\n",
    "    print df3_array.shape\n",
    "    \n",
    "    df3_avg=np.mean(df3_array,1)\n",
    "    df3_avg = pd.DataFrame(df3_avg).T        \n",
    "    df3_avg['times'] = times\n",
    "    negativity_ch_idx = df3_avg[(df3_avg['times']>= 0.55) & (df3_avg['times']< 0.7)].ix[:,8:16].min().idxmin()\n",
    "#     negativity_ch_idx = negativity_ch_idx - 3\n",
    "    print negativity_ch_idx\n",
    "\n",
    "    # plotting the spectra and FFR for the maximum negativity channel, the first trial\n",
    "    \n",
    "    ddf = tmp.values[negativity_ch_idx,:]\n",
    "    \n",
    "    ddf2 = np.reshape(ddf,(trials, -1))\n",
    "    print ddf2.shape\n",
    "    ddf2 = pd.DataFrame(ddf2)\n",
    "    for i in np.unique(opto_seq):\n",
    "        \n",
    "        for j in np.unique(trial_seq):\n",
    "\n",
    "            sel_idx = np.where((opto_seq == i) & (trial_seq == j))[0]\n",
    "            lfp = ddf2.iloc[sel_idx].mean()\n",
    "\n",
    "            tmp_df = pd.DataFrame({'et': et, 'lfp':lfp, 'paradigm': paradigm,  'stim1':j, 'rec':rec, 'exp':exp,\n",
    "                                   'fname':fname, 'times':times, 'opto':i })\n",
    "            ls.append(tmp_df)\n",
    "    tmp_df2 = pd.concat(ls)\n",
    "    ls2.append(tmp_df2)\n",
    "    \n",
    "kic_lfp = pd.concat(ls2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kic_lfp.to_pickle('kic-lfp_l4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\kic2_archt\\kic-lfp_sup.pkl\"\n",
    "lfp_sup = pd.read_pickle(path)\n",
    "\n",
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\kic2_archt\\kic-lfp_l4.pkl\"\n",
    "lfp_l4 = pd.read_pickle(path)\n",
    "\n",
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\kic2_archt\\kic-lfp_deep.pkl\"\n",
    "lfp_deep = pd.read_pickle(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opto_recs = lfp_deep.groupby('fname').min().reset_index()\n",
    "x = opto_recs['et'] + opto_recs['exp'] + opto_recs['rec']\n",
    "x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "_inp = lfp_sup[lfp_sup.opto == 0]\n",
    "# opto_recs = _inp.groupby('fname').lfp.min().reset_index()\n",
    "# opto_recs = opto_recs[opto_recs.lfp > -700].fname.unique()\n",
    "# _inp = _inp[_inp.fname.isin(opto_recs)]\n",
    "colors = my_pal\n",
    "colors = colors[::2]\n",
    "_inp = _inp.sort_values(by=['stim1'])\n",
    "# _inp = _inp[_inp.et.str.startswith('7')]\n",
    "ts = sns.tsplot(_inp.reset_index(), time = 'times', value = 'lfp', color  = colors,\n",
    "           unit = 'fname',   estimator=np.nanmean, condition = 'stim1'\n",
    "          )\n",
    "\n",
    "\n",
    "plt.axvspan(1, 1.5, alpha = 0.2, color='k')\n",
    "plt.axvspan(0.5, 1, alpha = 0.1, color='k')\n",
    "\n",
    "#ts.set(xlim=(1,2))\n",
    "#ts.set(ylim=(-200,200))\n",
    "sns.despine()\n",
    "# plt.savefig('sup_vep_opto_kic.pdf')\n",
    "# plt.savefig('sf_tc_post.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp.fname.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_inp = master_df[master_df.training=='pre'].reset_index()\n",
    "_inp = lfp_sup[lfp_sup.opto == 1\n",
    "              ]\n",
    "\n",
    "vep1 = _inp[(_inp.times > 0.55) & (_inp.times < 1)]\n",
    "vep1 = vep1.ix[vep1.groupby([ 'fname' ,'stim1']).lfp.idxmin().values]\n",
    "vep1 = vep1.groupby([ 'fname' ,'stim1']).min()\n",
    "vep1 = vep1.reset_index()\n",
    "\n",
    "\n",
    "# vep2 = _inp[(_inp.times>0.6) & (_inp.times<0.7)]\n",
    "# vep2 = vep2.ix[vep2.groupby(['stim1' ,'fname']).lfp.idxmax().values]\n",
    "# vep2 = vep2.groupby( ['stim1', 'fname']).max()\n",
    "# vep2 = vep2.reset_index()\n",
    "# vep2['vep'] = 'dd'\n",
    "\n",
    "# vep3 = _inp[(_inp.times>0.93) & (_inp.times<1.03)]\n",
    "# vep3 = vep3.ix[vep3.groupby('fname').lfp.idxmin().values]\n",
    "# vep3 = vep3.groupby('fname').min()\n",
    "# vep3['vep'] = 3\n",
    "colors = my_pal[::2]\n",
    "\n",
    "vep1 = vep1.sort_values(by=['stim1'])\n",
    "# _data = band_df\n",
    "g = sns.factorplot(x=\"stim1\", y=\"lfp\",  data=vep1,  kind = 'bar', ci = 68, palette = colors,\n",
    "#                     capsize = 0.02, saturation = 1,\n",
    "                   size = 5, aspect = 1 )\n",
    "# plt.ylim(-700, 700)\n",
    "# plt.savefig('deep_cir_vep_bar.pdf', transparent=True)\n",
    "# plt.savefig('sert_ko_amp_vep_all.png', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = vep1[vep1.stim1 == 1].lfp.values\n",
    "x2 = vep1[vep1.stim1 == 2].lfp.values\n",
    "x3 = vep1[vep1.stim1 == 3].lfp.values\n",
    "x4 = vep1[vep1.stim1 == 4].lfp.values\n",
    "\n",
    "print np.mean(x1), sstat.sem(x1)\n",
    "print np.mean(x2), sstat.sem(x2)\n",
    "print np.mean(x3), sstat.sem(x3)\n",
    "print np.mean(x4), sstat.sem(x4)\n",
    "\n",
    "print len(x1), len(x2), len(x3), len(x4)\n",
    "print sstat.kruskal(x1, x2, x3, x4)\n",
    "\n",
    "print len(x1)\n",
    "\n",
    "print sstat.mannwhitneyu(x1, x2)\n",
    "print sstat.mannwhitneyu(x1, x3)\n",
    "print sstat.mannwhitneyu(x1, x4)\n",
    "print sstat.mannwhitneyu(x2, x3)\n",
    "print sstat.mannwhitneyu(x2, x4)\n",
    "print sstat.mannwhitneyu(x3, x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import OpenOE_AC_map_functions_v1_08_30s as oem\n",
    "from  __builtin__ import any as b_any\n",
    "import matplotlib.ticker as tkr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "ls2= []\n",
    "ls_perm = []\n",
    "sf_col = [0.01, 0.02, 0.04, 0.08, 0.14 ]\n",
    "_data = _inp[(_inp.times > 0.1) & (_inp.times < 1.9)]\n",
    "#               & (sert_df.stim1 == '1')\n",
    "#               & (sert_df.stim2 == 'G')\n",
    "#                & (sert_df.times < 2.5)\n",
    "               #]\n",
    "\n",
    "param = 'stim1'\n",
    "n =sorted(_data[param].unique()) \n",
    "f, ax = plt.subplots( 1, len(n), sharex=True, sharey=True, figsize=(12,6), facecolor = 'w')\n",
    "t1, t2 = 350, 550\n",
    "\n",
    "\n",
    "for idx, val in enumerate(sorted(_data[param].unique())):\n",
    "    \n",
    "    #tmp = _data[_data['stim1']==sf].pivot_table(index = ['fname', 'et'], columns= 'times' ,values = 'lfp').mean(level=1)\n",
    "    tmp = _data[_data[param]==val].pivot(index = 'fname', columns= 'times' ,values = 'lfp') \n",
    " \n",
    "    tf, time, frex, tf3d = oem.tf_cmw(ax[idx], tmp, show=True, log_scale=False)\n",
    "    ax[idx].set_title('' + str(val))\n",
    "    \n",
    "    tf3_arr = np.dstack(tf3d)\n",
    "    tf3_arr = 10*np.log10(tf3_arr/tf3_arr[:,:300,:].mean(axis=1, keepdims=1))\n",
    "\n",
    "#     mask = frex[(frex>12) & (frex<25)]\n",
    "#     ax[idx].plot(((tf[1, int(mask[0]):int(mask[-1]), :]).mean(axis=0)))\n",
    "    ls_perm.append(tf3_arr)\n",
    "    \n",
    "    tmp = np.dstack(tf3d)\n",
    "    tmp = tmp.swapaxes(0,2)\n",
    "    tmp = 10*np.log10(tmp/tmp[:,:300,:].mean(axis=1, keepdims=1))\n",
    "\n",
    "    alpha = np.mean(tmp[(frex>8) & (frex<=12)], axis=0)[t1:t2]\n",
    "    alpha = np.mean(alpha,axis=0)\n",
    "\n",
    "    beta = np.mean(tmp[(frex>12) & (frex<=25)], axis=0)[t1:t2]\n",
    "    beta = np.mean(beta,axis=0)\n",
    "\n",
    "    low_gamma = np.mean(tmp[(frex>30) & (frex<=50)], axis=0)[t1:t2]\n",
    "    low_gamma = np.mean(low_gamma,axis=0)\n",
    "\n",
    "    high_gamma = np.mean(tmp[(frex>50) ], axis=0)[t1:t2]\n",
    "    high_gamma = np.mean(high_gamma,axis=0)\n",
    "\n",
    "    theta = np.mean(tmp[(frex>=4) & (frex<=8)], axis=0)[t1:t2]\n",
    "    theta = np.mean(theta,axis=0)\n",
    "\n",
    "\n",
    "    tf_tmp = pd.DataFrame({'cond': val,  'beta':beta, 'hg': high_gamma,\n",
    "                           'alpha': alpha,   'lg': low_gamma, 'theta': theta,\n",
    "                   'index': np.arange(0, _data[_data[param]==val].fname.unique().size)\n",
    "                     })\n",
    "    ls2.append(tf_tmp)\n",
    "# tf_out = pd.concat(ls2)\n",
    "# tf_tmp = pd.melt(tf_out, id_vars=['cond'], value_vars=['theta', 'alpha', 'beta', 'lg','hg' ])\n",
    "# tf_out = pd.melt(tf_out, id_vars=['cond','index' ], \n",
    "#                  value_vars=['theta', 'alpha', 'beta',  'lg','hg' ])\n",
    "# tf_out = tf_out.groupby(['cond','variable', 'index']).mean().reset_index()\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"tf_l4_kic.png\", transparent=True)\n",
    "# plt.savefig(\"deep_tf_kic.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.stats import ttest_1samp_no_p\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1e-3\n",
    "threshold_tfce = dict(start=5, step=1)\n",
    "stat_fun_hat = partial(ttest_1samp_no_p, sigma=sigma)\n",
    "T_obs, clusters, cluster_p_values, H0 = mne.stats.permutation_cluster_1samp_test(ls_perm[0],\n",
    "                             n_permutations = 100, threshold = 7.0,  tail = 1, t_power=1)\n",
    "\n",
    "T_obs_plot = np.nan * np.ones_like(T_obs)\n",
    "for c, p_val in zip(clusters, cluster_p_values):\n",
    "    if p_val <= 0.05:\n",
    "        T_obs_plot[c] = T_obs[c]\n",
    "\n",
    "# f, ax = plt.subplots(figsize = (8,2.5))\n",
    "# ax.set_xlim(0,2.5)\n",
    "# ax.set_yscale('log')\n",
    "# hm = ax.set_yticks(np.logspace(np.log10(2),np.log10(80),6))\n",
    "# ax.set_yticklabels(np.round(np.logspace(np.log10(2),np.log10(80),6)))\n",
    "# ax.contourf(time, frex, T_obs.T, \n",
    "           \n",
    "#            aspect='auto', origin='lower', cmap='gray')\n",
    "# hm = ax.contourf( T_obs_plot.T, \n",
    "           \n",
    "#            aspect='auto', origin='lower', cmap='jet')\n",
    "# plt.colorbar(hm)\n",
    "# plt.xlabel('Time (s)')\n",
    "# plt.ylabel('Frequency (Hz)')\n",
    "# plt.savefig(\"tf_permutation.png\", transparent=True)\n",
    "# plt.savefig(\"deep_tf_perm_kic_rot.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (2.8,5.6))\n",
    "hm = ax.contourf( time, frex, T_obs_plot.T, \n",
    "           \n",
    "           aspect='auto', origin='lower', cmap='jet')\n",
    "# plt.savefig(\"tf_perm_line.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CSD pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls2 = []\n",
    "\n",
    "times = np.linspace(0, 1.0, 1000)\n",
    "f, ax = plt.subplots(2, figsize = (1,1))\n",
    "for fname in matches:\n",
    "    if '201' in fname and 'CH65_2' in fname:\n",
    "        continue\n",
    "    else:\n",
    "        print fname\n",
    "#     t = fname.split('\\\\')[-1].split()[-1].split('_')[4]\n",
    "#     if b_any(t in x for x in good_recs.fname.unique()):\n",
    "#         print fname\n",
    "#     else:\n",
    "#         continue\n",
    "    \n",
    "    et = fname.split('\\\\')[-1].split()[0][1:]\n",
    "    rec = fname.split('\\\\')[-1].split()[1].split('_')[0]\n",
    "    paradigm = fname.split('\\\\')[-1].split()[-1].split('_')[-6]\n",
    "    exp =  fname.split('\\\\')[-1].split()[-1].split('_')[1]\n",
    "    \n",
    "    ls = []\n",
    "        \n",
    "    try:\n",
    "        tmp =  pd.read_hdf(fname, key = 'raw') \n",
    "    except:\n",
    "        continue\n",
    "    trials =  int(tmp.shape[1]/1000)\n",
    "\n",
    "\n",
    "    csd_tmp = np.split(tmp, 200, axis = 1)\n",
    "    csd_input = np.dstack(csd_tmp)\n",
    "    \n",
    "\n",
    "    for i in np.unique(opto_seq):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        for j in np.unique(trial_seq):\n",
    "        \n",
    "            sel_idx = np.where((opto_seq == i) & (trial_seq == j))[0]\n",
    "            csd_tmp = csd_input[:,:, sel_idx].mean(axis = 2)\n",
    "\n",
    "            csd_tmp = pd.DataFrame(csd_tmp)\n",
    "            csd_tmp = csd_tmp - csd_tmp.median(axis = 0)\n",
    "            ddf2 = csd_tmp\n",
    "            csd = oem.df_CSD_analysis( np.array(ddf2), ax, \n",
    "                              Channel_Number=np.shape(ddf2)[0], show_plot=False)\n",
    "            tmp_df = pd.DataFrame(csd).stack().reset_index()\n",
    "            tmp_df.columns = ['csd_step', 'samples', 'csd']\n",
    "            tmp_df['et'] = et\n",
    "            tmp_df['exp'] = exp\n",
    "            tmp_df['paradigm'] = paradigm\n",
    "            tmp_df['rec'] = rec\n",
    "            tmp_df['stim1'] = j\n",
    "            tmp_df['opto'] = i\n",
    "            tmp_df['fname'] = fname\n",
    "            tmp_df['times'] = tmp_df['samples']/1000\n",
    "            ls.append(tmp_df)\n",
    "    tmp_df2 = pd.concat(ls)\n",
    "    ls2.append(tmp_df2)\n",
    "kic_csd = pd.concat(ls2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kic_csd.groupby('fname').csd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = kic_csd[kic_csd.et.str.startswith('4')]\n",
    "n = sorted(_data.stim1.unique())\n",
    "f,ax = plt.subplots(len(n),1, figsize = (8,12), sharey= True, sharex=True,facecolor = 'w')\n",
    "cbar_ax = f.add_axes([.97, .3, .03, .4])\n",
    "formatter = tkr.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-2, 2))\n",
    "\n",
    "\n",
    "for idx, val in enumerate(n):\n",
    "    hm_input = _data[_data.stim1 == val].groupby(['csd_step', \n",
    "            'times']).mean().reset_index().pivot('csd_step', 'times', 'csd').values\n",
    "    hm = sns.heatmap(hm_input, cmap = 'jet',  annot=False, xticklabels=  500, ax = ax[idx], \n",
    "                cbar = 0==idx, \n",
    "                cbar_kws={ \"format\": formatter},\n",
    "                vmin = -1e6, vmax = 1e6, robust = True, yticklabels=False,  cbar_ax=None if idx else cbar_ax )\n",
    "    ax[idx].invert_yaxis()\n",
    "    ax[idx].set_title(val)\n",
    "# mappable = hm.get_children()[0]\n",
    "# plt.colorbar(mappable, ax = [ax[0],ax[1], ax[2], ax[3]],orientation = 'horizontal')\n",
    "# plt.savefig(\"csd_kic.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"U:\\Data_Analysis\\pak6\\Analysis of units\\kic2_archt\\kic-archt-csd-all-recs-01.pkl\"\n",
    "kic_csd = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_exp_csd.to_hdf('csd_column01_kic.hdf','data',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = mmn_exp_csd[(mmn_exp_csd.opto == 0) & (mmn_exp_csd.stim1 == 4)]\n",
    "ls = []\n",
    "for fname in _data.fname.unique():\n",
    "    out = _data[_data.fname == fname].reset_index().pivot('csd_step', 'times', 'csd').values\n",
    "    ls.append(out)\n",
    "csd = np.dstack(ls)\n",
    "csd4 = np.swapaxes(csd,0,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csd2 = np.swapaxes(csd2,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname.split('\\\\')[-1].split()[1].split('_')[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
